#+latex_class: dissertation
#+latex_class_options: [12pt,a4paper,twoside,openright]
#+latex_header: \usepackage[hyperref=true,url=true,backend=biber,natbib=true]{biblatex}
#+latex_header: \usepackage[margin=1in]{geometry}
#+latex_header: \usepackage{booktabs,ebproof,parskip,stmaryrd}
#+latex_header: \addbibresource{diss.bib}

# math operators
#+latex_header: \DeclareMathOperator{\True}{true}
#+latex_header: \DeclareMathOperator{\False}{false}
#+latex_header: \DeclareMathOperator{\If}{if}
#+latex_header: \DeclareMathOperator{\Then}{then}
#+latex_header: \DeclareMathOperator{\Else}{else}
#+latex_header: \DeclareMathOperator{\Let}{let}
#+latex_header: \DeclareMathOperator{\In}{in}
#+latex_header: \DeclareMathOperator{\Null}{null}
#+latex_header: \DeclareMathOperator{\First}{first}
#+latex_header: \DeclareMathOperator{\Flast}{flast}

# shorthand
#+latex_header: \newcommand\mre{\(\mu\)-regular expression}
#+latex_header: \newcommand\mres{\(\mu\)-regular expressions}
#+latex_header: \newcommand\ky{Krishnaswami-Yallop}

# try to avoid widows and orphans
#+latex_header: \raggedbottom
#+latex_header: \sloppy
#+latex_header: \clubpenalty1000%
#+latex_header: \widowpenalty1000%

# add more header depths
#+options: H:6

list-of-figures:nil

\pagestyle{headings}

* Introduction
  A /language/ is a set of strings over some alphabet of symbols. For example, a
  dictionary enumerates a language over written characters. Spoken English is a
  language, where the alphabet consists of phonemes. Programming languages are
  over an alphabet of ASCII or Unicode characters. IP packets are languages over
  bytes. The language of a decision problem is the set of valid inputs. Every
  computer science problem can be transformed into a question about languages.

  Whilst linear strings are helpful for data storage and transmission, they have
  limited use for other algorithms. Strings are used to encode other non-linear
  data. A /parser/ is a function that takes a language string and decodes it to
  return the underlying data. Parsers should be fast; why spend time decoding
  data when there is useful computation to be done? Unfortunately, hand writing
  efficient parsers is repetitive, with lots of boiler plate, and requires
  careful consideration of the order of operations.

  A /parser generator/ is a curried function taking a description of a language
  and returning a parser. If the target language is known ahead of time, then
  the parser can be computed ahead of time. Automated parser generation raises
  the level of abstraction for writing parsers, allowing people to quickly
  design a language so they can focus on solving difficult problems elsewhere.

  The de facto way to describe a language is with a /formal grammar/. Introduced
  by Chomsky cite:null, a grammar is a finite set of string rewriting rules.
  These rules are often given to the parser generator in Backus-Naur form (BNF).
  This form of parser generation has a couple of problems in practice. Firstly,
  BNF uses a single global mutually-recursive namespace. This is incompatible
  with modern programming languages that almost exclusively use lexical scoping.
  Secondly, there is no standard BNF format. Therefore a grammar cannot be
  easily shared between different projects. Finally, introducing an additional
  compilation step for parser generation is often difficult, sometimes needing
  to completely overhaul the build system being used.

  These issues, combined with a trend towards more powerful type systems, have
  led to an increase in use of /parser combinators/. Parser combinators are
  higher-order functions that take parsers and return a new parser. By using
  regular functions instead of parser generators, problems with variable scoping
  and compilation are completely bypassed.

  Unfortunately, the ease of use of parser combinators comes with a price. Most
  implementations of parser combinators use backtracking, which can lead to
  exponential worst-case parse time. Fortunately, a recent result by
  Krishnaswami and Yallop cite:null found a type system (the /\ky{} type system/)
  for some primitive parser combinators that only accepts linear-time
  deterministic parsers.
  
** Project Overview
   *TODO: Rewrite*

   Some parser combinators can be defined by composing smaller combinators
   together. The \ky{} type system handles this by evaluating the higher-order
   combinators until they reach the base combinators. In section ref:null I form
   the /LM type system/ by extending the \ky{} type system to handle higher-order
   combinators directly.

   Next, I design /Nibble/ in section ref:null. Nibble is a new DSL for
   describing languages, inspired by combinators used in the LM type system.
   Nibble should be able to represent the same languages as BNF.
   
   In section ref:null, I implement /Chomp/, a typed parser generator for
   Nibble. Chomp uses the LM type system to ensure its input is suitable for
   transformation into a recursive-descent parser. Chomp is implemented in Rust,
   and produces Rust source code as output. This output is a /chewed parser/.
   
   I end by evaluating the performance of chewed parsers. Critically in section
   ref:null we show that chewed parsers operate in linear time. Section ref:null
   demonstrates that chewed parsers have comparable performance to other
   generated parsers. Finally section ref:null demonstrates that Nibble is
   potentially usable in practice.
* Preparation
  *STORY: what's this chapter about?*
  
** Background
   This subsection starts with the definition of formal languages, generators
   and parsers. Understanding these definitions is essential for understanding
   the rest of this dissertation. Next, it discusses formal grammars. Formal
   grammars are the traditional way to study context-free languages *(why have
   them?)*. Following this, it describes parser combinators and an algebraic
   interpretation of them. This algebraic interpretation is the core of the \ky{}
   type system and the extended LM type system. Finally this subsection
   describes the \ky{} type system.
   
*** Formal Languages
    Given an alphabet, \( \Sigma \), a /language/
    \( L \subseteq \mathcal{P}(\Sigma^*) \) is a set of strings over this
    alphabet.

    Take a set \( D \) of /derivations/. A function \( g : D \to \Sigma^* \) is
    a /language generator/, with generated language
    \( L[g] = \{ w \in \Sigma^* \mid \exists d \in D. g(d) = w \} \).

    If \( g \) is an injection, then the generator is /unambiguous/. Every
    string in \( L[g] \) will have a unique derivation.

    Take a reversed function \( p : L[g] \to D \). \( p \) is a /parser/ of
    \( g \) if \( p \) is a right inverse of \( g \). In general, many such
    parsers could exist. Figure [[fig:ambiguous-parser]] shows a trivial example.
    The set of parsers for a generator is denoted \( P[g] \). When \( g \) is
    unambiguous, then \( g \) is both injective and surjective over \( L[g] \),
    hence \( p \) is unique.

    #+label: fig:ambiguous-parser
    #+name: fig:ambiguous-parser
    #+caption: A generated language with two parsers.
    #+begin_figure
    \[ D = \{ 0 , 1 \} \]
    \[ g(d) = a \]
    \begin{align*}
      p_1(a) &= 0 \\
      p_2(a) &= 1
    \end{align*}
    \[ g(p_1(a)) = a = g(p_2(a)) \]
    #+end_figure

    For a set of /language descriptions/, \( \mathcal{D} \), define an indexed
    set of generators, \( \mathcal{G} \). A /parser generator/ is a function
    \( \mathcal{P} \) such that
    \( \mathcal{P}(d) \in P[\mathcal{G}_d] \). For any valid language
    description, a parser generator then produces a parser for that language.
*** Formal Grammars
    Formal grammars are a set of language descriptions. Introduce a set of
    /non-terminal symbols/ \( N \). Distinguish a /start symbol/ \( S \in N \).
    Let \( V = \Sigma \uplus N \) be the /vocabulary/ of a grammar.

    A /production rule/ is a pair, \( u \mapsto v \), where \( u \in V^*NV^* \)
    and \( v \in (V/S)^* \). The relation \( wuw' \Mapsto wvw' \) is an
    /application/ of this production rule.

    A grammar \( G \) is a set of production rules. A sequence of applications
    \( S \Mapsto^* w \) is a derivation if \( w \in \Sigma^* \). The generator
    for \( G \) returns these \( w \). An example derivation is given in figure
    [[fig:grammar-example]].

    #+label: fig:grammar-example
    #+name: fig:grammar-example
    #+caption: An example grammar derivation.
    #+begin_figure
    \begin{align*}
      S &\mapsto aX \\
      X &\mapsto aX\\
      aX &\mapsto Xb \\
      X &\mapsto c
    \end{align*}
    \[ S \Mapsto aX \Mapsto aaX \Mapsto aXb \Mapsto acb \]
    #+end_figure

    These /unrestricted grammars/ correspond to recursively enumerable languages
    cite:null. Whilst any string in the language is accepted, rejecting strings
    is undecidable. Chomsky cite:null introduced a hierarchy of constrained
    grammars. Adding more constraints to production rules reduces the
    computational complexity of parsers, at the cost of reduced expressive
    power.

    Context-free grammars have rules of the form \( A \mapsto v \), where
    \( A \in N \). This transforms derivations into trees, with non-terminal
    internal nodes and alphabet strings as leaves.

    /Context-free grammars/ are the smallest class of grammars in the Chomsky
    hierarchy that include paired delimiters. The set of languages they
    represent are called /context-free languages/. Unfortunately, algorithms
    that parse general context-free grammars, such as Earley and CYK, have
    super-linear time complexity. *NOTE: why is this bad?*

    Chomsky cite:null found that context-free grammars can be parsed by
    nondeterministic push-down automata -- finite state machines with a stack.
    Restricting this to deterministic finite automata leads to /deterministic
    context-free grammars/. Their languages can be parsed in linear time, and
    are unambiguous.

    There are generally two approaches to parsing deterministic context-free
    grammars: top-down and bottom-up. Both of these methods are typically
    restricted to one symbol of /lookahead/. This means only one symbol of the
    input is visible at a time, and once the input is advanced it cannot be
    reversed.

    Top-down parsers, or left-most derivation parsers, start at the root of the
    derivation tree and recursively parse each non-terminal. Parsers like this
    one exclude grammars with /left-recursion/; rules of the form
    \( A \mapsto Av \). With only one symbol of lookahead, it is impossible to
    determine how deep the derivation needs to be.

    Bottom-up parsers (right-most derivation parsers) start at the leaves of the
    derivation tree. This eliminates the left-recursion problem, as the tree is
    only built up to the minimum necessary height.
*** Parser Combinators
    A /generator combinator/ is a higher-order language generator. They take
    some number of generators and generator combinators, and produce a new
    generator or generator combinator. A /parser combinator/ is likewise a
    higher-order parser.
    
    Mathematical analysis of arbitrary generator combinators is infeasible --
    they are arbitrary functions, after all. By restricting the combinators used
    to the set composing some primitive combinators, it is possible to introduce
    an algebra to describe them. Figure [[fig:mu-reg-def]] details one such algebra,
    named \mres{}.
    
    #+label: fig:mu-reg-def
    #+name: fig:mu-reg-def
    #+caption: \mres{} and their derivations.
    #+begin_figure
      *TODO: alignment of derivations is a little wonky*
      \[
        e = \bot
          \mid \epsilon
          \mid c
          \mid e \cdot e
          \mid e \vee e
          \mid \mu x. e
          \mid x
      \]
      
      \centering
      \bigskip
      \begin{math}
      \begin{array}{ccc}
        \begin{prooftree}
           \infer0[DEps]{\epsilon &\Mapsto \epsilon}
        \end{prooftree}
        & \qquad &
        \begin{prooftree}
           \infer0[DLit]{c &\Mapsto c}
        \end{prooftree}
        \\
        & \qquad & \\
        \begin{prooftree}
           \hypo{e &\Mapsto w}
           \infer1[DVeeL]{e \vee e' &\Mapsto w}
        \end{prooftree}
        & \qquad &
        \begin{prooftree}
           \hypo{e &\Mapsto w}
           \infer1[DVeeR]{e' \vee e &\Mapsto w}
        \end{prooftree}
        \\
        & \qquad & \\
        \begin{prooftree}
           \hypo{e &\Mapsto w}
           \hypo{e' &\Mapsto w'}
           \infer2[DCat]{e \cdot e' &\Mapsto ww'}
        \end{prooftree}
        & \qquad &
        \begin{prooftree}
           \hypo{e [ \mu x . e / x ] &\Mapsto w}
           \infer1[DFix]{\mu x . e &\Mapsto w}
        \end{prooftree}
      \end{array}
      \end{math}
    #+end_figure

    There are three first-order language generators: \(\bot\) for the empty
    language, \(\epsilon\) for the language of the empty string only, and
    \( c \) for a language containing the single-symbol string \( c \) only.

    There are two second-order combinators. Concatenation, \( g \cdot g' \)
    takes words from \( g \) and concatenates them with words from \( g' \).
    Alternation, \( g \vee g' \), forms the union of the languages \( g \) and
    \( g' \).

    Finally, there is the least-fixed-point combinator \(\mu g\). This is the
    union \( \bigcup_{n\in\mathbb{N}} g^n(\bot) \), assuming \( g \) is
    monotone. It is the fixed point as \( g (\mu g) = \mu g \).

    To complete the definition of \mres{} as language generators, figure
    [[fig:mu-reg-def]] also shows the derivation relation. Lei√ü cite:null found that
    \mres{} describe context-free languages. This means that for any \mre{}, there
    is a context-free grammar with the same language, and vice versa. One
    consequence of this means that general \mres{} take super-linear time to
    parse.

    Context-free grammars resolve the parse complexity problem by a
    transformation into a push-down automaton. The algebraic nature of \mres{}
    lends itself to a type system instead.
    
*** \ky{} Type System
    *Note: All definitions are taken from cite:null. To what extent do they need
    citations?*

    The \ky{} type system is a type judgement for \mres{}. If an expression is well
    typed, then there exists a top-down parser for the language of the
    expression.
    
    There are three properties of languages that are particularly interesting,
    named \( \Null \), \( \First \) and \( \Flast \). Their definitions are in
    figure [[fig:lang-props]]. To summarise, a langauge \( L \) is \( \Null \) when
    it contains the empty string. The \( \First \) set is the set of symbols
    starting strings in \( L \), and the \( \Flast \) set is the set of symbols
    that immediately follow strings in \( L \) to make a bigger string in
    \( L \).
    
    #+label: fig:lang-props
    #+name: fig:lang-props
    #+caption: Definitions of \( \Null \), \( \First \) and \( \Flast \)
    #+begin_figure
      \begin{gather*}
        \Null L \iff \epsilon \in L \\
        \begin{align*}
          \First L &= \{ c \in \Sigma \mid \exists w \in \Sigma^*.\, cw \in L \} \\
          \Flast L &=
             \{ c \in \Sigma
             \mid \exists w \in \Sigma^+, w' \in \Sigma^*.\,
               w \in L \wedge wcw' \in L
             \}
        \end{align*}
      \end{gather*}
    #+end_figure
    
    A /\ky{} type/ \( \tau \) is a record \( \{\textsc{Null} \in \mathbb{B} ,
    \textsc{First} \subseteq \Sigma , \textsc{Flast} \subseteq \Sigma \}\). A
    language /satisfies/ a type, \( L \vDash \tau \), when \( \Null L \le
    \tau.\textsc{Null} \wedge \First L \subseteq \tau.\textsc{First} \wedge
    \Flast L \subseteq \tau.\textsc{Flast} \). This definition means that a type
    always over-approximates a language's properties. As types are triples of
    values, they can be manipulated by functions. Figure [[fig:mu-type]] shows some
    basic types and some operations on them. It also describes two relations on
    types, used by the typing judgement.
    
    #+label: fig:mu-type
    #+name: fig:mu-type
    #+caption: Some \ky{} types and operations and relations on them
    #+begin_figure
    \[ b \Rightarrow s = \If b \Then s \Else \emptyset \]
    \begin{align*}
      \tau_{\bot} &= ( \False, \emptyset, \emptyset ) \\
      \tau_{\epsilon} &= ( \True, \emptyset, \emptyset ) \\
      \tau_{c} &= ( \False, \{ c \} , \emptyset )
    \end{align*}
    \begin{align*}
      \tau \vee \tau' &= \left\{ \begin{array}{rl}
           \textsc{Null} = &\tau.\textsc{Null} \vee \tau'.\textsc{Null} \\
           \textsc{First} = &\tau.\textsc{First} \cup \tau'.\textsc{First} \\
           \textsc{Flast} = &\tau.\textsc{Flast} \cup \tau'.\textsc{Flast}
         \end{array}\right\} \\
      \tau \cdot \tau' &= \left\{ \begin{array}{rl}
           \textsc{Null} = &\tau.\textsc{Null} \wedge \tau'.\textsc{Null} \\
           \textsc{First} = &\tau.\textsc{First} \cup (\tau.\textsc{Null} \Rightarrow \tau'.\textsc{First}) \\
           \textsc{Flast} = &\tau'.\textsc{Flast} \cup (\tau'.\textsc{Null} \Rightarrow \tau'.\textsc{First} \cup \tau.\textsc{Flast})
         \end{array}\right\}
    \end{align*}
    \begin{align*}
      \tau \circledast \tau' &= (\tau.\textsc{Flast} \cap \tau'.\textsc{First} = \emptyset) \wedge \neg \tau.\textsc{Null} \\
      \tau \# \tau' &= (\tau.\textsc{First} \cap \tau'.\textsc{First} = \emptyset) \wedge \neg (\tau.\textsc{Null} \wedge \tau'.\textsc{Null})
    \end{align*}
    #+end_figure

    Since the aim is to build a top-down parser, an expression cannot be left
    recursive. The \ky{} type system achieves this using two /variable contexts/. A
    variable context is a map from variables to type. One of the variable
    contexts is /unguarded/, meaning that variables can be used freely. The
    other context is /guarded/, meaning variables can only be used on the right
    side of a concatenation.

    Figure [[fig:mu-judge]] gives the full typing judgement of the \ky{} type system.
    Of particular note, the TFix rule assumes \( x \) is guarded in the
    hypothesis, the TCat rule shifts the guarded context into the unguarded one
    for the right side, and the TVar rule can only reference unguarded
    variables. Krishnaswami and Yallop showed cite:null that is an expression
    has a complete typing judgement when the two variable contexts are empty, it
    is possible to compute a parser for the language of that expression.
    
    #+label: fig:mu-judge
    #+name: fig:mu-judge
    #+caption: \ky{} typing judgement
    #+begin_figure
    \begin{math}
    \begin{array}{ccc}
      \begin{prooftree}
        \infer0[TBot]{\Gamma; \Delta &\vdash \bot : \tau_{\bot}}
      \end{prooftree}
      & \qquad &
      \\
      & \qquad &
      \begin{prooftree}
        \infer0[TEps]{\Gamma; \Delta &\vdash \epsilon : \tau_{\epsilon}}
      \end{prooftree}
      \\
      \begin{prooftree}
        \infer0[TChar]{\Gamma; \Delta &\vdash [ c ] : \tau_c}
      \end{prooftree}
      & \qquad &
      \\
      & \qquad &
      \begin{prooftree}
        \infer0[TVar]{\Gamma, x : \tau; \Delta &\vdash x : \tau}
      \end{prooftree}
      \\
      \begin{prooftree}
        \hypo{\Gamma; \Delta &\vdash e : \tau} 
        \hypo{\Gamma; \Delta &\vdash e' : \tau'} 
        \hypo{\tau &\# \tau'}
        \infer3[TVee]{\Gamma; \Delta &\vdash e \vee e' : \tau \vee \tau'}
      \end{prooftree}
      & \qquad &
      \\
      & \qquad &
      \begin{prooftree}
        \hypo{\Gamma; \Delta &\vdash e : \tau} 
        \hypo{\Gamma, \Delta; \cdot &\vdash e' : \tau'} 
        \hypo{\tau &\circledast \tau'}
        \infer3[TCat]{\Gamma; \Delta &\vdash e \cdot e' : \tau \cdot \tau'}
      \end{prooftree}
      \\
      \begin{prooftree}
        \hypo{\Gamma; \Delta, x : \tau &\vdash e : \tau} 
        \infer1[TFix]{\Gamma; \Delta &\vdash \mu x. e : \tau}
      \end{prooftree}
      & \qquad &
    \end{array}
    \end{math}
    #+end_figure
    
*** Hindley-Milner Type System
    *TODO: Proof read*
    
    The simply-typed lambda calculus (STLC) is possibly the simplest possible
    type system, consisting of ground terms and functions only. System F is an
    extension of the STLC, adding /polymorphism/, where values can have multiple
    types.

    /Type inference/ is the assignment of types to expressions such that the
    expression type checks. Whilst there are arguments for and against type
    inference, when types are difficult to express, the option to elide them is
    helpful. Unfortunately, type inference for System F is undecidable
    cite:null.

    To overcome this problem, Hindley and later Milner described a type system
    with /parametric polymorphism/. Values either have a monotype, or a
    polytype. A monotype is a regular STLC type. A polytype is an abstraction
    over a monotype by adding in type variables, which are placeholders for
    arbitrary monotypes. For example, the generic identity function has the
    polytype \( \forall \alpha. \alpha \to \alpha \).

    A key part of the HM type system is /specialisation/. This is the
    instantiation of one or more free variables in a polytype. The relation
    \( \sigma \sqsubseteq \sigma' \) holds if \(\sigma\) can specialise to
    \(\sigma'\).

    The syntax and type judgement for the HM type system is given in figure
    [[fig:hm-type]]. Notice how HMVar specialises types. Conversely, only HMLet can
    /generalise/ types -- monotypes with free variables become polytypes.

    #+label: fig:hm-type
    #+name: fig:hm-type
    #+caption: HM syntax and typing judgement
    #+begin_figure
    \begin{align*}
      e &= x \mid e e \mid \lambda x. e \mid \Let x = e \In e \\
      \tau &= \alpha \mid \tau \to \tau \\
      \sigma &= \tau \mid \forall \alpha. \sigma
    \end{align*}
    \begin{math}
    \begin{array}{ccc}
    \begin{prooftree}
      \hypo{\sigma \sqsubseteq \tau}
      \infer1[HMVar]{\Gamma, x : \sigma \vdash x : \tau}
    \end{prooftree}
    & \qquad &
    \begin{prooftree}
      \hypo{\Gamma \vdash e : \tau \to \tau'}
      \hypo{\Gamma \vdash e' : \tau}
      \infer2[HMApp]{\Gamma \vdash e e' : \tau'}
    \end{prooftree}
    \\ & \qquad & \\
    \begin{prooftree}
      \hypo{\Gamma, x : \tau \vdash e : \tau'}
      \infer1[HMAbs]{\Gamma \vdash \lambda x. e : \tau \to \tau'}
    \end{prooftree}
    & \qquad &
    \begin{prooftree}
      \hypo{\Gamma \vdash e : \tau}
      \hypo{\Gamma, x : \forall \alpha. \tau \vdash e' : \tau'}
      \infer2[HMLet]{\Gamma \vdash \Let x = e \In e' : \tau'}
    \end{prooftree}
    \end{array}
    \end{math}
    #+end_figure

    By restricting introduction of polymorphism to \( \Let \) statements only,
    type inference is not only possible, but is nearly linear is almost all
    cases. The inference algorithm, called the /J algorithm/ is usually given in
    tree form, as in figure [[fig:hm-infer]]. 

    #+label: fig:hm-infer
    #+name: fig:hm-infer
    #+caption: The J algorithm
    #+begin_figure
    \begin{prooftree*}
      \hypo{\tau = inst(\sigma)}
      \infer1[JVar]{\Gamma, x : \sigma \vdash x : \tau}
    \end{prooftree*}
    \begin{prooftree*}
      \hypo{\Gamma \vdash e : \tau}
      \hypo{\Gamma \vdash e' : \tau'}
      \hypo{\tau'' = newvar()}
      \hypo{unify(\tau, \tau' \to \tau'')}
      \infer4[JApp]{\Gamma \vdash e e' : \tau''}
    \end{prooftree*}
    \begin{prooftree*}
      \hypo{\tau = newvar()}
      \hypo{\Gamma, x : \tau \vdash e : \tau'}
      \infer2[JAbs]{\Gamma \vdash \lambda x. e : \tau \to \tau'}
    \end{prooftree*}
    \begin{prooftree*}
      \hypo{\Gamma \vdash e : \tau}
      \hypo{\Gamma, x : \forall \alpha. \tau \vdash e' : \tau'}
      \infer2[JLet]{\Gamma \vdash \Let x = e \In e' : \tau'}
    \end{prooftree*}
    #+end_figure

    Instead of performing specialisation, JVar instead returns a general
    instance of a polytype. All the bound type variables are instantiated by a
    fresh generic type instance.

    Specialisation is then performed by JApp. The \(unify\) function coerces
    both arguments to their join, or type inference fails if the join doesn't
    exist. Recall that the join of two values is the least value greater than
    them both. Therefore \(unify\) performs the least amount of specialisation
    to give both arguments the same shape.

    Variations of the HM type sytem are used by many functional programming
    languages, such as ML and Haskell cite:null. 
** Requirements Analysis
   My core deliverable focused on implementing the \ky{} type system. Having a well
   typed language description is nearly useless without a way to parse the
   language. Hence another core component was to /output a chewed parser/. These
   two components could then be used to create a parser from any Nibble
   description.

   *TODO: rewrite*
   
   One major feature of parser combinators is their composition into
   higher-order combinators. The \ky{} type system cannot directly type check these
   higher-order combinators and must first perform some evaluation down to
   combinators represented by \mres{}. This can lead to an exponential increase in
   size of \(mu\)-regular expressions. I attempt to eliminate this issue by
   /exploring adding functions and lambda expressions/ to \mres{} and to Nibble.

   There are many other ways Nibble could be extended. *TODO: list them*
** Starting Point
   I closely studied the \ky{} type system before beginning the project. I did not
   begin any work on possible extensions to it.

   The project builds on ideas about formal languages. These have been studied
   in the /Part IA Discrete Maths/ and /Part IB Compiler Construction/ courses.
   I also did a small personal project on them during the summer of 2018.

   Additionally, the project uses concepts from type systems, covered in the
   /Part IB Semantics of Programming Languages/, /Part II Types/ and /Part II
   Denotational Semantics/ courses.
** Software Engineering
*** Project Management
    After successful development of an initial core, extensions to a programming
    language naturally tend themselves to an iterative approach. Whilst you are
    mindful of future extensions, you work towards successful implementation of
    one at a time.

    This lends itself to the spiral development model. Each component follows a
    waterfall development cycle --- design, implementation, integration and
    testing --- and no two components are developed concurrently.

    *NOTE: the rest of this section could be cut*

    This model has several other benefits. At the end of each cycle, there is a
    functional deliverable. This means that even when there are unexpected
    delays in implementing a component, there is still a functional product to
    fall back on.

    Additionally, there is a lot of flexibility in what components are
    implemented and in what order. As you work on a product, you come to better
    understand what features can be added and the cost of doing so. *TODO: More
    words here*
*** Version Control
    I used git as a version-control and revision history system. New features
    were developed on individual branches. Upon completion, they were merged
    with the main branch.

    The git repository was mirrored on both a privately-owned server and GitHub.
    Regular commits and pushes ensured that very little data was lost if there
    was an issue with my device.

    The project is dual-licensed under the MIT and Apache 2.0 licenses, as is
    common for projects written in Rust. These are permissive licenses that
    encourage development whilst limiting personal liability.
*** Development Tools
    The standard Rust build system is called Cargo. It provides an easy way to
    run several kinds of checks against the whole code base. In particular
    clippy is a static analysis tool that highlights some style improvements and
    common bugs. Also, rustfmt was regularly used to consistently format code.
    
    Some tests were performed using Rust's built-in test harness. This allows
    the user to write unit tests anywhere. It also provides a method of
    performing integration tests.

    Benchmarks were written using  criterion. This micro-benchmarking library
    measures the performance of a function by measuring thousands of iterations.
    It also provides some simple statistical analysis and comparisons between
    functions.
* Implementation
  There are two areas of implementation for this project. The first is the
  design of Nibble, which is a DSL (section [[*Domain Specific Languages]]) for
  describing formal languages made to fix the problems of \mres{} as explored in
  section [[*\mres{}]]. It also describes the theoretical implementation of two type
  systems for Nibble. The second explores the implementation of Chomp, a parser
  generator implemented in Rust. Chomp takes Nibble expressions and, for
  well-typed expressions, produces Rust source code for a parser of the formal
  language described by the Nibble expression.

  We start in section [[*The Nibble Language]] by introducing the syntax and
  semantics of Nibble, explaining how it solves the problems of \mres{}. Then in
  section [[*Type Systems for Nibble]], we describe the design of two type systems
  for Nibble: \ky{}-\lambda in section [[*The \ky{}-\lambda Type System]] and LM in
  section [[*The LM Type System]].

  Next we move on to describing Chomp, starting with the structure of its code
  repository and overall architecture in section [[*Chomp Repository Overview]].
  Chomp has an architecture similar to other compilers and translators (section
  [[*Translators]]). The front-, middle- and back-ends of Chomp are described in
  sections [[*The Front-End: Parsing and Normalisation]], [[*The Middle-End: Type
  Inference]], and [[*The Back-End: Code Generation]] respectively.

** The Nibble Language
   Nibble is a DSL for describing formal languages. Semantically, a Nibble
   expression represents a formal language. Nibble is designed to be a
   user-friendly alternative to \mres{}. In section [[*\mres{}]], we discussed some
   issues with \mres{} that made them impractical to describe non-trivial
   languages. Nibble solves these problems by introducing let statements and
   lambda abstractions.

   For Nibble expressions to be a suitable replacement for \mres{}, Nibble must
   be able to describe the same set of languages. Nibble achieves this by
   directly embedding \mres{}. Listing [[lst:nibble-embeds-mu]] shows how Nibble
   embeds the \mre{} from figure [[fig:mu-baa]].

   #+label: lst:nibble-embeds-mu
   #+name: lst:nibble-embeds-mu
   #+caption: Nibble expressions embed \mres{}.
   #+begin_src rust
   match "b".!(/x/ (_|"a".x))
   #+end_src

   *TODO: Is this the best place for this paragraph? Could move to section
   [[*\mres{}]]*
   
   The first difference between Nibble and \mres{} is that Nibble does not embed
   \(\bot\). By itself, \(\bot\) has no practical use -- there is no need to
   parse the empty language. When combined with other combinators, \(\bot\) is
   either an annihilator or the identity *(TODO: link to figure)*. This means
   any \mre{} containing \(\bot\) is either semantically equivalent to \(\bot\),
   or semantically equivalent to an \mre without \(\bot\).

   In the current form, Nibble doesn't solve any of the problems with \mres{}.
   The first step to solving the repetition issue is to introduce let
   expressions. Listing [[lst:nibble-let-expression]] demonstrates how Nibble can
   eliminate the simple repetition from [[fig:mu-hex-colour]].

   #+label: lst:nibble-let-expression
   #+name: lst:nibble-let-expression
   #+caption: Nibble has let statements to introduce variables.
   #+begin_src rust
     let hex = "0"|"1"|"2"|"3"|"4"|"5"|"6"|"7"|"8"|"9"
             | "a"|"b"|"c"|"d"|"e"|"f"
             | "A"|"B"|"C"|"D"|"E"|"F";
     match "#".hex.hex.hex.hex.hex.hex.(_|hex.hex)
   #+end_src

   A let statement introduces a new variable name, the binding variable, and
   assigns it a Nibble expression, the bound expression. In this case, the
   variable ~hex~ is assigned to a hexadecimal character. The variable can be
   used repeatedly in following statements.

   *TODO: Is this too tangential for this section? Maybe the evaluation is a
   better fit...*

   Unlike BNF which, as we discussed in section [[*BNF]], has a mutually-recursive
   global namespace, Nibble's let statements use non-recursive lexical scoping.
   This means that a Nibble expression can only refer to variables in previous,
   complete let statements. Listing [[lst:nibble-lexical-scope]] shows two invalid
   variable uses.

   #+label: lst:nibble-lexical-scope
   #+name: lst:nibble-lexical-scope
   #+caption: Two Nibble let statements that violate the lexical scoping rules
   #+begin_src rust
     // Forbidden because `second` is defined after `first`.
     let first = "a" . second;
     let second = "b";

     // Forbidden because `rec` cannot refer to itself.
     let rec = _ | "a".rec;
   #+end_src

   Whilst let statements can eliminate verbatim repetition, they do not help
   with repetitive patterns, like we saw in figure [[fig:mu-many-plus]]. Nibble
   introduces lambda abstractions, which are demonstrated in [[lst:nibble-lambda]].

   #+label: lst:nibble-lambda
   #+name: lst:nibble-lambda
   #+caption: Nibble has lambda abstractions to eliminate repetitive patterns.
   #+begin_src rust
     let opt = /x/ _ | x;
     let plus(x) = !(/y/ x.opt y);
     match plus "a".plus "b".plus "c"
   #+end_src
   
   There are two ways to introduce a lambda abstraction: either through a lambda
   expression ~/x/ e~, or through a let-lambda statement ~let x(y) = e~.

   *TODO: move sugar to normalisation in section [[*The Front-End: Parsing and
   Normalisation]]?*

   The let-lambda statements are /syntactic sugar/ for a let-statement binding a
   lambda-expression. They are indistinguishable in their semantics and how they
   are type checked.

   Notice how in line two, the fixed point operator ~!~ takes the lambda
   expression as an argument. In general, the fixed point operator can accept
   any expression than evaluates to a first-order function. Whilst not
   demonstrated here, functions can take more than one argument. *TODO: Maybe
   demonstrate instead.*

   *TODO: summary?*
** Type Systems for Nibble
   I have designed two type systems for Nibble: \ky{}-\lambda and LM. The
   \ky{}-\lambda type system is a minimal departure from the \ky{} type system
   for \mres{}, which was presented in section [[*The \ky{} Type System]], treating
   lambda abstractions as parametric macros. This is what is implemented in
   Chomp. The LM type system is a theoretical system for Nibble, incorporating
   ideas from the Hindley-Milner type system, introduced in section [[*The
   Hindley-Milner Type System]]. This adds polymorphic function types on top of
   the \ky{} type system.
   
*** The \ky{}-\lambda Type System
    The \ky{}-\lambda type system is a type system for Nibble using the \ky{}
    type system, presented in section [[*The \ky{} Type System]], as the core. By
    treating lambda abstractions as parametric macros, Nibble expressions are
    reduced into embedded \mres{} using substitution. This \mre{} is type
    checked using the \ky{} type system.

    For a Nibble expression ~e~, we denote the reduction of ~e~ as \( \llbracket
    \mathtt{e} \rrbracket \). *Full definition is where?* To summarise,
    reduction performs call-by-name evaluation of Nibble expressions. The only
    exception is the fixed-point operator, ~!e~. This first reduces the argument
    ~e~. If ~e~ reduces to ~/x/ f~, then ~f~ is reduced, keeping ~x~ free.
    Otherwise, reduction fails.

    *TODO: include examples*

    There are some problems with this approach. Firstly, call-by-name evaluation
    of untyped terms is non-terminating. Consider listing [[lst:omega]]. The
    expression ~omega omega~ reduces to ~omega omega~. Users might be confused
    by the parser generator hanging instead of producing an error.
   
    #+label: lst:omega
    #+name: lst:omega
    #+caption: Non-reducing Nibble expression.
    #+begin_src rust
      let omega(x) = x x;
      match omega omega
    #+end_src

    Another issue is that unused expressions are completely ignored. Whilst this
    has some computational benefits, it could cause confusion for users. *TODO:
    example.*

    Both of these issues can be resolved with a purely syntactic type system.

*** The LM Type System
    *TODO: reread carefully and simplify. This whole section assumes prior
    knowledge readers won't have.*
    
    The LM type system is a purely-syntactic type system for Nibble. It combines
    features of the Hindley-Milner type system discussed in section [[*The
    Hindley-Milner Type System]] with the core of the \ky{} type system, presented
    in section [[*The \ky{} Type System]]. This allows for expressions bound by let
    statements to have polymorphic types, and removes the need to reduce
    expressions before type checking.

    Unlike all previous type systems, the variable contexts store both types and
    constraints. Constraints are relations between types in the \ky{} type
    system. They need to be stored in the variable context because these
    relations are not always decidable for type variables.
    
    The LM typing judgement has the form \( \Gamma; \Delta \vdash \mathtt{e} : \tau ; C
    \). This can be read: under unguarded variable context \Gamma and guarded
    context \Delta, the Nibble expression ~e~ has the type \tau given the
    constraints in \(C\) hold. Thus, type checking an expression becomes a
    two-step process: infer a type; then check the constraints hold.

    Figure [[fig:lm-type-rules]] shows the typing rules for the LM type system. We
    first talk through the various rules, before showing some example
    inferences. Looking at those examples may help with understanding these
    rules. 

    #+label: fig:lm-type-rules
    #+name: fig:lm-type-rules
    #+caption: Typing rules for the LM type system
    #+begin_figure
    \centering
    *TODO: some odd typesetting going on in here.*

    *TODO: define bar and substitutions.*
    
    \begin{math}
    \begin{array}{ccc}
      \begin{prooftree}
        \infer0{\Gamma; \Delta \vdash \mathtt{\_} : K(\tau_\epsilon); \emptyset}
      \end{prooftree}
      &&
      \begin{prooftree}
        \infer0{\Gamma; \Delta \vdash \mathtt{``cw"} : K(\tau_c); \emptyset}
      \end{prooftree} \\
      && \\
      \begin{prooftree}
        \hypo{\sigma = S\rho}
        \hypo{C' = S C}
        \infer2{\Gamma, x : (\rho, C); \Delta \vdash \mathtt{x} : \sigma; C'}
      \end{prooftree}
      && \\
      && \\
      \begin{prooftree}
        \hypo{\Gamma; \Delta \vdash \mathtt{e} : K(\tau); C}
        \hypo{\Gamma, \Delta; \cdot \vdash \mathtt{e'} : K(\tau'); C'}
        \infer2{\Gamma; \Delta \vdash \mathtt{e . e'} : K(\tau \cdot \tau'); C \cup C' \cup \{ \tau \circledast \tau' \}}
      \end{prooftree}
      &&
      \begin{prooftree}
        \hypo{\Gamma; \Delta \vdash \mathtt{e} : K(\tau); C}
        \hypo{\Gamma; \Delta \vdash \mathtt{e'} : K(\tau'); C'}
        \infer2{\Gamma; \Delta \vdash \mathtt{e | e'} : K(\tau \vee \tau'); C \cup C' \cup \{ \tau \# \tau' \}}
      \end{prooftree} \\
      && \\
      \begin{prooftree}
        \hypo{\Gamma, x : (\sigma, \emptyset); \Delta \vdash \mathtt{e} : \sigma'; C}
        \infer1{\Gamma; \Delta \vdash \mathtt{/x/ e} : \sigma \to \sigma'; C}
      \end{prooftree}
      &&
      \begin{prooftree}
        \hypo{\Gamma; \Delta, x : (\sigma, \emptyset) \vdash \mathtt{e} : \sigma'; C}
        \infer1{\Gamma; \Delta \vdash \mathtt{/x/ e} : \sigma \leadsto \sigma'; C}
      \end{prooftree} \\
      && \\
      \begin{prooftree}
        \hypo{\Gamma; \Delta \vdash \mathtt{e} : \sigma \to \sigma'; C}
        \hypo{\Gamma; \Delta \vdash \mathtt{e'} : \sigma; C'}
        \infer2{\Gamma; \Delta \vdash \mathtt{e e'} : \sigma'; C \cup C'}
      \end{prooftree}
      &&
      \begin{prooftree}
        \hypo{\Gamma; \Delta \vdash \mathtt{e} : \sigma \leadsto \sigma'; C}
        \hypo{\Gamma, \Delta; \cdot \vdash \mathtt{e'} : \sigma; C'}
        \infer2{\Gamma; \Delta \vdash \mathtt{e e'} : \sigma'; C \cup C'}
      \end{prooftree} \\
      && \\
      \begin{prooftree}
        \hypo{\Gamma; \Delta \vdash \mathtt{e} : K(\tau) \leadsto K(\tau); C}
        \infer1{\Gamma; \Delta \vdash \mathtt{!e} : K(\tau); C}
      \end{prooftree}
      &&
      \begin{prooftree}
        \hypo{\Gamma; \Delta \vdash \mathtt{e} : \sigma; C}
        \hypo{\Gamma; \Delta, x: (\bar{\sigma}, \bar{C}) \vdash \mathtt{e'} : \sigma'; C'}
        \infer2{\Gamma; \Delta \vdash \mathtt{let x = e; e'} : \sigma'; C'}
      \end{prooftree} \\
      && \\
    \end{array}
    \end{math}
    #+end_figure

    The epsilon and literal rules are the simplest LM typing rules, being
    largely unchanged from the \ky{} type system. The only differences are that
    the type is wrapped in a \(K\) constructor, and they both return an empty
    set of constraints.

    *TODO: wording*
    
    The variable rule is a combination of the rules from the Hindley-Milner and
    \ky{} type systems. First, the variable must be unguarded. Second, the
    output type is a specialisation of the type from the context. In addition to
    specialising the type, we also specialise the constraints.

    Concatenation and alternation remain similar to the \ky{} type system. Like
    the epsilon and literal rules, all the types are wrapped in a \(K\)
    constructor. Instead of the constraints appearing in the premise, as they do
    in the \ky{} type system, they are moved to the conclusion, where they can
    be checked later.

    Notice how there are two different typing rules for lambda abstraction. This
    is due to the two variable contexts from the \ky{} type system. One function
    type is for unguarded functions, where the formal parameter can be used in
    an unguarded context. The other function type is for guarded functions,
    where the formal parameter can only be used in guarded contexts. Because
    lambda expressions are monomorphic, type constraints pass straight through.

    Again due to the presence of two function types, there are two typing rules
    for application. If the called function is an unguarded function type, then
    the argument is evaluated in the same context. If the function is a guarded
    function, then the argument is evaluated in an unguarded context -- the
    function body ensures the parameter only appears on the right side of a
    concatenation, so all variables are accessible.

    *TODO: add guard < unguard to the specialisation rules instead?*
    
    The subsumption rule allows guarded functions to be used when an unguarded
    function was expected. This is safe due to an extension of the transfer
    property of the \ky{} type system *(TODO: link to transfer)*. 

    The fixed-point typing rule only accepts first-order guarded functions as
    the argument. Whilst fixed-points could accept higher-order functions, doing
    so would allow non-terminating reductions in Nibble. To prevent the
    left-factoring problem, described in section [[*Left Factoring]], the function
    must be guarded.

    Let expressions are taken from the Hindley-Milner type system almost
    directly. Note that bound variables are always unguarded. *Why?* This is also the
    only typing rule that adds constraints to variables in the context. A
    consequence of this typing rule is that constraints \(C\) on the bound
    expression are only checked if ~x~ is used in the body.

**** Examples
     
    *TODO: include examples, especially for function types*

    *TODO: ~let bad(x) = "a" | "a"; match "b"~*
    
    *TODO: subsumption rule*
       
#     Finally we consider type inference. Types in the \ky{} type system form an
#     algebra. For example, \( \alpha \cdot \tau_\epsilon = \alpha \) for all
#     types \( \alpha \). Because of this algebraic nature, it is difficult to
#     determine whether two types are equal. For instance, do we have \( ((\alpha
#     \vee \beta) \cdot \gamma) \cdot \delta = (\alpha \cdot (\gamma \cdot
#     \delta)) \vee ((\beta \cdot \tau_\epsilon) \cdot (\gamma \cdot \delta))
#     \)[fn:: Yes]? Introducing fixed points only makes determining type equality
#     more difficult.

#     Recall that unification takes two types and instantiates type variables
#     until they are equal. Given that equality is so complex, how can we unify
#     two variables efficiently? The solution is to only use structural equality
#     for unification. Whilst it will reject some otherwise well-typed Nibble
#     expressions, using structural equality should have a huge performance
#     benefit. In any case, for this small set of rejected expressions, there will
#     be a Nibble expression with an equivalent language.
** Chomp Repository Overview
   Chomp is a parser generator from Nibble to Rust. Chomp is implemented in
   Rust, so it can utilise the procedural macro system, described in section
   [[*Procedural Macros]]. Table [[tbl:overview]] gives a brief description of the
   repository structure for Chomp.

   #+label: tbl:overview
   #+name: tbl:overview
   #+caption: Brief outline of the code repository structure.
   #+attr_latex: :float t
   | Path                   | Description                             | Lines of Code |
   |------------------------+-----------------------------------------+---------------|
   | ~src/nibble~           | Nibble parser and normalisation         |           676 |
   | ~src/chomp~            | Chomp type inference algorithm          |          1676 |
   | ~src/lower~            | Chewed parser code generation           |           420 |
   | ~chewed~               | Shared library for all chewed parsers   |           270 |
   | ~chomp-macro/src~      | Procedural macro interface              |            41 |
   | ~chomp-macro/tests~    | Minimal end-to-end tests of Chomp       |           123 |
   | ~autochomp/src~        | AutoChomp implementation                |           570 |
   | ~autochomp/tests~      | Tests for correctness of AutoChomp      |           ??? |
   | ~autochomp/benches~    | Benchmarks of Chomp and AutoChomp       |           167 |
   | ~chomp-bench/**/json~  | Benchmarks of various parsers for JSON  |           430 |
   | ~chomp-bench/**/ascii~ | Benchmarks of various parsers for ASCII |           227 |

   The ~src~ directory contains the main Chomp library. It is split into three
   parts. The front end is in the ~src/nibble~ directory. This parses an input
   stream of Nibble expressions and produces Chomp's IR, an abstract syntax
   tree. The middle end is in the ~src/chomp~ directory. This performs type
   inference using the \ky{}-\lambda type system. It outputs a typed \mre{}. The
   back end is in the ~src/lower~ directory. Its responsible for converting the
   typed \mre{} into Rust source code for a parser.
   
   The output of Chomp is called a /chewed parser/. All these parsers share some
   common code, such as the trait definition (introduced in section [[*Traits]]) for
   a parser. This common code is kept in the ~chewed~ library. The reason it
   forms a separate library is so that consumers of Chomp only need to include
   the small ~chewed~ library with their final binary, instead of the relatively
   large Chomp library.
   
   To help make Chomp easier for developers to include in their projects, a
   procedural macro interface was created. Due to current limitations in Rust,
   this interface has its own, small library in the ~chomp-macro~ directory.
   This library also includes a few basic end-to-end tests for Chomp.
   
   The success criterion for this project required bootstrapping Chomp --
   replacing the original front-end with a chewed parser. This variant of Chomp,
   dubbed /AutoChomp/, is in the ~autochomp~ directory. This directory includes
   some simple tests of the correctness of AutoChomp, as well as some benchmarks
   to compare the relative performance of the handwritten and chewed parsers.
   
   Finally, there is the ~chomp-bench~ directory. This is a small library for
   comparing the performance of chewed parsers against a handwritten and a
   different generated parser. Ideally, this library would be part of
   ~chomp-macro~. However, limitations in the build system for the other
   generated parser makes this impossible.
** The Front-End: Parsing and Normalisation
   The front-end of Chomp is responsible for converting the input stream of
   characters representing a Nibble expression into Chomp's IR. This occurs in
   three stages. The first is lexing, where characters in the input stream are
   split into different tokens. The second is parsing, where this token stream
   is transformed into a concrete syntax tree. Finally, normalisation converts
   this concrete syntax tree into an abstract syntax tree.
   
   In Chomp, the lexing is performed by the ~syn~ library. This can convert
   streams of characters into tokens from the Rust language. Nibble uses a
   subset of the tokens found in Rust, so lexing into Rust tokens makes the
   parser simpler. It also makes integration with procedural macros
   significantly easier, because procedural macros receive a stream of Rust
   tokens as input. *TODO: link back to procedural macros?*
   
   The parser in Chomp also uses the ~syn~ library. It provides lightweight
   interface to parse some often-used data structures. For example, it provides
   the ~Punctuated<T, P>~ type, which represents a list of values of type ~T~,
   separated by values of type ~P~. *TODO: discuss more interesting parsing.*
   
   Listing [[lst:parse-term]] shows how Chomp parses a Nibble term. This function
   makes use of Rust's type inference and trait systems to call one of four
   different functions, all written as ~input.parse()~. By checking what the
   next input token is, it is possible to determine exactly what type of term
   can appear.
   
   #+label: lst:parse-term
   #+name: lst:parse-term
   #+caption: Code snippet from Chomp that parses a Nibble term.
   #+begin_src rust
     pub enum Term {
         Epsilon(Epsilon),
         Ident(Ident),
         Literal(Literal),
         Fix(Fix),
         Parens(ParenExpression),
     }

     impl Parse for Term {
         fn parse(input: ParseStream<'_>) -> syn::Result<Self> {
             let lookahead = input.lookahead1();

             if lookahead.peek(Token![_]) {
                 input.parse().map(Self::Epsilon)
             } else if lookahead.peek(LitStr) {
                 input.parse().map(Self::Literal)
             } else if lookahead.peek(Token![!]) {
                 input.parse().map(Self::Fix)
             } else if lookahead.peek(Paren) {
                 input.parse().map(Self::Parens)
             } else if lookahead.peek(Ident::peek_any) {
                 input.call(Ident::parse_any).map(Self::Ident)
             } else {
                 Err(lookahead.error())
             }
         }
     }
   #+end_src
   
   Finally there is normalisation. This converts the concrete syntax tree into
   an abstract syntax tree. This occurs in two stages that occur simultaneously.
   First, syntactic sugar is expanded. Second, variable names are converted to
   De Bruijn indices (introduced in section [[*De Bruijn Indices]]).
   
   In section [[*The Nibble Language]], we introduced let-lambda statements as
   syntactic sugar for a let statement binding a lambda expression. During
   normalisation, we convert let-lambda statements into this expanded form,
   instead of keeping them around as another node type in the abstract syntax
   tree. This reduces complexity in later stages of Chomp.
   
   The other part of normalisation is conversion to De Bruijn indices. Most of
   this work is achieved by the ~Context~ struct, shown in listing
   [[lst:parse-ctx]].
   
   #+label: lst:parse-ctx
   #+name: lst:parse-ctx
   #+caption: Code snippet from Chomp that provides conversion to De Bruijn indices.
   #+begin_src rust
     pub struct Context {
         bindings: Vec<Name>,
     }

     impl Context {
         /// Get the De Bruijn index of `name`, if it is defined.
         pub fn lookup(&self, name: &Name) -> Option<usize> {
             self.bindings
                 .iter()
                 .rev()
                 .enumerate()
                 .find(|(_, n)| *n == name)
                 .map(|(idx, _)| idx)
         }

         /// Permanently add the variable `name` to the top of the stack.
         pub fn push_variable(&mut self, name: Name) {
             self.bindings.push(name);
         }

         /// Call `f` with the variable `name` pushed to the top of the stack.
         pub fn with_variable<F: FnOnce(&mut Self) -> R, R>(
             &mut self, 
             name: Name, 
             f: F,
         ) -> R {
             self.bindings.push(name);
             let res = f(self);
             self.bindings.pop();
             res
         }
     }
   #+end_src
   
   As discussed in section [[*De Bruijn Indices]], when we introduced De Bruijn
   indices, variables in lexically-scoped languages form a stack. The
   most-recently declared variable is at the top of the stack. New variables are
   pushed on top of the stack, and popped off when they leave scope.
   
   There are two ways to introduce variables in Nibble. The binding variables
   from let statements are in scope for the rest of the Nibble expression. The
   formal parameters of lambda expressions are in scope only for the body of the
   lambda expression. This is reflected by the two different ways to push
   variables to the stack.
   
   ~push_variable~ adds a variable onto the stack of ~bindings~ in a ~Context~
   permanently. The API provides no way to remove variables. This is called
   by let statements, after converting the bound expression but before
   converting their body. The ~with_variable~ method pushes a variable onto the
   stack only for the duration of a call to the function ~f~. This is used by
   lambda expressions, where ~f~ will convert the lambda body.
   
   The ~lookup~ method does all of the heavy-lifting. *TODO: explain how.*

** The Middle-End: Type Inference
   The middle end of Chomp performs type inference using the \ky{}-\lambda type
   system, to produce a typed \mre{}. First, the abstract syntax tree computed
   by the front-end is reduced. Next, its type is inferred using the \ky{} type
   system, and the output expression is built.
   
   Both reduction and the type inference algorithm use the visitor design
   pattern. *TODO: explain the visitor pattern.*
   
   Recall from section [[*The \ky{}-\lambda Type System]] how reduction of Nibble is
   essentially call-by-name evaluation. The reduction process can be simplified
   to three steps: find a function call; substitute in the unreduced arguments;
   reduce the result. A single visitor, called ~Reduce~, can both find function
   calls and reduce the results. Substitution requires a different visitor,
   which searches for the relevant variable and replaces it with the target
   expression. Care needs to be taken when passing through a lambda expression
   to rename free variables correctly. This can be done with a third Visitor.
** The Back-End: Code Generation

# *** Parsing
#     Chomp uses a parser framework called ~syn~. The parser takes a stream of
#     lexical tokens and produces a concrete parse tree. This is then converted
#     into an abstract syntax tree, which is used by the type inference stage.
    
#     The parser was written by hand. Nibble is a small language, designed to be
#     easily parsed. This greatly simplifies the task of writing a parser for it.

#     *example: parsing concatenation?*

#     After parsing to the concrete syntax tree, the input stage then normalises
#     it to an AST. This has two parts, which occur concurrently. One is
#     desugaring, which is the elimination of syntactic shortcuts; and the other
#     is introduction of De Bruijn indices, which is a way of naming variables.

#     Syntactic sugar is part of a language that doesn't add any expressive power,
#     and only makes it easier to read. For example, the Nibble expression ~let
#     opt(x) = _|x; ...~ is semantically equivalent to ~let opt = /x/ _|x; ...~.
#     The let-combinator syntax is redundant, but makes it easier to determine if
#     a let expression is a combinator or a full language description. Desugaring
#     expands all the syntactic sugar into the base expressions.

#     *TODO: segue.*
#     In lexically-scoped languages, variable bindings form a stack. Take the
#     Nibble expression ~/x/ (/y/ y x) (/z/ z)~. ~x~ is bound first, then ~y~ is
#     bound and unbound, and finally ~z~ is bound and unbound. Also note that the
#     names of variables have little impact.

#     Making use of these two observations, we can arrive at De Bruijn indices,
#     where variables are referenced by their position from the top of the stack.
#     Using the previous example, the expression becomes ~// (// 0 1) (// 0)~.
#     This helps improve the efficiency of later compilation stages, as well as
#     spotting usage of undeclared variables early.

#     *algorthim: normalisation in practice*
# *** Type Inference
#     Type inference takes an AST and produces a typed syntax tree, by assigning
#     every node a type. This section starts with a discussion about where type
#     annotations are stored. Next, it describes the visitor pattern used in the
#     type inference algorithm. Finally, it details the design of the variable
#     context used during type checking.

#     There are generally two ways to annotate a tree with types. Internal
#     annotations define a new tree type with almost identical structure to the
#     AST. The only difference is that every node also stores type information.
#     External annotations provide an external function to find the type of a
#     node, without modifying the original data structure at all.

#     *Why does Chomp use internal annotations?*
    
#     # Chomp uses internal annotations for a few reasons. Firstly, anyone should be
#     # able to create and modify an AST but only some languages should have typed
#     # syntax trees. By using two separate types, ASTs can have public
#     # constructors and typed syntax trees can have private constructors.

#     # Secondly, 

#     # Secondly, *usage in code generation.*

#     Type inference is performed using the visitor pattern. This pattern is
#     depicted in figure [[fig:visitor]]. A visitor has different methods each
#     accepting a different type of expression. An expression has a method that
#     takes a visitor and dispatches the call for the correct expression type.

#     #+label: fig:visitor
#     #+name: fig:visitor
#     #+caption: UML diagram showing the visitor pattern.
#     #+begin_figure
#     *TODO: finish figure*
#     #+end_figure

#     The visitor design pattern keeps the code for each visitor in one location,
#     instead of split across all the different expression types. This is
#     particularly important for reduction, which was also implemented using
#     the visitor pattern. Reduction requires many recursive manipulations of the
#     expression tree. Keeping each stage separate helps to verify the correctness
#     over the whole tree.

#     The visitor pattern also provides external users of Chomp an interface to
#     include their own manipulations. It can also help if Chomp is split into
#     three libraries for the front-end, middle-end and back-end.

#     *application: visitor pattern for substitution?*

#     Finally there is the variable context. The LM type system splits the
#     variable context into two parts: an unguarded context and a guarded context.
#     To further complicate things, some variables move back and forward between
#     the unguarded and guarded contexts, whilst others are always unguarded. 

#     *TODO: rewrite to describe what's implemented*
    
#     # Listing [[lst:var-context]] shows the data structure and API I used to solve all
#     # these problems. At its core, a ~Context~ is a wrapper for ~vars~, a vector
#     # of types. These types are either ~Dynamic~, going between guarded and
#     # unguarded, or ~Static~, remaining always unguarded.

#     # #+label: lst:var-context
#     # #+name: lst:var-context
#     # #+caption: Variable context structure and API.
#     # #+begin_src rust
#     # *TODO: finish figure*
#     # #+end_src

#     # To determine whether a ~Dynamic~ type is guarded requires looking at
#     # ~unguard_points~. This is a separate stack, recording the length of ~vars~
#     # when it was last unguarded. If a variable is deeper in the stack then the
#     # last index in ~unguard_points~, it is unguarded. Otherwise, it is only
#     # unguarded if it is ~Static~.

#     # *TODO: proof read. Very fumbly*
    
#     # There are four main functions in the API for ~Context~. ~get_variable_type~
#     # is a fallible way of retrieving a variable, returning an error if the
#     # desired variable is guarded. ~with_unguard~, ~with_unguarded_type~ and
#     # ~with_guarded_type~ unguard the context, introduce a new unguarded type and
#     # introduce a new guarded type respectively. Each of them takes a function
#     # argument. Due to the nature of stacks, each one pushes a value, calls the
#     # supplied function, and then pops the value before returning the result.
#     # Because Rust is an optimising compiler, it is likely each of these calls
#     # will be fully inlined, resulting in no size increase to the call stack.
    
# *** Code Generation
#     Code generation transforms a typed syntax tree into Rust tokens describing a
#     parser. This requires tree parts: translation, type generation and parser
#     implementation. First, the tree is translated into an \mre{}. This avoids
#     the problem of representing functions as data types. Type generation defines
#     all the data types produced by the parser. By using Rust's zero-sized types,
#     it is possible to achieve very small storage footprints whilst using rich
#     types. The implementation describes how to parse each data type. Rust's
#     trait system aids this process.

#     Ideally, the translation step would be unnecessary. Instead, Rust's generics
#     would allow parametric data types and parser implementations. There are two
#     practical problems with this. First, generic type arguments cannot be given
#     their own type arguments. Consider the function ~/f/ f "a"~. Ideally, this
#     would have the datatype declaration ~type Foo<F> = F<A>;~. Unfortunately,
#     the Rust compiler rejects the type argument ~<A>~ on ~F~. Secondly, generic
#     type arguments cannot provide compile-time constants to the generic type.
#     Consider the alternation ~"a"|"b"~. To get the full performance benefit of
#     type-checked parser combinators, the branching conditions, i.e. the first
#     sets, for each alternative need to be known at compile time. For a generic
#     implementation ~Alt<A, B>~, there is no way to get this information as a
#     compile-time constant.

#     *TODO: talk about other alternatives?*
    
#     Recall reduction used by the \ky{}-\lambda type system, in section
#     [[*\ky{}-\(\lambda\) Type System]]. There, reduction stopped when the top-level
#     expression was not a let expression or lambda expression. Translation
#     instead continues the reduction process until the result has no more lambda
#     expressions, let expressions, or function calls. The resulting expression
#     can be trivially translated into a \mre{}, as every other component of
#     Nibble expressions has a corresponding component in \mres{}. *example*
    
#     One downside of this approach is that it creates significantly more code
#     than generics would. This could increase the compilation time of the chewed
#     parser. Naming types also becomes more challenging. Every instance of a
#     *let-lambda* expression creates a new datatype, that all want the same name.
#     Chomp solves this by adding a unique number to the end of each type. This
#     can make discovering the correct type to use more difficult that generics
#     would.

#     *Segue.*
#     A zero-sized type is a data type that has exactly one instance. For example,
#     the unit type ~()~ is zero-sized. Because there is only a single instance, a
#     value of a zero-sized type has no information. This means the Rust compiler
#     does not need to store values of this type, reducing the memory footprint of
#     various data structures. Chomp exploits zero-sized types by translating
#     epsilon expressions and literal expressions into zero-sized types. The
#     language of an epsilon statement is the empty string, so it only has one
#     instance. Similarly, each literal string can be parsed in exactly one way,
#     so they also only have one instance. This means literal strings and epsilon
#     statements require no storage in the final parse tree.

#     Concatenation expressions are translated into structures -- records of
#     fields. Whilst the \ky{} type system uses binary concatenation, Nibble uses
#     \(n\)-ary concatenation, to match real-world usage. *Example.* 

#     Alternation expressions are translated into enumerated types. Similar to
#     concatenation, the generated enumerations are \(n\)-ary instead of binary.
#     From experience gained writing various tests, it is much more pleasant for
#     the user to only consider one, large alternation than many small nested
#     ones. *Example*

#     *TODO: relevance?* 
    
#     By default, Rust stores all values on the stack. The size of values on the
#     stack must be known at compile time, so the compiler can allocate enough
#     space for them. These two facts mean that Rust does not allow
#     directly-recursive data types. *Example: linked list.* This list could store
#     zero, one, or more items on the stack, each giving a value with a different
#     size. To allow recursion, a recursive reference has to be indirect, for
#     example with an owned heap-reference like ~Box~.

#     *TODO: describe as implemented*
   
#     Now we know how Nibble expressions are converted into data types. Next we
#     explore how they are parsed.

#     Listing [[lst:parse-def]] gives the core of the definition of the ~Parse~ trait,
#     used to parse streams of characters into the data types we made earlier.
#     ~take~ removes characters from the stream to produce an instance of the
#     type, until the next character is not in the first set of the expression's
#     type. For instance, the ~take~ method for epsilon expressions always
#     succeeds, leaving the stream alone. The ~take~ method for literal
#     expressions succeeds if and only if the stream starts with that literal
#     string. *example*.

#     #+label: lst:parse-def
#     #+name: lst:parse-def
#     #+caption: ~Parse~ trait definition
#     #+begin_src rust
#       fixme!()
#     #+end_src

#     Because the expression is well-typed, we know that any fixed-point recursion
#     is guarded. Therefore fixed-point recursion is unchecked. Likewise, because
#     first and flast sets are disjoint for concatenation expressions, the prefix
#     can greedily take characters from the stream before the suffix does. For
#     alternations, we know that the first sets for each alternative are disjoint,
#     so we can easily chose which alternative to parse.

#     *example: generated code.*

# ** Procedural Macros
#    Procedural macros are compile-time procedures that operate over Rust tokens.
#    They let users perform arbitrary transformations to Rust tokens. This can be
#    used for mundane cases, like creating debugging information, to exotic, such
#    as embedding Nibble in Rust code.

#    Tokens used in Nibble are a subset of tokens used in Rust. This design choice
#    means that Chomp could be used as a procedural macro -- it takes Rust
#    (Nibble) tokens and produces a different stream of Rust tokens. Integrating
#    Nibble into other projects then becomes simple: declare that the project uses
#    Chomp, include the ~chewed~ library, write some Nibble in a macro, and then
#    enjoy the chewed parser.
   
#    *STORY: I'm not sure what I want to say here.*

#    One key requirement for Nibble was that it can describe itself. The easiest
#    way to test this was to replace the parser in Chomp with a chewed parser.
* Evaluation
  This section starts by discussing whether the project fulfilled the original
  requirements. Next, we perform qualitative analysis of Nibble, such as
  comparing Nibble to other ways of describing languages. Finally, this section
  quantitatively compares the performance of Chomp and chewed parsers against
  other parsing and generation techniques.
  
** Meeting the Success Criterion
   Overall, I have achieved the core requirements of this project, as stated in
   section [[*Requirements Analysis]]. I have also made significant progress with
   the theory of a major stretch requirement, although implementation is
   incomplete. The rest of this section looks back at the success criterion in
   the project [[*Project Proposal]].

   First, Nibble is able to describe itself, as demonstrated in the
   ~autochomp/src/lib.rs~ file. This strongly implies that Nibble is
   sufficiently expressive to describe practical languages.

   Secondly, Chomp accepts this self-description of Nibble and produces a chewed
   parser. That Chomp produces this parser, called /AutoNibble/, means that
   Nibble is well-typed in the \ky{}-\lambda type system. Hence, Nibble is
   unambiguous and can be parsed efficiently.

   Third, Chomp can be modified to use AutoNibble in the parsing stage. The
   ~autochomp~ library produces a binary with an identical interface to Chomp,
   named /AutoChomp/ using the AutoNibble parser. This is evidence that a chewed
   parser can be used successfully in a practical system.

   Finally, AutoChomp produces identical code to Chomp. This is demonstrated by
   the tests in ~autochomp/tests/compare/~. AutoNibble and the parser from Chomp
   are used to parse Nibble expressions, and their outputs are compared. After
   the normalisation stage, Chomp and AutoChomp are identical -- only the parser
   has changed. The fact the tests pass is strong evidence the parsers are also
   identical.

** Analysis of Nibble
   In section [[*Project Overview]], I assert that Nibble was inspired by \mres{}. I
   also claim that it should be able to describe the same languages as BNF. This
   section starts by comparing Nibble to \mres{}. In particular, I determine
   whether Nibble is a practical replacement for \mres{}. Next, I compare Nibble
   against BNF, firstly for expressive power, and then for ease of use in
   untyped and typed applications.

*** Comparison with \mu-Regular Expressions
    In section [[*The Nibble Language]], I describe how to translate Nibble
    expressions into \mres{}. This translation is surjective for \mres{} that do
    not include \(\bot\) -- all \mre{} syntax, except for \(\bot\), has an
    equivalent in Nibble. In fact, all \mres{} either express the empty
    language, or there is a Nibble expression with the same language.
    
    I will use three criteria to compare how practical a programming language
    is. One programming language is more practical than another if: there is
    less repetition; the syntax is more descriptive; and there is less cognitive
    load on a programmer. Less repetition means programmers are less likely to
    make mistakes. More descriptive syntax makes reading code easier. Less
    cognitive load means programmers can spend more mental capacity solving the
    problems they want to.

    Nibble is less repetitive than \mres{}, because common subexpressions can be
    extracted into let expressions. This is demonstrated back in figure (*TODO:
    other figure*). *Point out particular expression.* Let expressions also make
    Nibble syntax more descriptive than \mres{}. Labels also allow parts of
    expressions to be given descriptive names inline. Finally, the abstraction
    from let expressions and lambda expressions reduces the cognitive load of
    Nibble compared to \mres{}.

*** Comparison with BNF
    Nibble is as expressive as standard BNF. As discussed earlier, surjective
    translation means that Nibble expressions describe the same set of languages
    as \mres{}. As stated in section [[*Parser Combinators]], Lei√ü cite:null found
    that \mres{} describe all context-free languages. BNF also describes all
    context-free languages, so Nibble is at least as expressive as BNF.

    Untyped Nibble is a more practical description of languages than BNF. Nibble
    is less repetitive, more descriptive and has a lower cognitive load. BNF can
    only have alternatives at the top level. This means that the BNF declaration
    for the Nibble expression ~match x.("b"|"c").x~ would have to double the
    number of occurrences of ~x~. BNF also has no equivalent for the lambda
    expressions found in Nibble. Going back to (*TODO: figure*), whilst Nibble
    can invoke the ~list~ expression twice, BNF has to fully expand it twice.

    *TODO: descriptive.*

    BNF uses a single mutually-recursive namespace. This is demonstrated in
    listing [[lst:bnf-mut-rec]]. *Example.* This means that when a programmer finds
    a BNF non-terminal, its declaration could be anywhere in the file. By
    contrast, Nibble uses multiple nested lexical scopes. All variables are
    declared earlier in the expression, either from a let expression or lambda
    expression. *How does this help?*
    
    #+label: lst:bnf-mut-rec
    #+name: lst:bnf-mut-rec
    #+caption: Demonstration of BNF's single mutually-recursive namespace
    #+begin_src text
      TODO: finish source block
    #+end_src

    Typed Nibble is much less practical than BNF used by most other parser
    generators. Typed Nibble reintroduces some repetition. Listing
    [[lst:nibble-left-factor]] shows two Nibble expressions. Their languages are
    equivalent, but the first fails to type check whilst the second succeeds. 

    #+label: lst:nibble-left-factor
    #+name: lst:nibble-left-factor
    #+caption: Left factoring of a Nibble expression
    #+begin_src rust
      // Unfactored
      let x = ...;
      let y = ...;
      let alpha = "a" | "b" | "c" | ... | "z";
      let ident = [rec](alpha.(_|rec));
      match "if".x | "iter".y | ident;

      // Left factored
      let alpha = "a" | "b" | "c" | ... | "z";
      let ident = [rec](alpha.(_|rec));
      let ident_cont = _|ident;
      match
        "i".(_|
             "f".(x|ident_cont)|
             "t".(_|
                  "e".(_|
                       "r".(y|ident_cont)|
                       ("a"|"b"|...).ident_cont)|
                  ("a"|"b"|...).ident_cont)
             ("a"|"b"|...).ident_cont)|
        ("a"|"b"|...).ident_cont;
    #+end_src

    The first expression is not well-typed because the first sets are not
    disjoint. ~"if"~, ~"iter"~ and ~ident~ can all start with ~i~. By
    left-factoring, we remove an ~"i"~ from each expression and then try and
    parse the rest. This has to be recursively repeated until every first set is
    disjoint.

    A left-factored expression has a lot of repetition. Originally, ~ident~ was
    used once, and the long concatenation of characters appeared only once.
    After left-factoring, ~ident_cont~ appears five times, and there are four
    additional long alternations of characters. There is also a huge increase in
    cognitive load. Before, it was easy to tell that something special could
    happen after parsing an ~if~ or an ~iter~. This is obfuscated in the
    left-factored version.

    Parser generators using BNF typically do not experience this problem. The
    bottom-up nature of the parsers generated from BNF allow for BNF
    declarations that are not left factored. They also typically include a
    lexer, which would split ~"if"~, ~"iter~ and ~ident~ into distinct tokens.
    
*** Comparison with Other Rust Libraries
    Finally, we compare how much effort goes is required to integrate Nibble and
    Chomp into a project compared to integrating ~lalrpop~, a popular
    traditional parser generator for Rust. The first major difference is that
    ~lalrpop~ requires using a build script. *why is this bad?*

    ~lalrpop~ features semantic actions that are not present in Nibble. Semantic
    actions provide a way for a parser to execute arbitrary code during parsing.
    This can eliminate the need for parse tree data structures, and let the
    parser build the desired datatype directly. 

    As stated in section [[*Requirements Analysis]], semantic actions were a stretch
    goal for Nibble. Upon further research, there were some conflicting
    requirements that would complicate their implementation. Notably, Nibble
    expressions should be independent of the target programming language of
    Chomp. Typically, parser generators pass semantic actions straight through
    to the target programming language. Such a technique would break the
    requirement for Nibble. Hence, another programming language would have to be
    defined, which could be translated into the target programming language.
    This appeared to be significantly more effort than other stretch
    requirements, so semantic actions have not been added to Chomp.

    Chomp produces Rust source code. Rust has a strong separation between where
    data types are declared and where methods are defined on them. Although
    Chomp declares the parse tree data types and some of their methods, these
    features of Rust allow the user to define their own methods. Using the
    procedural macro system, semantic actions can be defined near the Nibble
    expression, even though they cannot be embedded within it. One problem with
    this approach is that it requires the user to reference data types generated
    by Chomp. Most of these names are automatically generated, and there is no
    easy way to discover those names.

    The final significant usability difference between Nibble and ~lalrpop~ is
    that  ~lalrpop~ uses a lexer. Chomp does not use a lexer for two reasons.
    First, it would require Nibble to include a datatype definition for the
    tokens. To do this in a way that is independent from Chomp's target language
    would need a translator and language syntax. Secondly, many practical
    languages change the lexer behaviour depending on the parsing state. For
    example, characters in a JSON string are treated differently from characters
    elsewhere. If the lexer requires the parser state, and the parser can
    perform the functions of the lexer, the complexity of adding a stateful
    lexer does not seem justified to me.
** Quantitative
   A major claim of Chomp is that it produces linear time parsers. We evaluate
   this by looking at the performance of AutoNibble. Next, we compare the
   performance of AutoNibble to the handwritten Nibble parser. Finally, we
   compare the performance of chewed parsers against both handwritten and
   another generated parser for two languages: JSON, and arithmetic.
   
*** Methodology
    Benchmarks were performed using the ~criterion~ library for Rust. Each
    benchmark consists of a single function evaluation, repeated many times.
    During a three second warm-up period, the benchmark function is ran for an
    increasing number of iterations. This is used to approximate how many
    iterations can be performed in a five second timing window.

    This iteration estimate is divided by 5050 (the 100th triangle number). This
    splits the five second timing window into 5050 work units. The wall time
    is measured after one work unit, then after a further two units, then after
    three more and so on. This results in 100 samples for each benchmark
    function. Using linear regression, we can approximate the duration of one
    work unit and from that we can calculate the duration of one function
    iteration.

    Benchmarks only use a single thread at a time. It was performed on a desktop
    computer with the following specifications: *TODO: specs*

*** AutoNibble
    This section explores the performance of AutoNibble. We start by showing
    that the performance is more likely to be linear than polynomial. Next, we
    compare AutoNibble and the performance of the original Nibble parser.

    Figure [[fig:autonibble-chomp-nibble]] shows the performance of AutoNibble and
    the handwritten Chomp parser on expressions of various sizes. The largest
    input is a Nibble expression used to describe a past iteration of Nibble.
    Smaller inputs are small modifications to truncated versions, each roughly
    twice the size of the last.

    #+label: fig:autonibble-chomp-nibble
    #+caption: Performance of AutoNibble against the handwritten Nibble parser on expressions of various sizes.
    #+name: fig:autonibble-chomp-nibble
    [[./autonibble.png]]

    *TODO: test that performance is linear.*

    Figure [[fig:autonibble-chomp-nibble]] suggests that AutoNibble is more
    efficient than the handwritten Nibble parser. This is possibly because
    AutoNibble has fewer features than the full Nibble parser.

    Firstly, AutoNibble can only parse the ASCII subset of Nibble. The Nibble
    language is defined on Unicode characters. AutoNibble was restricted to the
    ASCII subset because it is only a technical demonstration of Nibble and
    Chomp.

    Secondly, AutoNibble discards information about the origin of tokens. To aid
    users in writing well-typed Nibble, Chomp preserves the source code location
    of tokens. When Chomp finds a type error, it can report the exact location
    to users. AutoChomp will not be used in practice, so it does not construct
    or preserve this information.

    Finally, *third thing?*
    
    * Chewed parsers have linear time complexity
      * Use an F test - linear is no worse than other model
        * linear vs exponential: \(y = a + bx + e ^{cx} + \epsilon\)
        * linear vs polynomial: \(y = a + b x^c + \epsilon\)
    * Chewed parser performance
      * Use a Chow test - combined is no worse than separate
*** Chewed Parser Performance
    Figures [[fig:bench-json]] and [[fig:bench-arith]] compare the performance of chewed
    parsers against handwritten and other generated parsers for two different
    languages, JSON and arithmetic. For JSON, the parsers were made to output a
    Rust representation of the object. For arithmetic, the parsers computed the
    final value of the expression. These extra steps emulate using the parsers
    in practice -- the target data type is fixed, but a developer can choose how
    to arrive there.

    #+label: fig:bench-json
    #+name: fig:bench-json
    #+caption: Performance comparision of various parsers for consuming JSON.
    [[./json.png]]

    #+label: fig:bench-arith
    #+name: fig:bench-arith
    #+caption: Performance comparision of various parsers for consuming arithmetic.
    [[./arith.png]]

    In both cases, the handwritten parser performs better than the chewed parser
    and the other generated parser. There are some potential reasons for this.
    Firstly, the handwritten parser produces values of the target datatype
    directly. Because the developer is in control of all the code, there do not
    need to be any intermediate conversions. In contrast, chewed parsers have to
    produce a full parse tree before conversion. *What about ~lalrpop~?*

    Secondly, *another reason.*

    Chewed parsers have comparable performance to ~lalrpop~ parsers. *Why?*
    *Can verify this claim with a Chow test.*

* Conclusion
  My project was a success. I completed the success criterion of implementing
  and testing AutoChomp. AutoChomp even managed to outperform Chomp, although
  this could be due to AutoChomp having a less-powerful parser.
  
  I also completed one extension requirement and made significant progress
  towards implementing another. Chomp is integrated with Rust's procedural macro
  system, which makes integrating Chomp much easier than integrating some other
  parser generators. I also designed the LM type system, which can assign
  polymorphic function types to Nibble expressions.
  
  Chewed parsers have performance comparable to other generated parsers (section
  [[*Quantitative]]) and, unlike ordinary parser combinators, come with a
  compile-time guarantee of linear performance (section [[*AutoNibble]]).
  
** Lessons Learned
   My courses during Lent term took up more time than I anticipated. This lead
   to me being able to spend less time on my project than I had hoped.
   Fortunately, my proposal allocated most time during Lent term to working on
   extensions. This, and other scheduled slack time, allowed me to ensure the
   core of my project was complete and at a high standard.
    
   There are broadly two architectures when building a compiler or translator,
   such as Chomp. The first uses transformations, where pipeline stages are
   applied to the whole input sequentially. The second uses queries, which uses
   memoisation to perform different pipeline stages on different parts of the
   input in an arbitrary order. Chomp uses transformations, which were
   acceptable during early development for the \ky{} type system, and when
   translating expressions before type checking. For the \ky{}-\lambda type
   system, code generation requires the original expression structure, whilst
   type-checking performs translation first. Keeping these two structures
   together is much more challenging for transformational systems compared to
   query-based ones.
    
   Whilst there was a very low initial cost for using a transformational
   architecture, the cost became very large as the type system became more
   complex. If I spent more time researching extensions to the \ky{} type system
   before development began, I could have avoided this large, deferred cost.
   This advice is almost certainly applicable to future projects I might
   undertake.
    
   During benchmarking, I discovered a bug with the handwritten Nibble parser.
   Despite it not using any global state, each parse iteration would take some
   more time to run. Instead of the iteration-time graph being linear as
   expected, it was quadratic or maybe even cubic. I spent several days trying
   to find the cause of the problem, before deciding it would be more effective
   to start rewriting the parser instead. I started by writing the most complex
   again from scratch. After this five-minute rewrite, I decided to test the
   benchmarks again and found the iteration-time graph became linear. I still do
   not know the cause of the bug, but I cam away with the knowledge that
   rewriting code can be significantly faster than finding the cause of bugs.
** Future Work
   The LM Type system generates the potential for lots of future work. First,
   there is no proof that it is a useful type system. Whilst the modifications
   to the Hindley-Milner and \ky{} type systems are small, these changes could
   have drastic consequences to soundness and completeness properties. Formal
   proofs of these properties are necessary to make sure the LM type system
   achieves what it sets out to complete.
    
   Secondly, Chomp could be modified to use the LM type system. Even without
   proof, a practical implementation can provide evidence for the claims made by
   the LM type system. It can also help to justify some design decisions of the
   LM type system, such as the choice to use structural unification for types.
   Work on this implementation can proceed alongside a proof for the type
   system. Using a dependant-type system such as Agda could allow for the proof
   and implementation to be tightly coupled, in the sense that changes to one
   necessitates changes to the other.
    
   The LM type system probably does not accept expressions for a larger class of
   languages than the \ky{} type system does. Neither type system accepts an
   \mre{} such as \( (\epsilon \vert a) \cdot b \), due to the \(\circledast\)
   constraint, even though there is a linear-time parser for this language.
   Searching for a type system for \mres{} that permits accepts a wider range of
   expressions would make writing well-typed Nibble easier.
    
   Recall that an unrestricted version of the \ky{}-\lambda type system can
   perform arbitrary computation. This makes it possible to write a
   repeat-\(n\)-times combinator using the \ky{}-\lambda type system. Due to the
   way polymorphism is used by the LM type system, such combinators are
   impossible, because there is no way to encode the natural numbers. Nibble
   could be extended to support the naturals and other data types, making it
   possible to write a richer set of parser combinators.
* References
  \printbibliography[heading=none]{}

* Project Proposal
  TODO!
  
* Pink Book
** Introduction [0/3]
   * [ ] Principal motivation
   * [ ] Fits into surrounding CS
   * [ ] Summary of previous work
** Preparation [0/6]
   The work before the code was written.

   * [ ] Complex theories needing understanding
   * [ ] New programming languages
   * [ ] Requirements analysis
   * [ ] Software engineering techniques
   * [ ] Refinements to proposal
   * [ ] Starting point
** Implementation [1/5]
   What the project managed to produce.

   * [X] Describe key theories
   * [-] Repository overview
     * [ ] One page
     * [ ] High-level structure
     * [X] Line counts
   * [ ] Describe key algorithms
     * [ ] With code
   * [ ] Demonstrate design for testing
   * [ ] Describe design choices
** Evaluation [0/3]
   Signs of success

   * [ ] Meeting the success criterion
   * [ ] Meeting goals of Nibble
   * [ ] Meeting goals of Chomp
** Conclusion [0/3]
   * [ ] Return to introduction
   * [ ] Lessons learned
   * [ ] Future work
* List of Tasks
  - [ ] Structure!
    - [ ] More section references.
    - [ ] Introductions need to be higher level
    - [ ] Headings are big claims!
  - [ ] Target audience!
  - [ ] \ky{}-\lambda substitution
    - [ ] 10.1145/582153.582176
 
  - [ ] https://savage.net.au/Marpa.html

  - [ ] Surjective!
 
  - [ ] Fix first and flast typography.
  - [ ] Major edit to introduction to improve story telling
  - [ ] All of Alan's tweaks to the introduction
  - [ ] Delete depending on amount of extension. Shouldn't be too significant a
    difference overall
  - [ ] Reduction => Translation
  - [ ] Remove a bunch of maths delimiters.
  - [ ] Describe BNF
  - [ ] Define lexer in background
* Incomplete Sections
** Domain Specific Languages
** BNF
** \mres{}
   #+label: fig:mu-baa
   #+name: fig:mu-baa
   #+caption: *TODO: fig:mu-baa*
   #+begin_figure
   #+end_figure
   #+label: fig:mu-hex-colour
   #+name: fig:mu-hex-colour
   #+caption: *TODO: fig:mu-hex-colour*
   #+begin_figure
   #+end_figure
   #+label: fig:mu-many-plus
   #+name: fig:mu-many-plus
   #+caption: *TODO: fig:mu-many-plus*
   #+begin_figure
   #+end_figure
** The \ky{} Type System
*** Left Factoring
** The Hindley-Milner Type System
** Translators
*** De Bruijn Indices
** Rust
*** Procedural Macros
*** Traits
