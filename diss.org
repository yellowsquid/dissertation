#+latex_class: dissertation
#+latex_class_options: [12pt,a4paper,twoside,openright]
#+latex_header: \usepackage[hyperref=true,url=true,backend=biber,natbib=true]{biblatex}
#+latex_header: \usepackage[margin=1in]{geometry}
#+latex_header: \usepackage{booktabs,ebproof,parskip,stmaryrd}
#+latex_header: \addbibresource{diss.bib}

# math operators
#+latex_header: \DeclareMathOperator{\True}{true}
#+latex_header: \DeclareMathOperator{\False}{false}
#+latex_header: \DeclareMathOperator{\If}{if}
#+latex_header: \DeclareMathOperator{\Then}{then}
#+latex_header: \DeclareMathOperator{\Else}{else}
#+latex_header: \DeclareMathOperator{\Let}{let}
#+latex_header: \DeclareMathOperator{\In}{in}
#+latex_header: \DeclareMathOperator{\Null}{null}
#+latex_header: \DeclareMathOperator{\First}{first}
#+latex_header: \DeclareMathOperator{\Flast}{flast}

# shorthand
#+latex_header: \newcommand\mre{\(\mu\)-regular expression}
#+latex_header: \newcommand\mres{\(\mu\)-regular expressions}

# try to avoid widows and orphans
#+latex_header: \raggedbottom
#+latex_header: \sloppy
#+latex_header: \clubpenalty1000%
#+latex_header: \widowpenalty1000%

# add more header depths
#+options: H:6

list-of-figures:nil

\pagestyle{headings}
* Pink Book
** Introduction [0/3]
   * [ ] Principal motivation
   * [ ] Fits into surrounding CS
   * [ ] Summary of previous work
** Preparation [0/6]
   The work before the code was written.

   * [ ] Complex theories needing understanding
   * [ ] New programming languages
   * [ ] Requirements analysis
   * [ ] Software engineering techniques
   * [ ] Refinements to proposal
   * [ ] Starting point
** Implementation [0/5]
   What the project managed to produce.

   * [ ] Describe key theories
   * [ ] Repository overview
     * [ ] One page
     * [ ] High-level structure
     * [ ] Line counts
   * [ ] Describe key algorithms
     * [ ] With code
   * [ ] Demonstrate design for testing
   * [ ] Describe design choices
** Evaluation [0/3]
   Signs of success

   * [ ] Meeting the success criterion
   * [ ] Meeting goals of Nibble
   * [ ] Meeting goals of Chomp
** Conclusion [0/3]
   * [ ] Return to introduction
   * [ ] Lessons learned
   * [ ] Future work
* List of Tasks
  - [ ] Structure!
    - [ ] More section references.
    - [ ] Introductions need to be higher level
    - [ ] Headings are big claims!
  - [ ] Target audience!
  - [ ] KY-\lambda substitution
    - [ ] 10.1145/582153.582176
 
  - [ ] https://savage.net.au/Marpa.html

  - [ ] Surjective!
 
  - [ ] Fix first and flast typography.
  - [ ] Major edit to introduction to improve story telling
  - [ ] All of Alan's tweaks to the introduction
  - [ ] Delete depending on amount of extension. Shouldn't be too significant a
    difference overall
  - [ ] Reduction => Translation
  - [ ] Remove a bunch of maths delimiters.
  - [ ] Describe BNF
  - [ ] Define lexer in background
* Introduction
  A /language/ is a set of strings over some alphabet of symbols. For example, a
  dictionary enumerates a language over written characters. Spoken English is a
  language, where the alphabet consists of phonemes. Programming languages are
  over an alphabet of ASCII or Unicode characters. IP packets are languages over
  bytes. The language of a decision problem is the set of valid inputs. Every
  computer science problem can be transformed into a question about languages.

  Whilst linear strings are helpful for data storage and transmission, they have
  limited use for other algorithms. Strings are used to encode other non-linear
  data. A /parser/ is a function that takes a language string and decodes it to
  return the underlying data. Parsers should be fast; why spend time decoding
  data when there is useful computation to be done? Unfortunately, hand writing
  efficient parsers is repetitive, with lots of boiler plate, and requires
  careful consideration of the order of operations.

  A /parser generator/ is a curried function taking a description of a language
  and returning a parser. If the target language is known ahead of time, then
  the parser can be computed ahead of time. Automated parser generation raises
  the level of abstraction for writing parsers, allowing people to quickly
  design a language so they can focus on solving difficult problems elsewhere.

  The de facto way to describe a language is with a /formal grammar/. Introduced
  by Chomsky cite:null, a grammar is a finite set of string rewriting rules.
  These rules are often given to the parser generator in Backus-Naur form (BNF).
  This form of parser generation has a couple of problems in practice. Firstly,
  BNF uses a single global mutually-recursive namespace. This is incompatible
  with modern programming languages that almost exclusively use lexical scoping.
  Secondly, there is no standard BNF format. Therefore a grammar cannot be
  easily shared between different projects. Finally, introducing an additional
  compilation step for parser generation is often difficult, sometimes needing
  to completely overhaul the build system being used.

  These issues, combined with a trend towards more powerful type systems, have
  led to an increase in use of /parser combinators/. Parser combinators are
  higher-order functions that take parsers and return a new parser. By using
  regular functions instead of parser generators, problems with variable scoping
  and compilation are completely bypassed.

  Unfortunately, the ease of use of parser combinators comes with a price. Most
  implementations of parser combinators use backtracking, which can lead to
  exponential worst-case parse time. Fortunately, a recent result by
  Krishnaswami and Yallop cite:null found a type system (the /KY type system/)
  for some primitive parser combinators that only accepts linear-time
  deterministic parsers.
  
** Project Overview
   *TODO: Rewrite*

   Some parser combinators can be defined by composing smaller combinators
   together. The KY type system handles this by evaluating the higher-order
   combinators until they reach the base combinators. In section ref:null I form
   the /LM type system/ by extending the KY type system to handle higher-order
   combinators directly.

   Next, I design /Nibble/ in section ref:null. Nibble is a new DSL for
   describing languages, inspired by combinators used in the LM type system.
   Nibble should be able to represent the same languages as BNF.
   
   In section ref:null, I implement /Chomp/, a typed parser generator for
   Nibble. Chomp uses the LM type system to ensure its input is suitable for
   transformation into a recursive-descent parser. Chomp is implemented in Rust,
   and produces Rust source code as output. This output is a /chewed parser/.
   
   I end by evaluating the performance of chewed parsers. Critically in section
   ref:null we show that chewed parsers operate in linear time. Section ref:null
   demonstrates that chewed parsers have comparable performance to other
   generated parsers. Finally section ref:null demonstrates that Nibble is
   potentially usable in practice.
* Preparation
  *STORY: what's this chapter about?*
  
** Background
   This subsection starts with the definition of formal languages, generators
   and parsers. Understanding these definitions is essential for understanding
   the rest of this dissertation. Next, it discusses formal grammars. Formal
   grammars are the traditional way to study context-free languages *(why have
   them?)*. Following this, it describes parser combinators and an algebraic
   interpretation of them. This algebraic interpretation is the core of the KY
   type system and the extended LM type system. Finally this subsection
   describes the KY type system.
   
*** Formal Languages
    Given an alphabet, \( \Sigma \), a /language/
    \( L \subseteq \mathcal{P}(\Sigma^*) \) is a set of strings over this
    alphabet.

    Take a set \( D \) of /derivations/. A function \( g : D \to \Sigma^* \) is
    a /language generator/, with generated language
    \( L[g] = \{ w \in \Sigma^* \mid \exists d \in D. g(d) = w \} \).

    If \( g \) is an injection, then the generator is /unambiguous/. Every
    string in \( L[g] \) will have a unique derivation.

    Take a reversed function \( p : L[g] \to D \). \( p \) is a /parser/ of
    \( g \) if \( p \) is a right inverse of \( g \). In general, many such
    parsers could exist. Figure [[fig:ambiguous-parser]] shows a trivial example.
    The set of parsers for a generator is denoted \( P[g] \). When \( g \) is
    unambiguous, then \( g \) is both injective and surjective over \( L[g] \),
    hence \( p \) is unique.

    #+label: fig:ambiguous-parser
    #+name: fig:ambiguous-parser
    #+caption: A generated language with two parsers.
    #+begin_figure
    \[ D = \{ 0 , 1 \} \]
    \[ g(d) = a \]
    \begin{align*}
      p_1(a) &= 0 \\
      p_2(a) &= 1
    \end{align*}
    \[ g(p_1(a)) = a = g(p_2(a)) \]
    #+end_figure

    For a set of /language descriptions/, \( \mathcal{D} \), define an indexed
    set of generators, \( \mathcal{G} \). A /parser generator/ is a function
    \( \mathcal{P} \) such that
    \( \mathcal{P}(d) \in P[\mathcal{G}_d] \). For any valid language
    description, a parser generator then produces a parser for that language.
*** Formal Grammars
    Formal grammars are a set of language descriptions. Introduce a set of
    /non-terminal symbols/ \( N \). Distinguish a /start symbol/ \( S \in N \).
    Let \( V = \Sigma \uplus N \) be the /vocabulary/ of a grammar.

    A /production rule/ is a pair, \( u \mapsto v \), where \( u \in V^*NV^* \)
    and \( v \in (V/S)^* \). The relation \( wuw' \Mapsto wvw' \) is an
    /application/ of this production rule.

    A grammar \( G \) is a set of production rules. A sequence of applications
    \( S \Mapsto^* w \) is a derivation if \( w \in \Sigma^* \). The generator
    for \( G \) returns these \( w \). An example derivation is given in figure
    [[fig:grammar-example]].

    #+label: fig:grammar-example
    #+name: fig:grammar-example
    #+caption: An example grammar derivation.
    #+begin_figure
    \begin{align*}
      S &\mapsto aX \\
      X &\mapsto aX\\
      aX &\mapsto Xb \\
      X &\mapsto c
    \end{align*}
    \[ S \Mapsto aX \Mapsto aaX \Mapsto aXb \Mapsto acb \]
    #+end_figure

    These /unrestricted grammars/ correspond to recursively enumerable languages
    cite:null. Whilst any string in the language is accepted, rejecting strings
    is undecidable. Chomsky cite:null introduced a hierarchy of constrained
    grammars. Adding more constraints to production rules reduces the
    computational complexity of parsers, at the cost of reduced expressive
    power.

    Context-free grammars have rules of the form \( A \mapsto v \), where
    \( A \in N \). This transforms derivations into trees, with non-terminal
    internal nodes and alphabet strings as leaves.

    /Context-free grammars/ are the smallest class of grammars in the Chomsky
    hierarchy that include paired delimiters. The set of languages they
    represent are called /context-free languages/. Unfortunately, algorithms
    that parse general context-free grammars, such as Earley and CYK, have
    super-linear time complexity. *NOTE: why is this bad?*

    Chomsky cite:null found that context-free grammars can be parsed by
    nondeterministic push-down automata -- finite state machines with a stack.
    Restricting this to deterministic finite automata leads to /deterministic
    context-free grammars/. Their languages can be parsed in linear time, and
    are unambiguous.

    There are generally two approaches to parsing deterministic context-free
    grammars: top-down and bottom-up. Both of these methods are typically
    restricted to one symbol of /lookahead/. This means only one symbol of the
    input is visible at a time, and once the input is advanced it cannot be
    reversed.

    Top-down parsers, or left-most derivation parsers, start at the root of the
    derivation tree and recursively parse each non-terminal. Parsers like this
    one exclude grammars with /left-recursion/; rules of the form
    \( A \mapsto Av \). With only one symbol of lookahead, it is impossible to
    determine how deep the derivation needs to be.

    Bottom-up parsers (right-most derivation parsers) start at the leaves of the
    derivation tree. This eliminates the left-recursion problem, as the tree is
    only built up to the minimum necessary height.
*** Parser Combinators
    A /generator combinator/ is a higher-order language generator. They take
    some number of generators and generator combinators, and produce a new
    generator or generator combinator. A /parser combinator/ is likewise a
    higher-order parser.
    
    Mathematical analysis of arbitrary generator combinators is infeasible --
    they are arbitrary functions, after all. By restricting the combinators used
    to the set composing some primitive combinators, it is possible to introduce
    an algebra to describe them. Figure [[fig:mu-reg-def]] details one such algebra,
    named \mres{}.
    
    #+label: fig:mu-reg-def
    #+name: fig:mu-reg-def
    #+caption: \mres{} and their derivations.
    #+begin_figure
      *TODO: alignment of derivations is a little wonky*
      \[
        e = \bot
          \mid \epsilon
          \mid c
          \mid e \cdot e
          \mid e \vee e
          \mid \mu x. e
          \mid x
      \]
      
      \centering
      \bigskip
      \begin{math}
      \begin{array}{ccc}
        \begin{prooftree}
           \infer0[DEps]{\epsilon &\Mapsto \epsilon}
        \end{prooftree}
        & \qquad &
        \begin{prooftree}
           \infer0[DLit]{c &\Mapsto c}
        \end{prooftree}
        \\
        & \qquad & \\
        \begin{prooftree}
           \hypo{e &\Mapsto w}
           \infer1[DVeeL]{e \vee e' &\Mapsto w}
        \end{prooftree}
        & \qquad &
        \begin{prooftree}
           \hypo{e &\Mapsto w}
           \infer1[DVeeR]{e' \vee e &\Mapsto w}
        \end{prooftree}
        \\
        & \qquad & \\
        \begin{prooftree}
           \hypo{e &\Mapsto w}
           \hypo{e' &\Mapsto w'}
           \infer2[DCat]{e \cdot e' &\Mapsto ww'}
        \end{prooftree}
        & \qquad &
        \begin{prooftree}
           \hypo{e [ \mu x . e / x ] &\Mapsto w}
           \infer1[DFix]{\mu x . e &\Mapsto w}
        \end{prooftree}
      \end{array}
      \end{math}
    #+end_figure

    There are three first-order language generators: \(\bot\) for the empty
    language, \(\epsilon\) for the language of the empty string only, and
    \( c \) for a language containing the single-symbol string \( c \) only.

    There are two second-order combinators. Concatenation, \( g \cdot g' \)
    takes words from \( g \) and concatenates them with words from \( g' \).
    Alternation, \( g \vee g' \), forms the union of the languages \( g \) and
    \( g' \).

    Finally, there is the least-fixed-point combinator \(\mu g\). This is the
    union \( \bigcup_{n\in\mathbb{N}} g^n(\bot) \), assuming \( g \) is
    monotone. It is the fixed point as \( g (\mu g) = \mu g \).

    To complete the definition of \mres{} as language generators, figure
    [[fig:mu-reg-def]] also shows the derivation relation. Leiß cite:null found that
    \mres{} describe context-free languages. This means that for any \mre{}, there
    is a context-free grammar with the same language, and vice versa. One
    consequence of this means that general \mres{} take super-linear time to
    parse.

    Context-free grammars resolve the parse complexity problem by a
    transformation into a push-down automaton. The algebraic nature of \mres{}
    lends itself to a type system instead.
    
*** KY Type System
    *Note: All definitions are taken from cite:null. To what extent do they need
    citations?*

    The KY type system is a type judgement for \mres{}. If an expression is well
    typed, then there exists a top-down parser for the language of the
    expression.
    
    There are three properties of languages that are particularly interesting,
    named \( \Null \), \( \First \) and \( \Flast \). Their definitions are in
    figure [[fig:lang-props]]. To summarise, a langauge \( L \) is \( \Null \) when
    it contains the empty string. The \( \First \) set is the set of symbols
    starting strings in \( L \), and the \( \Flast \) set is the set of symbols
    that immediately follow strings in \( L \) to make a bigger string in
    \( L \).
    
    #+label: fig:lang-props
    #+name: fig:lang-props
    #+caption: Definitions of \( \Null \), \( \First \) and \( \Flast \)
    #+begin_figure
      \begin{gather*}
        \Null L \iff \epsilon \in L \\
        \begin{align*}
          \First L &= \{ c \in \Sigma \mid \exists w \in \Sigma^*.\, cw \in L \} \\
          \Flast L &=
             \{ c \in \Sigma
             \mid \exists w \in \Sigma^+, w' \in \Sigma^*.\,
               w \in L \wedge wcw' \in L
             \}
        \end{align*}
      \end{gather*}
    #+end_figure
    
    A /KY type/ \( \tau \) is a record \( \{\textsc{Null} \in \mathbb{B} ,
    \textsc{First} \subseteq \Sigma , \textsc{Flast} \subseteq \Sigma \}\). A
    language /satisfies/ a type, \( L \vDash \tau \), when \( \Null L \le
    \tau.\textsc{Null} \wedge \First L \subseteq \tau.\textsc{First} \wedge
    \Flast L \subseteq \tau.\textsc{Flast} \). This definition means that a type
    always over-approximates a language's properties. As types are triples of
    values, they can be manipulated by functions. Figure [[fig:mu-type]] shows some
    basic types and some operations on them. It also describes two relations on
    types, used by the typing judgement.
    
    #+label: fig:mu-type
    #+name: fig:mu-type
    #+caption: Some KY types and operations and relations on them
    #+begin_figure
    \[ b \Rightarrow s = \If b \Then s \Else \emptyset \]
    \begin{align*}
      \tau_{\bot} &= ( \False, \emptyset, \emptyset ) \\
      \tau_{\epsilon} &= ( \True, \emptyset, \emptyset ) \\
      \tau_{c} &= ( \False, \{ c \} , \emptyset )
    \end{align*}
    \begin{align*}
      \tau \vee \tau' &= \left\{ \begin{array}{rl}
           \textsc{Null} = &\tau.\textsc{Null} \vee \tau'.\textsc{Null} \\
           \textsc{First} = &\tau.\textsc{First} \cup \tau'.\textsc{First} \\
           \textsc{Flast} = &\tau.\textsc{Flast} \cup \tau'.\textsc{Flast}
         \end{array}\right\} \\
      \tau \cdot \tau' &= \left\{ \begin{array}{rl}
           \textsc{Null} = &\tau.\textsc{Null} \wedge \tau'.\textsc{Null} \\
           \textsc{First} = &\tau.\textsc{First} \cup (\tau.\textsc{Null} \Rightarrow \tau'.\textsc{First}) \\
           \textsc{Flast} = &\tau'.\textsc{Flast} \cup (\tau'.\textsc{Null} \Rightarrow \tau'.\textsc{First} \cup \tau.\textsc{Flast})
         \end{array}\right\}
    \end{align*}
    \begin{align*}
      \tau \circledast \tau' &= (\tau.\textsc{Flast} \cap \tau'.\textsc{First} = \emptyset) \wedge \neg \tau.\textsc{Null} \\
      \tau \# \tau' &= (\tau.\textsc{First} \cap \tau'.\textsc{First} = \emptyset) \wedge \neg (\tau.\textsc{Null} \wedge \tau'.\textsc{Null})
    \end{align*}
    #+end_figure

    Since the aim is to build a top-down parser, an expression cannot be left
    recursive. The KY type system achieves this using two /variable contexts/. A
    variable context is a map from variables to type. One of the variable
    contexts is /unguarded/, meaning that variables can be used freely. The
    other context is /guarded/, meaning variables can only be used on the right
    side of a concatenation.

    Figure [[fig:mu-judge]] gives the full typing judgement of the KY type system.
    Of particular note, the TFix rule assumes \( x \) is guarded in the
    hypothesis, the TCat rule shifts the guarded context into the unguarded one
    for the right side, and the TVar rule can only reference unguarded
    variables. Krishnaswami and Yallop showed cite:null that is an expression
    has a complete typing judgement when the two variable contexts are empty, it
    is possible to compute a parser for the language of that expression.
    
    #+label: fig:mu-judge
    #+name: fig:mu-judge
    #+caption: KY typing judgement
    #+begin_figure
    \begin{math}
    \begin{array}{ccc}
      \begin{prooftree}
        \infer0[TBot]{\Gamma; \Delta &\vdash \bot : \tau_{\bot}}
      \end{prooftree}
      & \qquad &
      \\
      & \qquad &
      \begin{prooftree}
        \infer0[TEps]{\Gamma; \Delta &\vdash \epsilon : \tau_{\epsilon}}
      \end{prooftree}
      \\
      \begin{prooftree}
        \infer0[TChar]{\Gamma; \Delta &\vdash [ c ] : \tau_c}
      \end{prooftree}
      & \qquad &
      \\
      & \qquad &
      \begin{prooftree}
        \infer0[TVar]{\Gamma, x : \tau; \Delta &\vdash x : \tau}
      \end{prooftree}
      \\
      \begin{prooftree}
        \hypo{\Gamma; \Delta &\vdash e : \tau} 
        \hypo{\Gamma; \Delta &\vdash e' : \tau'} 
        \hypo{\tau &\# \tau'}
        \infer3[TVee]{\Gamma; \Delta &\vdash e \vee e' : \tau \vee \tau'}
      \end{prooftree}
      & \qquad &
      \\
      & \qquad &
      \begin{prooftree}
        \hypo{\Gamma; \Delta &\vdash e : \tau} 
        \hypo{\Gamma, \Delta; \cdot &\vdash e' : \tau'} 
        \hypo{\tau &\circledast \tau'}
        \infer3[TCat]{\Gamma; \Delta &\vdash e \cdot e' : \tau \cdot \tau'}
      \end{prooftree}
      \\
      \begin{prooftree}
        \hypo{\Gamma; \Delta, x : \tau &\vdash e : \tau} 
        \infer1[TFix]{\Gamma; \Delta &\vdash \mu x. e : \tau}
      \end{prooftree}
      & \qquad &
    \end{array}
    \end{math}
    #+end_figure
    
*** Hindley-Milner Type System
    *TODO: Proof read*
    
    The simply-typed lambda calculus (STLC) is possibly the simplest possible
    type system, consisting of ground terms and functions only. System F is an
    extension of the STLC, adding /polymorphism/, where values can have multiple
    types.

    /Type inference/ is the assignment of types to expressions such that the
    expression type checks. Whilst there are arguments for and against type
    inference, when types are difficult to express, the option to elide them is
    helpful. Unfortunately, type inference for System F is undecidable
    cite:null.

    To overcome this problem, Hindley and later Milner described a type system
    with /parametric polymorphism/. Values either have a monotype, or a
    polytype. A monotype is a regular STLC type. A polytype is an abstraction
    over a monotype by adding in type variables, which are placeholders for
    arbitrary monotypes. For example, the generic identity function has the
    polytype \( \forall \alpha. \alpha \to \alpha \).

    A key part of the HM type system is /specialisation/. This is the
    instantiation of one or more free variables in a polytype. The relation
    \( \sigma \sqsubseteq \sigma' \) holds if \(\sigma\) can specialise to
    \(\sigma'\).

    The syntax and type judgement for the HM type system is given in figure
    [[fig:hm-type]]. Notice how HMVar specialises types. Conversely, only HMLet can
    /generalise/ types -- monotypes with free variables become polytypes.

    #+label: fig:hm-type
    #+name: fig:hm-type
    #+caption: HM syntax and typing judgement
    #+begin_figure
    \begin{align*}
      e &= x \mid e e \mid \lambda x. e \mid \Let x = e \In e \\
      \tau &= \alpha \mid \tau \to \tau \\
      \sigma &= \tau \mid \forall \alpha. \sigma
    \end{align*}
    \begin{math}
    \begin{array}{ccc}
    \begin{prooftree}
      \hypo{\sigma \sqsubseteq \tau}
      \infer1[HMVar]{\Gamma, x : \sigma \vdash x : \tau}
    \end{prooftree}
    & \qquad &
    \begin{prooftree}
      \hypo{\Gamma \vdash e : \tau \to \tau'}
      \hypo{\Gamma \vdash e' : \tau}
      \infer2[HMApp]{\Gamma \vdash e e' : \tau'}
    \end{prooftree}
    \\ & \qquad & \\
    \begin{prooftree}
      \hypo{\Gamma, x : \tau \vdash e : \tau'}
      \infer1[HMAbs]{\Gamma \vdash \lambda x. e : \tau \to \tau'}
    \end{prooftree}
    & \qquad &
    \begin{prooftree}
      \hypo{\Gamma \vdash e : \tau}
      \hypo{\Gamma, x : \forall \alpha. \tau \vdash e' : \tau'}
      \infer2[HMLet]{\Gamma \vdash \Let x = e \In e' : \tau'}
    \end{prooftree}
    \end{array}
    \end{math}
    #+end_figure

    By restricting introduction of polymorphism to \( \Let \) statements only,
    type inference is not only possible, but is nearly linear is almost all
    cases. The inference algorithm, called the /J algorithm/ is usually given in
    tree form, as in figure [[fig:hm-infer]]. 

    #+label: fig:hm-infer
    #+name: fig:hm-infer
    #+caption: The J algorithm
    #+begin_figure
    \begin{prooftree*}
      \hypo{\tau = inst(\sigma)}
      \infer1[JVar]{\Gamma, x : \sigma \vdash x : \tau}
    \end{prooftree*}
    \begin{prooftree*}
      \hypo{\Gamma \vdash e : \tau}
      \hypo{\Gamma \vdash e' : \tau'}
      \hypo{\tau'' = newvar()}
      \hypo{unify(\tau, \tau' \to \tau'')}
      \infer4[JApp]{\Gamma \vdash e e' : \tau''}
    \end{prooftree*}
    \begin{prooftree*}
      \hypo{\tau = newvar()}
      \hypo{\Gamma, x : \tau \vdash e : \tau'}
      \infer2[JAbs]{\Gamma \vdash \lambda x. e : \tau \to \tau'}
    \end{prooftree*}
    \begin{prooftree*}
      \hypo{\Gamma \vdash e : \tau}
      \hypo{\Gamma, x : \forall \alpha. \tau \vdash e' : \tau'}
      \infer2[JLet]{\Gamma \vdash \Let x = e \In e' : \tau'}
    \end{prooftree*}
    #+end_figure

    Instead of performing specialisation, JVar instead returns a general
    instance of a polytype. All the bound type variables are instantiated by a
    fresh generic type instance.

    Specialisation is then performed by JApp. The \(unify\) function coerces
    both arguments to their join, or type inference fails if the join doesn't
    exist. Recall that the join of two values is the least value greater than
    them both. Therefore \(unify\) performs the least amount of specialisation
    to give both arguments the same shape.

    Variations of the HM type sytem are used by many functional programming
    languages, such as ML and Haskell cite:null. 
** Requirements Analysis
   My core deliverable focused on implementing the KY type system. Having a well
   typed language description is nearly useless without a way to parse the
   language. Hence another core component was to /output a chewed parser/. These
   two components could then be used to create a parser from any Nibble
   description.

   *TODO: rewrite*
   
   One major feature of parser combinators is their composition into
   higher-order combinators. The KY type system cannot directly type check these
   higher-order combinators and must first perform some evaluation down to
   combinators represented by \mres{}. This can lead to an exponential increase in
   size of \(mu\)-regular expressions. I attempt to eliminate this issue by
   /exploring adding functions and lambda expressions/ to \mres{} and to Nibble.

   There are many other ways Nibble could be extended. *TODO: list them*
** Starting Point
   I closely studied the KY type system before beginning the project. I did not
   begin any work on possible extensions to it.

   The project builds on ideas about formal languages. These have been studied
   in the /Part IA Discrete Maths/ and /Part IB Compiler Construction/ courses.
   I also did a small personal project on them during the summer of 2018.

   Additionally, the project uses concepts from type systems, covered in the
   /Part IB Semantics of Programming Languages/, /Part II Types/ and /Part II
   Denotational Semantics/ courses.
** Software Engineering
*** Project Management
    After successful development of an initial core, extensions to a programming
    language naturally tend themselves to an iterative approach. Whilst you are
    mindful of future extensions, you work towards successful implementation of
    one at a time.

    This lends itself to the spiral development model. Each component follows a
    waterfall development cycle --- design, implementation, integration and
    testing --- and no two components are developed concurrently.

    *NOTE: the rest of this section could be cut*

    This model has several other benefits. At the end of each cycle, there is a
    functional deliverable. This means that even when there are unexpected
    delays in implementing a component, there is still a functional product to
    fall back on.

    Additionally, there is a lot of flexibility in what components are
    implemented and in what order. As you work on a product, you come to better
    understand what features can be added and the cost of doing so. *TODO: More
    words here*
*** Version Control
    I used git as a version-control and revision history system. New features
    were developed on individual branches. Upon completion, they were merged
    with the main branch.

    The git repository was mirrored on both a privately-owned server and GitHub.
    Regular commits and pushes ensured that very little data was lost if there
    was an issue with my device.

    The project is dual-licensed under the MIT and Apache 2.0 licenses, as is
    common for projects written in Rust. These are permissive licenses that
    encourage development whilst limiting personal liability.
*** Development Tools
    The standard Rust build system is called Cargo. It provides an easy way to
    run several kinds of checks against the whole code base. In particular
    clippy is a static analysis tool that highlights some style improvements and
    common bugs. Also, rustfmt was regularly used to consistently format code.
    
    Some tests were performed using Rust's built-in test harness. This allows
    the user to write unit tests anywhere. It also provides a method of
    performing integration tests.

    Benchmarks were written using  criterion. This micro-benchmarking library
    measures the performance of a function by measuring thousands of iterations.
    It also provides some simple statistical analysis and comparisons between
    functions.
* Implementation
  This section is split into two parts. The first describes Nibble, a new
  practical DSL for describing languages, and two type systems for it. The
  second describes Chomp, which is a parser generator from Nibble to Rust,
  implemented in Rust.

  We start by explaining the changes from \mres{} to Nibble. This is followed by
  descriptions of two type systems. KY-\(\lambda\) is a macro-based type
  system, making heavy use of syntactic replacement. This is the type system
  used by Chomp. \(LM\) is a Hindley-Milner type system with a constraint
  checker, adding polymorphic types to the *KY-\(\lambda\)* type system.

  The second part describes Chomp, starting with an overview of the code
  repository and a quick look at why Rust was used. A detailed view of the core
  generation pipeline follows this. Finally there is a discussion of the
  procedural macro system in Rust and how it is used by Chomp.

** Syntax of Nibble
   Nibble is a language to describe languages. This means that they are
   semantically interpreted as language generators and generator combinators,
   just like \mres{}. The best way to introduce Nibble is by a direct comparison
   with \mres{}, like in figure [[fig:nibble-vs-mu]].

   #+label: fig:nibble-vs-mu
   #+name: fig:nibble-vs-mu
   #+caption: Comparison of Nibble and \mres{} describing signed binary arithmetic.
   #+begin_figure
   *TODO: figure*
   #+end_figure

   \mres{} contain lots of repetitions. For example, *example*. Nibble
   eliminates this repetition using two mechanisms. One is named let
   expressions. These introduce a new variable binding, eliminating identical
   code structures. The other is lambda expressions. These are parameterised
   expressions. Semantically, both let expressions and lambda expressions
   correspond to generator combinators.

   Recall that a language generator is a function from derivations into strings.
   Therefore we need a way to convert Nibble expressions into derivations. This
   can be done by /reducing/ the Nibble expressions by eliminating let
   expressions and function calls at the top level. The resulting expression has
   a combinator borrowed from \mres{} at the top level, and uses the
   corresponding derivation rule.

   Reduction is similar to call-by-name evaluation of programming languages.
   Given a let expression, all occurrences of the binding variable in the body are
   substituted with the bound expression. A function call on a lambda expression
   behaves similarly. *Examples.*
   
   There are several other minor changes between Nibble and \mres{}. Nibble does
   not include an equivalent for \(\bot\): there are few, if any, practical uses
   for the empty language. Literals can be a sequence of characters instead of
   only a single character. This aligns closer with programmer intentions than
   lots of single character concatenations. Other syntactic elements of \mres{}
   were converted to ASCII for ease of input. Finally, fixed-point expressions
   take a lambda abstraction instead of directly introducing the fixed-point
   variable.
   
** Nibble Type Systems
   Nibble has two different type systems of different complexities. The
   KY-\(\lambda\) type system is a minimal departure from the \(KY\) type
   system, by reducing Nibble expressions to \mres. The LM type system works
   directly on Nibble expressions. It is a modified Hindley-Milner type system,
   introducing algebraic types and type constraints. 

*** KY-\(\lambda\) Type System
    KY-\(\lambda\) is a minimal type system for Nibble. Instead of type checking
    Nibble expressions, it first performs reduction to get \mres{}, which it
    then type checks using the KY type system. Under this system, let
    expressions and lambda expressions are like parametric macros: a
    substitution of expressions.

    An immediate benefit of this system is its simplicity. After performing the
    relatively simple task of reduction, Chomp can then check expressions using
    the already-developed KY type system.

    There are some problems with this system. Firstly, Nibble forms a superset
    of the lambda calculus. Therefore reduction of expressions is
    non-terminating. An example of such an is in listing [[lst:omega]]. This
    can be mitigated by forbidding variables to appear in the function position
    of an application expression. Even still, the size of expressions can
    increase exponentially.

    #+label: lst:omega
    #+name: lst:omega
    #+caption: Non-reducing Nibble expression.
    #+attr_latex: :centering
    #+begin_src rust
      let omega(x) = x x;
      match omega omega
    #+end_src

    Secondly, a reduced expression can contain the same \mre{} multiple times.
    This means a naive type checker will waste time on redundant computation.
    This could be mitigated by including some form of caching.

    Both of these problems can be resolved if types are applied directly to
    Nibble expressions.
*** LM Type System
    The LM type system is a modified Hindley-Milner type system. Instead of
    reducing Nibble expressions, they are type checked directly. This requires
    algebraic base types, function types and parametric polymorphism.

    The Hindley-Milner type system uses the structure of expressions to infer
    their types. Instead of treating \(\tau \cdot \tau\) and \(\tau \vee \tau\)
    as operators, we use them as constructors. We also introduce
    \( \mu \alpha. \tau \) as a type constructor.

    Due to guarding rules, variables are treated differently depending on
    whether they appear on the left or right of concatenation rules. This means
    that for application expressions the way the function uses the variable
    changes how the argument can be type checked. This is resolved by
    introducing two function types -- \(\tau \to \tau\) and \(\tau \leadsto
    \tau\) for when the formal parameter is used in an unguarded and guarded
    context respectively.

    In the KY type system, the concatenation and alternation rules included
    constraints on types in their hypotheses. Because of the polymorphism
    present in Hindley-Milner type systems, it is impossible to know when these
    constraints are satisfied until the type variables are instantiated.
    Instead, the LM defers checking these constraints by collecting them in the
    conclusion of type rules. Polytypes must then carry with them the set of
    constraints that any instances must satisfy.

    We say an expression is well typed if the typing rules can assign it a type,
    and all constraints are satisfied.
    
    Finally we consider type inference. Types in the KY type system form an
    algebra. For example, \( \alpha \cdot \tau_\epsilon = \alpha \) for all
    types \( \alpha \). Because of this algebraic nature, it is difficult to
    determine whether two types are equal. For instance, do we have \( ((\alpha
    \vee \beta) \cdot \gamma) \cdot \delta = (\alpha \cdot (\gamma \cdot
    \delta)) \vee ((\beta \cdot \tau_\epsilon) \cdot (\gamma \cdot \delta))
    \)[fn:: Yes]? Introducing fixed points only makes determining type equality
    more difficult.

    Recall that unification takes two types and instantiates type variables
    until they are equal. Given that equality is so complex, how can we unify
    two variables efficiently? The solution is to only use structural equality
    for unification. Whilst it will reject some otherwise well-typed Nibble
    expressions, using structural equality should have a huge performance
    benefit. In any case, for this small set of rejected expressions, there will
    be a Nibble expression with an equivalent language.
** Repository Overview
   Table [[tbl:overview]] gives a brief description of the repository structure for
   Chomp. The main Chomp library and binary are contained in the ~src~
   directory. Other directories correspond to separate libraries and binaries
   built around Chomp.

   #+label: tbl:overview
   #+name: tbl:overview
   #+caption: Brief outline of the code repository structure.
   #+attr_latex: :float t
   | Path                   | Description                                              | Lines of Code |
   |------------------------+----------------------------------------------------------+---------------|
   | ~src/nibble~           | Nibble parser and normalisation                          |           676 |
   | ~src/chomp~            | Chomp type inference algorithm                           |          1676 |
   | ~src/lower~            | Chewed parser code generation                            |           420 |
   | ~chewed~               | Shared library for all chewed parsers                    |           270 |
   | ~chomp-macro/src~      | Procedural macro interface                               |            41 |
   | ~chomp-macro/tests~    | Minimal end-to-end tests of Chomp                        |           123 |
   | ~autochomp/src~        | AutoChomp -- Chomp using a chewed parser for parsing     |           570 |
   | ~autochomp/bench~      | Performance comparison between Chomp and Autochomp       |           167 |
   | ~chomp-bench/**/json~  | Performance comparison between various parsers for JSON  |           430 |
   | ~chomp-bench/**/ascii~ | Performance comparison between various parsers for ASCII |           227 |

   Rust was chosen as an implementation language for a variety of reasons.
   First is the ownership, which is part of the Rust type system that ensures
   data is stored in one place at a time. This has consequences for chewed
   parsers. Once a character is taken from the input stream, there is no subtle
   way to put it back without transferring ownership. In fact, the API in the
   ~chewed~ library prevents this possibility.

   Another reason Rust was chosen is the procedural macro system. This system
   provides an easy way to integrate Chomp into the Rust build process. It is
   discussed in more detail in section [[*Procedural Macros]].

   Rust uses a highly optimising compiler. Whilst the algorithms used by chewed
   parsers are simple, they include many nested method calls by design. The Rust
   compiler performs optimisations which dramatically increases the performance
   of these parsers without having to be clever with code generation.

   Another useful feature of Rust is its exhaustive pattern matching. This is
   relevant to Chomp in two ways. Inside of Chomp, exhaustive pattern matching
   ensures all transformations are implemented for all Nibble expressions. For
   the chewed parsers, the exhaustive pattern matching ensures that all possible
   input characters have to be dealt with in a sane way; either continuing to
   parse, or returning an error.
   
** Core Generation Pipeline
   Chomp has three main phases: parsing, type inference, and code generation.
   Parsing takes an stream of input Nibble tokens for and produces an abstract
   syntax tree (AST). Type inference analyses this AST and assigns each node a
   type, or produces an error. If the root is given a type, the tree is
   transformed into a typed syntax tree. Finally, the typed syntax tree is
   converted into output Rust tokens for the chewed parser.

*** Parsing
    Chomp uses a parser framework called ~syn~. The parser takes a stream of
    lexical tokens and produces a concrete parse tree. This is then converted
    into an abstract syntax tree, which is used by the type inference stage.
    
    The parser was written by hand. Nibble is a small language, designed to be
    easily parsed. This greatly simplifies the task of writing a parser for it.

    *example: parsing concatenation?*

    After parsing to the concrete syntax tree, the input stage then normalises
    it to an AST. This has two parts, which occur concurrently. One is
    desugaring, which is the elimination of syntactic shortcuts; and the other
    is introduction of De Bruijn indices, which is a way of naming variables.

    Syntactic sugar is part of a language that doesn't add any expressive power,
    and only makes it easier to read. For example, the Nibble expression ~let
    opt(x) = _|x; ...~ is semantically equivalent to ~let opt = /x/ _|x; ...~.
    The let-combinator syntax is redundant, but makes it easier to determine if
    a let expression is a combinator or a full language description. Desugaring
    expands all the syntactic sugar into the base expressions.

    *TODO: segue.*
    In lexically-scoped languages, variable bindings form a stack. Take the
    Nibble expression ~/x/ (/y/ y x) (/z/ z)~. ~x~ is bound first, then ~y~ is
    bound and unbound, and finally ~z~ is bound and unbound. Also note that the
    names of variables have little impact.

    Making use of these two observations, we can arrive at De Bruijn indices,
    where variables are referenced by their position from the top of the stack.
    Using the previous example, the expression becomes ~// (// 0 1) (// 0)~.
    This helps improve the efficiency of later compilation stages, as well as
    spotting usage of undeclared variables early.

    *algorthim: normalisation in practice*
*** Type Inference
    Type inference takes an AST and produces a typed syntax tree, by assigning
    every node a type. This section starts with a discussion about where type
    annotations are stored. Next, it describes the visitor pattern used in the
    type inference algorithm. Finally, it details the design of the variable
    context used during type checking.

    There are generally two ways to annotate a tree with types. Internal
    annotations define a new tree type with almost identical structure to the
    AST. The only difference is that every node also stores type information.
    External annotations provide an external function to find the type of a
    node, without modifying the original data structure at all.

    *Why does Chomp use internal annotations?*
    
    # Chomp uses internal annotations for a few reasons. Firstly, anyone should be
    # able to create and modify an AST but only some languages should have typed
    # syntax trees. By using two separate types, ASTs can have public
    # constructors and typed syntax trees can have private constructors.

    # Secondly, 

    # Secondly, *usage in code generation.*

    Type inference is performed using the visitor pattern. This pattern is
    depicted in figure [[fig:visitor]]. A visitor has different methods each
    accepting a different type of expression. An expression has a method that
    takes a visitor and dispatches the call for the correct expression type.

    #+label: fig:visitor
    #+name: fig:visitor
    #+caption: UML diagram showing the visitor pattern.
    #+begin_figure
    *TODO: finish figure*
    #+end_figure

    The visitor design pattern keeps the code for each visitor in one location,
    instead of split across all the different expression types. This is
    particularly important for reduction, which was also implemented using
    the visitor pattern. Reduction requires many recursive manipulations of the
    expression tree. Keeping each stage separate helps to verify the correctness
    over the whole tree.

    The visitor pattern also provides external users of Chomp an interface to
    include their own manipulations. It can also help if Chomp is split into
    three libraries for the front-end, middle-end and back-end.

    *application: visitor pattern for substitution?*

    Finally there is the variable context. The LM type system splits the
    variable context into two parts: an unguarded context and a guarded context.
    To further complicate things, some variables move back and forward between
    the unguarded and guarded contexts, whilst others are always unguarded. 

    *TODO: rewrite to describe what's implemented*
    
    # Listing [[lst:var-context]] shows the data structure and API I used to solve all
    # these problems. At its core, a ~Context~ is a wrapper for ~vars~, a vector
    # of types. These types are either ~Dynamic~, going between guarded and
    # unguarded, or ~Static~, remaining always unguarded.

    # #+label: lst:var-context
    # #+name: lst:var-context
    # #+caption: Variable context structure and API.
    # #+begin_src rust
    # *TODO: finish figure*
    # #+end_src

    # To determine whether a ~Dynamic~ type is guarded requires looking at
    # ~unguard_points~. This is a separate stack, recording the length of ~vars~
    # when it was last unguarded. If a variable is deeper in the stack then the
    # last index in ~unguard_points~, it is unguarded. Otherwise, it is only
    # unguarded if it is ~Static~.

    # *TODO: proof read. Very fumbly*
    
    # There are four main functions in the API for ~Context~. ~get_variable_type~
    # is a fallible way of retrieving a variable, returning an error if the
    # desired variable is guarded. ~with_unguard~, ~with_unguarded_type~ and
    # ~with_guarded_type~ unguard the context, introduce a new unguarded type and
    # introduce a new guarded type respectively. Each of them takes a function
    # argument. Due to the nature of stacks, each one pushes a value, calls the
    # supplied function, and then pops the value before returning the result.
    # Because Rust is an optimising compiler, it is likely each of these calls
    # will be fully inlined, resulting in no size increase to the call stack.
    
*** Code Generation
    Code generation transforms a typed syntax tree into Rust tokens describing a
    parser. This requires tree parts: translation, type generation and parser
    implementation. First, the tree is translated into an \mre{}. This avoids
    the problem of representing functions as data types. Type generation defines
    all the data types produced by the parser. By using Rust's zero-sized types,
    it is possible to achieve very small storage footprints whilst using rich
    types. The implementation describes how to parse each data type. Rust's
    trait system aids this process.

    Ideally, the translation step would be unnecessary. Instead, Rust's generics
    would allow parametric data types and parser implementations. There are two
    practical problems with this. First, generic type arguments cannot be given
    their own type arguments. Consider the function ~/f/ f "a"~. Ideally, this
    would have the datatype declaration ~type Foo<F> = F<A>;~. Unfortunately,
    the Rust compiler rejects the type argument ~<A>~ on ~F~. Secondly, generic
    type arguments cannot provide compile-time constants to the generic type.
    Consider the alternation ~"a"|"b"~. To get the full performance benefit of
    type-checked parser combinators, the branching conditions, i.e. the first
    sets, for each alternative need to be known at compile time. For a generic
    implementation ~Alt<A, B>~, there is no way to get this information as a
    compile-time constant.

    *TODO: talk about other alternatives?*
    
    Recall reduction used by the KY-\lambda type system, in section
    [[*KY-\(\lambda\) Type System]]. There, reduction stopped when the top-level
    expression was not a let expression or lambda expression. Translation
    instead continues the reduction process until the result has no more lambda
    expressions, let expressions, or function calls. The resulting expression
    can be trivially translated into a \mre{}, as every other component of
    Nibble expressions has a corresponding component in \mres{}. *example*
    
    One downside of this approach is that it creates significantly more code
    than generics would. This could increase the compilation time of the chewed
    parser. Naming types also becomes more challenging. Every instance of a
    *let-lambda* expression creates a new datatype, that all want the same name.
    Chomp solves this by adding a unique number to the end of each type. This
    can make discovering the correct type to use more difficult that generics
    would.

    *Segue.*
    A zero-sized type is a data type that has exactly one instance. For example,
    the unit type ~()~ is zero-sized. Because there is only a single instance, a
    value of a zero-sized type has no information. This means the Rust compiler
    does not need to store values of this type, reducing the memory footprint of
    various data structures. Chomp exploits zero-sized types by translating
    epsilon expressions and literal expressions into zero-sized types. The
    language of an epsilon statement is the empty string, so it only has one
    instance. Similarly, each literal string can be parsed in exactly one way,
    so they also only have one instance. This means literal strings and epsilon
    statements require no storage in the final parse tree.

    Concatenation expressions are translated into structures -- records of
    fields. Whilst the KY type system uses binary concatenation, Nibble uses
    \(n\)-ary concatenation, to match real-world usage. *Example.* 

    Alternation expressions are translated into enumerated types. Similar to
    concatenation, the generated enumerations are \(n\)-ary instead of binary.
    From experience gained writing various tests, it is much more pleasant for
    the user to only consider one, large alternation than many small nested
    ones. *Example*

    *TODO: relevance?* 
    
    By default, Rust stores all values on the stack. The size of values on the
    stack must be known at compile time, so the compiler can allocate enough
    space for them. These two facts mean that Rust does not allow
    directly-recursive data types. *Example: linked list.* This list could store
    zero, one, or more items on the stack, each giving a value with a different
    size. To allow recursion, a recursive reference has to be indirect, for
    example with an owned heap-reference like ~Box~.

    *TODO: describe as implemented*
   
    Now we know how Nibble expressions are converted into data types. Next we
    explore how they are parsed.

    Listing [[lst:parse-def]] gives the core of the definition of the ~Parse~ trait,
    used to parse streams of characters into the data types we made earlier.
    ~take~ removes characters from the stream to produce an instance of the
    type, until the next character is not in the first set of the expression's
    type. For instance, the ~take~ method for epsilon expressions always
    succeeds, leaving the stream alone. The ~take~ method for literal
    expressions succeeds if and only if the stream starts with that literal
    string. *example*.

    #+label: lst:parse-def
    #+name: lst:parse-def
    #+caption: ~Parse~ trait definition
    #+begin_src rust
      fixme!()
    #+end_src

    Because the expression is well-typed, we know that any fixed-point recursion
    is guarded. Therefore fixed-point recursion is unchecked. Likewise, because
    first and flast sets are disjoint for concatenation expressions, the prefix
    can greedily take characters from the stream before the suffix does. For
    alternations, we know that the first sets for each alternative are disjoint,
    so we can easily chose which alternative to parse.

    *example: generated code.*

** Procedural Macros
   Procedural macros are compile-time procedures that operate over Rust tokens.
   They let users perform arbitrary transformations to Rust tokens. This can be
   used for mundane cases, like creating debugging information, to exotic, such
   as embedding Nibble in Rust code.

   Tokens used in Nibble are a subset of tokens used in Rust. This design choice
   means that Chomp could be used as a procedural macro -- it takes Rust
   (Nibble) tokens and produces a different stream of Rust tokens. Integrating
   Nibble into other projects then becomes simple: declare that the project uses
   Chomp, include the ~chewed~ library, write some Nibble in a macro, and then
   enjoy the chewed parser.
   
   *STORY: I'm not sure what I want to say here.*

   One key requirement for Nibble was that it can describe itself. The easiest
   way to test this was to replace the parser in Chomp with a chewed parser.
* Evaluation
  This section starts by discussing whether the project fulfilled the original
  requirements. Next, we perform qualitative analysis of Nibble, such as
  comparing Nibble to other ways of describing languages. Finally, this section
  quantitatively compares the performance of Chomp and chewed parsers against
  other parsing and generation techniques.
  
** Meeting the Success Criterion
   Overall, I have achieved the core requirements of this project, as stated in
   section [[*Requirements Analysis]]. I have also made significant progress with
   the theory of a major stretch requirement, although implementation is
   incomplete. The rest of this section looks back at the success criterion in
   the project [[*Project Proposal]].

   First, Nibble is able to describe itself, as demonstrated in the
   ~autochomp/src/lib.rs~ file. This strongly implies that Nibble is
   sufficiently expressive to describe practical languages.

   Secondly, Chomp accepts this self-description of Nibble and produces a chewed
   parser. That Chomp produces this parser, called /AutoNibble/, means that
   Nibble is well-typed in the KY-\lambda type system. Hence, Nibble is
   unambiguous and can be parsed efficiently.

   Third, Chomp can be modified to use AutoNibble in the parsing stage. The
   ~autochomp~ library produces a binary with an identical interface to Chomp,
   named /AutoChomp/ using the AutoNibble parser. This is evidence that a chewed
   parser can be used successfully in a practical system.

   Finally, AutoChomp produces identical code to Chomp. This is demonstrated by
   the tests in ~autochomp/tests/compare/~. AutoNibble and the parser from Chomp
   are used to parse Nibble expressions, and their outputs are compared. After
   the normalisation stage, Chomp and AutoChomp are identical -- only the parser
   has changed. The fact the tests pass is strong evidence the parsers are also
   identical.

** Analysis of Nibble
   In section [[*Project Overview]], I assert that Nibble was inspired by \mres{}. I
   also claim that it should be able to describe the same languages as BNF. This
   section starts by comparing Nibble to \mres{}. In particular, I determine
   whether Nibble is a practical replacement for \mres{}. Next, I compare Nibble
   against BNF, firstly for expressive power, and then for ease of use in
   untyped and typed applications.

*** Comparison with \mu-Regular Expressions
    In section [[*Syntax of Nibble]], I describe how to translate Nibble expressions
    into \mres{}. This translation is surjective for \mres{} that do not include
    \(\bot\) -- all \mre{} syntax, except for \(\bot\), has an equivalent in
    Nibble. In fact, all \mres{} either express the empty language, or there is
    a Nibble expression with the same language.
    
    I will use three criteria to compare how practical a programming language
    is. One programming language is more practical than another if: there is
    less repetition; the syntax is more descriptive; and there is less cognitive
    load on a programmer. Less repetition means programmers are less likely to
    make mistakes. More descriptive syntax makes reading code easier. Less
    cognitive load means programmers can spend more mental capacity solving the
    problems they want to.

    Nibble is less repetitive than \mres{}, because common subexpressions can be
    extracted into let expressions. This is demonstrated back in figure
    [[fig:nibble-vs-mu]]. *Point out particular expression.* Let expressions also
    make Nibble syntax more descriptive than \mres{}. Labels also allow parts of
    expressions to be given descriptive names inline. Finally, the abstraction
    from let expressions and lambda expressions reduces the cognitive load of
    Nibble compared to \mres{}.

*** Comparison with BNF
    Nibble is as expressive as standard BNF. As discussed earlier, surjective
    translation means that Nibble expressions describe the same set of languages
    as \mres{}. As stated in section [[*Parser Combinators]], Leiß cite:null found
    that \mres{} describe all context-free languages. BNF also describes all
    context-free languages, so Nibble is at least as expressive as BNF.

    Untyped Nibble is a more practical description of languages than BNF. Nibble
    is less repetitive, more descriptive and has a lower cognitive load. BNF can
    only have alternatives at the top level. This means that the BNF declaration
    for the Nibble expression ~match x.("b"|"c").x~ would have to double the
    number of occurrences of ~x~. BNF also has no equivalent for the lambda
    expressions found in Nibble. Going back to figure [[fig:nibble-vs-mu]], whilst
    Nibble can invoke the ~list~ expression twice, BNF has to fully expand it
    twice.

    *TODO: descriptive.*

    BNF uses a single mutually-recursive namespace. This is demonstrated in
    listing [[lst:bnf-mut-rec]]. *Example.* This means that when a programmer finds
    a BNF non-terminal, its declaration could be anywhere in the file. By
    contrast, Nibble uses multiple nested lexical scopes. All variables are
    declared earlier in the expression, either from a let expression or lambda
    expression. *How does this help?*
    
    #+label: lst:bnf-mut-rec
    #+name: lst:bnf-mut-rec
    #+caption: Demonstration of BNF's single mutually-recursive namespace
    #+begin_src text
      TODO: finish source block
    #+end_src

    Typed Nibble is much less practical than BNF used by most other parser
    generators. Typed Nibble reintroduces some repetition. Listing
    [[lst:nibble-left-factor]] shows two Nibble expressions. Their languages are
    equivalent, but the first fails to type check whilst the second succeeds. 

    #+label: lst:nibble-left-factor
    #+name: lst:nibble-left-factor
    #+caption: Left factoring of a Nibble expression
    #+begin_src rust
      // Unfactored
      let x = ...;
      let y = ...;
      let alpha = "a" | "b" | "c" | ... | "z";
      let ident = [rec](alpha.(_|rec));
      match "if".x | "iter".y | ident;

      // Left factored
      let alpha = "a" | "b" | "c" | ... | "z";
      let ident = [rec](alpha.(_|rec));
      let ident_cont = _|ident;
      match
        "i".(_|
             "f".(x|ident_cont)|
             "t".(_|
                  "e".(_|
                       "r".(y|ident_cont)|
                       ("a"|"b"|...).ident_cont)|
                  ("a"|"b"|...).ident_cont)
             ("a"|"b"|...).ident_cont)|
        ("a"|"b"|...).ident_cont;
    #+end_src

    The first expression is not well-typed because the first sets are not
    disjoint. ~"if"~, ~"iter"~ and ~ident~ can all start with ~i~. By
    left-factoring, we remove an ~"i"~ from each expression and then try and
    parse the rest. This has to be recursively repeated until every first set is
    disjoint.

    A left-factored expression has a lot of repetition. Originally, ~ident~ was
    used once, and the long concatenation of characters appeared only once.
    After left-factoring, ~ident_cont~ appears five times, and there are four
    additional long alternations of characters. There is also a huge increase in
    cognitive load. Before, it was easy to tell that something special could
    happen after parsing an ~if~ or an ~iter~. This is obfuscated in the
    left-factored version.

    Parser generators using BNF typically do not experience this problem. The
    bottom-up nature of the parsers generated from BNF allow for BNF
    declarations that are not left factored. They also typically include a
    lexer, which would split ~"if"~, ~"iter~ and ~ident~ into distinct tokens.
    
*** Comparison with Other Rust Libraries
    Finally, we compare how much effort goes is required to integrate Nibble and
    Chomp into a project compared to integrating ~lalrpop~, a popular
    traditional parser generator for Rust. The first major difference is that
    ~lalrpop~ requires using a build script. *why is this bad?*

    ~lalrpop~ features semantic actions that are not present in Nibble. Semantic
    actions provide a way for a parser to execute arbitrary code during parsing.
    This can eliminate the need for parse tree data structures, and let the
    parser build the desired datatype directly. 

    As stated in section [[*Requirements Analysis]], semantic actions were a stretch
    goal for Nibble. Upon further research, there were some conflicting
    requirements that would complicate their implementation. Notably, Nibble
    expressions should be independent of the target programming language of
    Chomp. Typically, parser generators pass semantic actions straight through
    to the target programming language. Such a technique would break the
    requirement for Nibble. Hence, another programming language would have to be
    defined, which could be translated into the target programming language.
    This appeared to be significantly more effort than other stretch
    requirements, so semantic actions have not been added to Chomp.

    Chomp produces Rust source code. Rust has a strong separation between where
    data types are declared and where methods are defined on them. Although
    Chomp declares the parse tree data types and some of their methods, these
    features of Rust allow the user to define their own methods. Using the
    procedural macro system, semantic actions can be defined near the Nibble
    expression, even though they cannot be embedded within it. One problem with
    this approach is that it requires the user to reference data types generated
    by Chomp. Most of these names are automatically generated, and there is no
    easy way to discover those names.

    The final significant usability difference between Nibble and ~lalrpop~ is
    that  ~lalrpop~ uses a lexer. Chomp does not use a lexer for two reasons.
    First, it would require Nibble to include a datatype definition for the
    tokens. To do this in a way that is independent from Chomp's target language
    would need a translator and language syntax. Secondly, many practical
    languages change the lexer behaviour depending on the parsing state. For
    example, characters in a JSON string are treated differently from characters
    elsewhere. If the lexer requires the parser state, and the parser can
    perform the functions of the lexer, the complexity of adding a stateful
    lexer does not seem justified to me.
** Quantitative
   A major claim of Chomp is that it produces linear time parsers. We evaluate
   this by looking at the performance of AutoNibble. Next, we compare the
   performance of AutoNibble to the handwritten Nibble parser. Finally, we
   compare the performance of chewed parsers against both handwritten and
   another generated parser for two languages: JSON, and arithmetic.
   
*** Methodology
    Benchmarks were performed using the ~criterion~ library for Rust. Each
    benchmark consists of a single function evaluation, repeated many times.
    During a three second warm-up period, the benchmark function is ran for an
    increasing number of iterations. This is used to approximate how many
    iterations can be performed in a five second timing window.

    This iteration estimate is divided by 5050 (the 100th triangle number). This
    splits the five second timing window into 5050 work units. The wall time
    is measured after one work unit, then after a further two units, then after
    three more and so on. This results in 100 samples for each benchmark
    function. Using linear regression, we can approximate the duration of one
    work unit and from that we can calculate the duration of one function
    iteration.

    Benchmarks only use a single thread at a time. It was performed on a desktop
    computer with the following specifications: *TODO: specs*

*** AutoNibble
    This section explores the performance of AutoNibble. We start by showing
    that the performance is more likely to be linear than polynomial. Next, we
    compare AutoNibble and the performance of the original Nibble parser.

    Figure [[fig:autonibble-chomp-nibble]] shows the performance of AutoNibble and
    the handwritten Chomp parser on expressions of various sizes. The largest
    input is a Nibble expression used to describe a past iteration of Nibble.
    Smaller inputs are small modifications to truncated versions, each roughly
    twice the size of the last.

    #+label: fig:autonibble-chomp-nibble
    #+caption: Performance of AutoNibble against the handwritten Nibble parser on expressions of various sizes.
    #+name: fig:autonibble-chomp-nibble
    [[./autonibble.png]]

    *TODO: test that performance is linear.*

    Figure [[fig:autonibble-chomp-nibble]] suggests that AutoNibble is more
    efficient than the handwritten Nibble parser. This is possibly because
    AutoNibble has fewer features than the full Nibble parser.

    Firstly, AutoNibble can only parse the ASCII subset of Nibble. The Nibble
    language is defined on Unicode characters. AutoNibble was restricted to the
    ASCII subset because it is only a technical demonstration of Nibble and
    Chomp.

    Secondly, AutoNibble discards information about the origin of tokens. To aid
    users in writing well-typed Nibble, Chomp preserves the source code location
    of tokens. When Chomp finds a type error, it can report the exact location
    to users. AutoChomp will not be used in practice, so it does not construct
    or preserve this information.

    Finally, *third thing?*
    
    * Chewed parsers have linear time complexity
      * Use an F test - linear is no worse than other model
        * linear vs exponential: \(y = a + bx + e ^{cx} + \epsilon\)
        * linear vs polynomial: \(y = a + b x^c + \epsilon\)
    * Chewed parser performance
      * Use a Chow test - combined is no worse than separate
*** Chewed Parser Performance
    Figures [[fig:bench-json]] and [[fig:bench-arith]] compare the performance of chewed
    parsers against handwritten and other generated parsers for two different
    languages, JSON and arithmetic. For JSON, the parsers were made to output
    a Rust representation of the object. For arithmetic, the parsers computed
    the final value of the expression. These extra steps emulate using the
    parsers in practice -- the target data type is fixed, but a developer can
    choose how to arrive there. 

    #+label: fig:bench-json
    #+name: fig:bench-json
    #+caption: Performance comparision of various parsers for consuming JSON.
    [[./json.png]]

    #+label: fig:bench-arith
    #+name: fig:bench-arith
    #+caption: Performance comparision of various parsers for consuming arithmetic.
    [[./arith.png]]

    In both cases, the handwritten parser performs better than the chewed parser
    and the other generated parser. There are some potential reasons for this.
    Firstly, the handwritten parser produces values of the target datatype
    directly. Because the developer is in control of all the code, there do not
    need to be any intermediate conversions. In contrast, chewed parsers have to
    produce a full parse tree before conversion. *What about ~lalrpop~?*

    Secondly, *another reason.*

    Chewed parsers have comparable performance to ~lalrpop~ parsers. *Why?*
    *Can verify this claim with a Chow test.*

* Conclusion
  My project was a success. I completed the success criterion of implementing
  and testing AutoChomp. AutoChomp even managed to outperform Chomp, although
  this could be due to AutoChomp having a less-powerful parser.
  
  I also completed one extension requirement and made significant progress
  towards implementing another. Chomp is integrated with Rust's procedural macro
  system, which makes integrating Chomp much easier than integrating some other
  parser generators. I also designed the LM type system, which can assign
  polymorphic function types to Nibble expressions.
  
  Chewed parsers have performance comparable to other generated parsers
  (section [[*Quantitative]]) and, unlike ordinary parser combinators, come
  with a compile-time guarantee of linear performance (section [[*AutoNibble]]).
  
** Lessons Learned
   My courses during Lent term took up more time than I anticipated. This lead
   to me being able to spend less time on my project than I had hoped.
   Fortunately, my proposal allocated most time during Lent term to working on
   extensions. This, and other scheduled slack time, allowed me to ensure the
   core of my project was complete and at a high standard.
    
   There are broadly two architectures when building a compiler or translator,
   such as Chomp. The first uses transformations, where pipeline stages are
   applied to the whole input sequentially. The second uses queries, which uses
   memoisation to perform different pipeline stages on different parts of the
   input in an arbitrary order. Chomp uses transformations, which were
   acceptable during early development for the KY type system, and when
   translating expressions before type checking. For the KY-\lambda type system,
   code generation requires the original expression structure, whilst
   type-checking performs translation first. Keeping these two structures
   together is much more challenging for transformational systems compared to
   query-based ones.
    
   Whilst there was a very low initial cost for using a transformational
   architecture, the cost became very large as the type system became more
   complex. If I spent more time researching extensions to the KY type system
   before development began, I could have avoided this large, deferred cost.
   This advice is almost certainly applicable to future projects I might
   undertake.
    
   During benchmarking, I discovered a bug with the handwritten Nibble parser.
   Despite it not using any global state, each parse iteration would take some
   more time to run. Instead of the iteration-time graph being linear as
   expected, it was quadratic or maybe even cubic. I spent several days trying
   to find the cause of the problem, before deciding it would be more effective
   to start rewriting the parser instead. I started by writing the most complex
   again from scratch. After this five-minute rewrite, I decided to test the
   benchmarks again and found the iteration-time graph became linear. I still do
   not know the cause of the bug, but I cam away with the knowledge that
   rewriting code can be significantly faster than finding the cause of bugs.
** Future Work
   The LM Type system generates the potential for lots of future work. First,
   there is no proof that it is a useful type system. Whilst the modifications
   to the Hindley-Milner and KY type systems are small, these changes could have
   drastic consequences to soundness and completeness properties. Formal proofs
   of these properties are necessary to make sure the LM type system achieves
   what it sets out to complete.
    
   Secondly, Chomp could be modified to use the LM type system. Even without
   proof, a practical implementation can provide evidence for the claims made by
   the LM type system. It can also help to justify some design decisions of the
   LM type system, such as the choice to use structural unification for types.
   Work on this implementation can proceed alongside a proof for the type
   system. Using a dependant-type system such as Agda could allow for the proof
   and implementation to be tightly coupled, in the sense that changes to one
   necessitates changes to the other.
    
   The LM type system probably does not accept expressions for a larger class of
   languages than the KY type system does. Neither type system accepts an \mre{}
   such as \( (\epsilon \vert a) \cdot b \), due to the \(\circledast\)
   constraint, even though there is a linear-time parser for this language.
   Searching for a type system for \mres{} that permits accepts a wider range of
   expressions would make writing well-typed Nibble easier.
    
   Recall that an unrestricted version of the KY-\lambda type system can perform
   arbitrary computation. This makes it possible to write a repeat-\(n\)-times
   combinator using the KY-\lambda type system. Due to the way polymorphism is
   used by the LM type system, such combinators are impossible, because there is
   no way to encode the natural numbers. Nibble could be extended to support the
   naturals and other data types, making it possible to write a richer set of
   parser combinators.
* References
  \printbibliography[heading=none]{}

* Project Proposal
  TODO!
