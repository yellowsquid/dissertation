#+latex_class: dissertation
#+latex_class_options: [12pt,a4paper,twoside]
#+latex_header: \usepackage[hyperref=true,url=true,backend=biber,natbib=true]{biblatex}
#+latex_header: \usepackage[vmargin=2cm,hmargin=1in]{geometry}
#+latex_header: \usepackage[chapter]{minted}
#+latex_header: \usepackage[binary-units]{siunitx}
#+latex_header: \usepackage{booktabs,ebproof,parskip,standalone,stmaryrd,syntax}
#+latex_header: \addbibresource{diss.bib}

# math operators
#+latex_header: \DeclareMathOperator{\True}{true}
#+latex_header: \DeclareMathOperator{\False}{false}
#+latex_header: \DeclareMathOperator{\If}{if}
#+latex_header: \DeclareMathOperator{\Then}{then}
#+latex_header: \DeclareMathOperator{\Else}{else}
#+latex_header: \DeclareMathOperator{\Let}{let}
#+latex_header: \DeclareMathOperator{\In}{in}
#+latex_header: \DeclareMathOperator{\Null}{null}
#+latex_header: \DeclareMathOperator{\First}{first}
#+latex_header: \DeclareMathOperator{\Flast}{flast}

# shorthand
#+latex_header: \newcommand\mre{\(\mu\)-regular expression}
#+latex_header: \newcommand\mres{\(\mu\)-regular expressions}
#+latex_header: \newcommand\ky{KY}
#+latex_header: \newcommand\hm{Hindley-Milner}
#+latex_header: \newcommand\debruijn{De~Bruijn}

# try to avoid widows and orphans
#+latex_header: \raggedbottom
#+latex_header: \sloppy
#+latex_header: \clubpenalty1000%
#+latex_header: \widowpenalty1000%

# Other options
#+options: toc:nil H:6

# Word count
#+name: word-count
#+begin_src shell :exports none
  tmp="$(mktemp)"
  sed -e '/begin{minted}/,/end{minted}/d' diss.tex >"$tmp"
  texcount -sum -1 "$tmp"
  rm "$tmp"
#+end_src

#+RESULTS: word-count
: 11062

#+begin_src emacs-lisp :exports none
  (defun tables-recalc (backend)
    (org-table-recalculate-buffer-tables))

  (add-hook 'org-export-before-processing-hook #'tables-recalc)
#+end_src

#+RESULTS:
| tables-recalc |

#+latex: %TC:ignore
# ##############################################################################
# Title
\pagestyle{empty}
\rightline{\LARGE\bf Greg Brown}

\vspace*{60mm}
\begin{center}
\Huge
{\bf A Typed, Algebraic Parser Generator} \\[5mm]
Computer Science Tripos -- Part II \\[5mm]
Queens' College \\[5mm]
\today
\end{center}
  
# ##############################################################################
# Declaration of Originality 
\pagebreak{}

** Declaration
   :PROPERTIES:
   :UNNUMBERED: notoc
   :END:

   I, Greg Brown of Queens' College, being a candidate for Part II of the
   Computer Science Tripos, hereby declare that this dissertation and the work
   described in it are my own work, unaided except as may be specified below,
   and that the dissertation does not contain material that has already been
   used to any substantial extent for a comparable purpose.

   I am content for my dissertation to be made available to the students and
   staff of the University.

   \bigskip
   \leftline{Signed Greg Brown}
   
   \medskip
   \leftline{Date \today}

# ##############################################################################
# Proforma
* Proforma
  :PROPERTIES:
  :UNNUMBERED: notoc
  :END:
  
  \pagestyle{plain}
  \pagenumbering{roman}

  | \large Candidate Number:   | \large 2374B                                                                                          |
  | \large Project Title:      | \large *A Typed, Algebraic Parser Generator*                                                          |
  | \large Examination:        | \large *Computer Science Tripos -- Part II, 2021*                                                     |
  | \large Word Count:         | \large 11062[fn:: Calculated using ~texcount~. \url{https://app.uio.no/ifi/texcount/}]                |
  | \large Line Count:         | \large 4758[fn:: Calculated using ~scc~, ignoring test inputs. (\url{https://github.com/boyter/scc})] |
  | \large Project Originator: | \large The dissertation author                                                                        |
  | \large Supervisor:         | \large Prof. Alan Mycroft                                                                             |
  #+TBLFM: @4$2='(concat "\\large " (org-sbe "word-count") "[fn\:\: Calculated using ~texcount~. (\\url{https://app.uio.no/ifi/texcount/})]")

** Original Aims of the Project

   The core aims were to design a language /Nibble/ to describe formal
   languages, and then implement the \ky{} type system in a parser generator
   /Chomp/, which takes Nibble expressions as input. Unlike traditional parser
   generators, which view formal languages as automata, Chomp instead takes the
   approach that languages form an algebraic system. Nibble and parsers
   generated by Chomp would be compared against the traditional analogues,
   evaluating whether this new approach to parsing has benefits for language
   designers, conceptually or computationally.
   
** Work Completed
   
   Exceeded success criterion. One extension was completed which added functions
   to the Nibble language and the \ky{} type system, reducing repetition in
   Nibble and redundant computations in Chomp. Two different type systems were
   developed for the Nibble language, including one with polymorphic types.
   Chomp can successfully generate parsers from Nibble expressions. These
   parsers have comparable performance to traditionally-generated parsers, and
   in some cases outperform handwritten parsers.
   
** Special Difficulties
   None.

# ##############################################################################
# Contents

#+toc: headlines 2
list-of-figures:nil
#+toc: listings
# #+toc: tables

# ##############################################################################
# Dissertation Body
#+latex: %TC:endignore
* Introduction
  \pagestyle{headings}
  \pagenumbering{arabic}
  
  A /formal language/ is a set of strings over some alphabet of symbols. For
  example, a dictionary enumerates a language over written characters. Spoken
  English is a language, where the alphabet consists of phonemes. Programming
  languages are over an alphabet of ASCII or Unicode characters. IP packets are
  languages over bytes. The language of a decision problem is the set of valid
  inputs.
  
  Whilst linear strings are helpful for data storage and transmission, they have
  limited use for other algorithms. Strings are used to encode other non-linear
  data. A /parser/ is a function that takes a language string and decodes it to
  return the underlying data. Parsers should be fast; why spend time decoding
  data when there is useful computation to be done? Unfortunately, hand writing
  efficient parsers is repetitive, with lots of boiler plate, and requires
  careful consideration of the order of operations.

  To help solve this problem, /parser generators/ were developed -- programs
  that take descriptions of languages and output source code for a parser.
  Typical efficient parser generators operate by parsing languages with a finite
  state machine. By storing the state transition table in memory, the run-time
  cost of a parser is minimal.

  As optimising compilers become more powerful, more complex source code can be
  compiled down to equally-efficient machine code. This allows us to approach
  parsing from a different direction, without incurring a performance penalty.
  Instead of parsing formal languages using finite state machines, languages can
  be interpreted as algebraic expressions. One example algebra is \mres{}, which
  were described by Leiß cite:10.1007/BFb0023771. This algebraic approach allows
  for the creation of intuitive function-based parsing algorithms, as opposed to
  the opaque parsing tables used in traditional parser generators.

  Krishnaswami and Yallop took this approach in a recent paper
  cite:10.1145/3314221.3314625. One key contribution the pair made was the
  development of a type system (the \ky{} type system) for \mres{}. If a \mre{}
  was well-typed, then they could produce a linear-time parser for it. They
  produced a implementation of the \ky{} type system using parser combinators,
  which are higher order functions that combine zero or more parsers together to
  make a new parser.
  
** Project Overview
  This project seeks to produce a parser generator based on \mres{}. It makes
  the following contributions:
  
   * I design the /Nibble/ language to describe parsers, based on \mres{}
     (section [[*The Nibble Language]]). This language adds functions and let
     bindings to \mres{}, making it more ergonomic. I determine that Nibble is
     as easy to use as BNF, meaning future parser generators could use
     Nibble-like languages as input.

   * I describe *two* type systems for the Nibble language, extending the \ky{}
     type system (section [[*Type Systems for Nibble]]). Both type systems have
     simple inference algorithms, eliminating the need for type annotations in
     the Nibble language.

   * I implement the /Chomp/ parser generator, which produces Rust source code
     for parsers from a Nibble description (section [[*Chomp Repository Overview]]).
     Chomp uses one of the type systems I described to ensure Chomp-generated
     parsers run in linear time (section [[*Performance of Chomp-generated
     Parsers]]).

   * I demonstrate the Nibble language and Chomp parser generator are suitable
     for use in complex projects by creating /AutoNibble/ (section [[*Meeting the
     Success Criterion]]). AutoNibble is a Chomp-generated parser for the Nibble
     language. AutoNibble outperforms a handwritten parser for the Nibble
     language (section [[*Performance of AutoNibble]]).

* Preparation
  I begin this chapter by describing the wider computer science necessary to
  understand the rest of this dissertation. Next, I discuss the requirements for
  the Nibble language and the Chomp parser generator, and the software
  engineering techniques used to achieve them. I then mention the starting point
  of the project.
  
** Background
   This section starts with a recap on formal languages, from the perspective of
   formal grammars and finite automata. Next it covers \mres{} and the \ky{}
   type system, which view languages as algebraic objects. It then skips over to
   discuss translators, in particular the architecture they typically use. The
   section concludes with a discussion on the features of Rust used by the
   implementation of the Chomp parser generator.
   
*** Formal Languages
    A formal language is a set of strings over some finite alphabet. For
    example, written English words are a formal language over the English
    alphabet, spoken sentences are a formal language over phonemes, and
    programming languages such as Rust are formal languages over Unicode
    characters.

    Most useful formal languages have some structure to them, where every string
    has a derivation that describes this structure. Parsing is the task of
    computing a derivation from a string. Consider the following example. Sheep
    can only say "baa" followed by some number of additional "a"s. The
    derivations for this sheep language could be the natural numbers. A parser
    would count the total number of "a"s, and subtract two. Notice how a
    derivation has no connection to the meaning, or semantics, of a string.

    A parser generator is a program that takes a description of a formal
    language and produces a parser for it. Because all of the strings in a
    language can be generated from a derivation, and a parser finds a derivation
    for a given string, a parser generator only needs to receive a description
    of the form of derivations to be able to generate a parser.

**** The Chomsky Hierarchy
     Traditionally, languages have been specified using formal grammars. We
     extend the original alphabet with some additional /non-terminal/ symbols.
     One of these is the start symbol, \(S\). To create a string in the language
     of a grammar, we start with the string consisting of the start symbol.
     Then, we repeatedly apply string rewriting rules called /production rules/
     until there are no more non-terminal symbols. Every production rule must
     consume at least one non-terminal, although they can produce many more.
     
     An example grammar, and the derivation of a string in the grammar, are
     shown in figure [[fig:grammar-sheep]]. This grammar describes the language
     used by sheep. The start symbol gives us the prefix "baa", and a looping
     non-terminal \(A\). \(A\) either pushes an "a" symbol before it, or
     removes itself from the string.

     #+label: fig:grammar-sheep
     #+name: fig:grammar-sheep
     #+caption: An example formal grammar and a derivation.
     #+begin_figure
     \centering
     \begin{align*}
       S &\Mapsto baaA \\
       A &\Mapsto aA \\
       A &\Mapsto \epsilon
     \end{align*}
     \begin{math}
       S \Mapsto baaA \Mapsto baaaA \Mapsto baaaaA \Mapsto baaaa
     \end{math}
     #+end_figure

     Chomsky cite:10.1016/S0019-99585990362-6 detailed a classification of
     formal grammars depending on the form of the production rules: type 0
     through type 3. The smaller the number, the less restricted the rules are,
     and the larger the class of possible languages. Chomsky further discovered
     that each class can be parsed by a different form of finite automata.

     The languages of type 2 grammars are commonly called /context-free
     languages/. These are the most-restrictive grammars in the hierarchy that
     have matched delimiters, which are essential for programming languages.
     These grammars take polynomial time to parse in general. Fortunately, there
     are some sub-classes of context-free languages that can be parsed in linear
     time. The most common of these are LL and LR languages, covered in the Part
     IB Compiler Construction course.
     
**** Backus-Naur Form
     BNF is a formal language to describe grammars. Its syntax is designed to
     resemble the production rules of the mathematical definition. Literal
     symbols are surrounded by quotes. Non-terminal symbols are surrounded by
     angle brackets. Listing [[lst:bnf-sheep]] shows a BNF description of the sheep
     language. The ~<start>~ form corresponds to rules for the \(S\)
     non-terminal. Similarly, the ~<loop>~ form corresponds to the \(A\)
     non-terminal.

     #+label: lst:bnf-sheep
     #+name: lst:bnf-sheep
     #+caption: An example BNF description.
     #+begin_src bnf
     <start> ::= "baa" <loop>
     <loop>  ::= "" | "a" <loop>
     #+end_src

     BNF has a single global namespace, such that when a form is declared, it
     can be used anywhere else in the description. For example, ~<loop>~ is used
     before its declaration. BNF uses mutually-recursive scope -- different
     forms can refer to themselves in a cycle.
*** \mres{}
    As an alternative to viewing languages as described by grammars, languages
    are also algebraic objects. This was the viewpoint considered by Leiß when
    they described \mres{} cite:10.1007/BFb0023771, described in figure
    [[fig:mre-syntax]].
    
    #+label: fig:mre-syntax
    #+name: fig:mre-syntax
    #+caption: The syntax of \mres{}
    #+begin_figure
    \centering
    \begin{math}
      e = \bot
        \mid \epsilon
        \mid c
        \mid e \cdot e
        \mid e \vee e
        \mid \mu x. e
        \mid x
    \end{math}
    #+end_figure

    There are three constant languages: \(\bot\) for the empty language,
    \(\epsilon\) for the language of the empty string only, and \( c \) for a
    language containing the single-symbol string \( c \) only.

    These are combined with two binary operators. Concatenation, \( g \cdot g'
    \) takes strings from \(g\) and concatenates them with strings from \(g'\).
    Alternation, \( g \vee g' \), forms the union of the languages \(g\) and
    \(g'\). For brevity, sometimes juxtaposition is used instead of the
    concatenation operator.

    The last expression form is the least-fixed-point operator, \(\mu x. g(x)\).
    This finds the smallest language for \(x\) that contains all the strings in
    \(g(x)\).

    Figure [[fig:mre-sheep]] shows an example \mre{}, again describing the sheep
    language. Like the BNF example (listing [[lst:bnf-sheep]]), it starts with the
    constant prefix \(baa\). Next it has a fixed point expression. This is the
    union of the empty string and the symbol \(a\) followed by the fixed point
    expression -- a string of zero or more "a" symbols.

    #+label: fig:mre-sheep
    #+name: fig:mre-sheep
    #+caption: An example \mre{}.
    #+begin_figure
    \centering
    \begin{math}
      baa \cdot \mu x. (\epsilon \vee a \cdot x)
    \end{math}
    #+end_figure
    
    Leiß cite:10.1007/BFb0023771 found that \mres{} describe the full set of
    context-free languages only. This means that for every \mre{}, there is a
    BNF description for the same language.

    Unlike BNF, where alternatives are split into many small, reusable rules,
    \mres{} are always part of one long expression. This has problems for
    readability, especially for some repetitive real-world languages. See figure
    [[fig:mre-hex-colour]] which gives a \mre{} for describing a colour in
    hexadecimal ~#RRGGBBAA~ format, where the alpha component is optional. The
    whole list of hexadecimal digits is listed eight times.
    
    #+label: fig:mre-hex-colour
    #+name: fig:mre-hex-colour
    #+caption: \mres{} can contain a lot of repetition. The full list of
    #+caption: hexadecimal digits must be listed eight times.
    #+begin_figure
    \centering
    \begin{math}
      \# \cdot (0 \vee \cdots \vee F) \cdot (0 \vee \cdots \vee F) \cdot (0 \vee \cdots \vee F) \cdot (0 \vee \cdots \vee F) \cdot (0 \vee \cdots \vee F) \cdot (0 \vee \cdots \vee F) \cdot (\epsilon \vee (0 \vee \cdots \vee F) \cdot (0 \vee \cdots \vee F))
    \end{math}
    #+end_figure
   
**** The \ky{} Type System
     The \ky{} type system is a type judgement for \mres{}. If an expression is
     well typed, then there exists a linear-time parser for the language of the
     expression.
    
     There are three properties of languages that are particularly interesting,
     named \( \Null \), \( \First \) and \( \Flast \). Their definitions are in
     figure [[fig:lang-props]]. To summarise, a language \( L \) is \( \Null \) when
     it contains the empty string. The \( \First \) set is the set of symbols
     starting strings in \( L \), and the \( \Flast \) set is the set of symbols
     that immediately follow strings in \( L \) to make a bigger string in \( L
     \).
    
     #+label: fig:lang-props
     #+name: fig:lang-props
     #+caption: Definitions of the \( \Null \), \( \First \) and \( \Flast \)
     #+caption: properties of languages.
     #+begin_figure
     \centering
     \begin{math}
       \Null L \iff \epsilon \in L
     \end{math}
     \begin{align*}
       \First L &= \{ c \in \Sigma \mid \exists w \in \Sigma^*.\, cw \in L \} \\
       \Flast L &=
          \{ c \in \Sigma
          \mid \exists w \in \Sigma^+, w' \in \Sigma^*.\,
            w \in L \wedge wcw' \in L
          \}
     \end{align*}
     #+end_figure
    
     A /\ky{} type/ \( \tau \) is a record \( \{\textsc{Null} \in \mathbb{B} ,
     \textsc{First} \subseteq \Sigma , \textsc{Flast} \subseteq \Sigma \}\). As
     types are triples of values, they can be manipulated by functions. Figure
     [[fig:mre-type]] shows some basic types and some operations on them. It also
     describes two constraints on types, used by the typing judgement.
    
     #+label: fig:mre-type
     #+name: fig:mre-type
     #+caption: The \ky{} types, two binary operations on them, and the two
     #+caption: constraints \(\circledast\) and \(\#\).
     #+begin_figure
     \centering
     \begin{math}
       b \Rightarrow s = \If b \Then s \Else \emptyset
     \end{math}
     \begin{align*}
       \tau_{\bot} &= ( \False, \emptyset, \emptyset ) \\
       \tau_{\epsilon} &= ( \True, \emptyset, \emptyset ) \\
       \tau_{c} &= ( \False, \{ c \} , \emptyset )
     \end{align*}
     \begin{align*}
       \tau \vee \tau' &= \left\{ \begin{array}{rl}
            \textsc{Null} = &\tau.\textsc{Null} \vee \tau'.\textsc{Null} \\
            \textsc{First} = &\tau.\textsc{First} \cup \tau'.\textsc{First} \\
            \textsc{Flast} = &\tau.\textsc{Flast} \cup \tau'.\textsc{Flast}
          \end{array}\right\} \\
       \tau \cdot \tau' &= \left\{ \begin{array}{rl}
            \textsc{Null} = &\tau.\textsc{Null} \wedge \tau'.\textsc{Null} \\
            \textsc{First} = &\tau.\textsc{First} \cup (\tau.\textsc{Null} \Rightarrow \tau'.\textsc{First}) \\
            \textsc{Flast} = &\tau'.\textsc{Flast} \cup (\tau'.\textsc{Null} \Rightarrow \tau'.\textsc{First} \cup \tau.\textsc{Flast})
          \end{array}\right\}
     \end{align*}
     \begin{align*}
       \tau \circledast \tau' &= (\tau.\textsc{Flast} \cap \tau'.\textsc{First} = \emptyset) \wedge \neg \tau.\textsc{Null} \\
       \tau \# \tau' &= (\tau.\textsc{First} \cap \tau'.\textsc{First} = \emptyset) \wedge \neg (\tau.\textsc{Null} \wedge \tau'.\textsc{Null})
     \end{align*}
     #+end_figure

     The \ky{} type system uses two different variable contexts, so it can
     distinguish between /guarded/ and /unguarded/ variables. Guarded variables
     can only occur on the right of a non-empty string. This is to make sure
     recursion is deterministic in a naive parser implementation.
     
     Figure [[fig:ky-rules]] gives the full typing judgement of the \ky{} type
     system. Of particular note, the KYFix rule assumes \( x \) is guarded in
     the hypothesis, the KYCat rule shifts the guarded context into the
     unguarded one for the argument on the right side, and the KYVar rule can
     only reference unguarded variables. Krishnaswami and Yallop showed
     cite:10.1145/3314221.3314625 that if an expression has a complete typing
     judgement when the two variable contexts are empty, it is possible to
     compute a parser for the language of that expression.
    
     #+label: fig:ky-rules
     #+name: fig:ky-rules
     #+caption: The \ky{} typing judgement.
     #+begin_figure
     \centering
     \begin{math}
     \begin{array}{ccc}
       \begin{prooftree}
         \infer0[KYBot]{\Gamma; \Delta &\vdash \bot : \tau_{\bot}}
       \end{prooftree}
       & \qquad &
       \begin{prooftree}
         \infer0[KYEps]{\Gamma; \Delta &\vdash \epsilon : \tau_{\epsilon}}
       \end{prooftree}
       \\
       & \qquad &
       \\
       \begin{prooftree}
         \infer0[KYChar]{\Gamma; \Delta &\vdash c : \tau_c}
       \end{prooftree}
       & \qquad &
       \begin{prooftree}
         \infer0[KYVar]{\Gamma, x : \tau; \Delta &\vdash x : \tau}
       \end{prooftree}
       \\
       & \qquad &
       \\
       \begin{prooftree}
         \hypo{\Gamma; \Delta &\vdash e : \tau} 
         \hypo{\Gamma; \Delta &\vdash e' : \tau'} 
         \hypo{\tau \# \tau'}
         \infer3[KYAlt]{\Gamma; \Delta &\vdash e \vee e' : \tau \vee \tau'}
       \end{prooftree}
       & \qquad &
       \begin{prooftree}
         \hypo{\Gamma; \Delta, x : \tau &\vdash e : \tau} 
         \infer1[KYFix]{\Gamma; \Delta &\vdash \mu x. e : \tau}
       \end{prooftree}
       \\
       & \qquad &
       \\
       \begin{prooftree}
         \hypo{\Gamma; \Delta &\vdash e : \tau} 
         \hypo{\Gamma, \Delta; \cdot &\vdash e' : \tau'} 
         \hypo{\tau \circledast \tau'}
         \infer3[KYCat]{\Gamma; \Delta &\vdash e \cdot e' : \tau \cdot \tau'}
       \end{prooftree}
       & \qquad &
     \end{array}
     \end{math}
     #+end_figure
*** The \hm{} Type System
    The simply-typed lambda calculus is possibly the simplest type system,
    consisting of ground terms and functions only. System F extends the lambda
    calculus by adding /polymorphism/, where values can have multiple types.
    Unfortunately, type inference, the problem of assigning types to
    expressions, is undecidable for System F cite:10.1109/LICS.1994.316068.

    To overcome this problem, Hindley cite:10.2307/1995158 and later Milner
    cite:10.1016/0022-00007890014-4 described a different type system with
    decidable inference. Like System F, it has polymorphic types and type
    variables. The difference is that only let expressions can have polymorphic
    types. The typing rules are detailed in figure [[fig:hm-rules]].

     #+label: fig:hm-rules
     #+name: fig:hm-rules
     #+caption: The \hm{} typing judgement.
     #+begin_figure
     \centering
     \begin{math}
     \begin{array}{ccc}
     \begin{prooftree}
       \hypo{\tau = S \sigma}
       \infer1[HMVar]{\Gamma, x : \sigma \vdash x : \tau}
     \end{prooftree}
     & \qquad &
     \begin{prooftree}
       \hypo{\Gamma \vdash e : \tau \to \tau'}
       \hypo{\Gamma \vdash e' : \tau}
       \infer2[HMApp]{\Gamma \vdash e e' : \tau'}
     \end{prooftree}
     \\ & \qquad & \\
     \begin{prooftree}
       \hypo{\Gamma, x : \tau \vdash e : \tau'}
       \infer1[HMAbs]{\Gamma \vdash \lambda x. e : \tau \to \tau'}
     \end{prooftree}
     & \qquad &
     \begin{prooftree}
       \hypo{\Gamma \vdash e : \tau}
       \hypo{\Gamma, x : \forall \alpha. \tau \vdash e' : \tau'}
       \infer2[HMLet]{\Gamma \vdash \Let x = e \In e' : \tau'}
     \end{prooftree}
     \end{array}
     \end{math}
     #+end_figure
     
    A key part of the \hm{} type system is /specialisation/. This is the
    instantiation of one or more free variables in a polymorphic type. If a type
    \sigma can be specialised to type \(\sigma'\) by a map \(S\) of
    instantiations, then \(\sigma' = S \sigma\). Only the HMVar rule can
    specialise types.

    Conversely, the HMLet rule /generalises/ types. First, the bound expression
    is type checked. Then, all the free type variables are bound by the
    universal quantification, before the body is type checked with this new
    expression.

*** Translators
    Translators are programs that transform one formal language into another
    whilst preserving the semantics. A familiar example are natural language
    translators, which map sentences from one language into another whilst
    preserving the meaning. An example from computer science are compilers,
    which translate source code into machine code, such that when they are both
    executed the result is the same.

    Translators consist of three different phases, named "ends". The front-end
    parses the source language into a source derivation. The middle-end
    transforms the source derivation into a target derivation, preserving the
    semantics. The back-end generates the target language from the target
    derivation.

    We will consider two examples: compilers and parser generators. The
    front-end of a compiler parses the source code into an abstract syntax tree.
    Functional languages typically introduce \debruijn{} indices at this stage,
    which are introduced later in this section. The middle-end has two roles.
    The first is to type check the abstract syntax tree. If this fails, then the
    compiler cannot guarantee that language semantics are preserved, so
    execution stops. Otherwise, the middle-end produces intermediate code -- the
    derivation for machine code. Using this intermediate representation, the
    back-end produces machine code. In some cases, the back-end is itself a
    translator.

    Parser generators are another example of translators. The front-end of a
    parser generator receives a string describing a formal language. This is
    parsed into an abstract syntax tree for the input language. The middle-end
    then attempts to produce an abstract description for the parsing algorithm.
    Some parser generators produce action tables you would use to describe
    Turing machines. Others create decision trees. In any case, if the generator
    cannot produce an algorithm that matches the described formal language then
    the translator produces an error. The back-end of parser generators use the
    abstract algorithm description to generate source code for the target
    programming language.

    Often, the most complex part of a translator is the middle-end. By creating
    a strong separation between the three phases, the front-end and back-ends
    can be easily modified or replaced to accept different source languages or
    output different target languages respectively. For example, the back-end of
    many compilers can produce machine code for different instruction sets and
    platforms. Clang and GCC, two popular C compilers, can both act as
    front-ends to the GCC compiler.

**** \debruijn{} Indices
     Most functional programming languages have a property called
     \alpha-renaming. To summarise, given any program, if you rename all
     occurrences of any variable then the semantics of the program do not
     change. \debruijn{} indices cite:10.1016/1385-72587290034-0 exploit the
     lexical scoping features of functional programming languages to provide all
     variables with a name that is invariant under \alpha-renaming. This can
     simplify many algorithms, such as substitution.

     Consider the OCaml expression ~(fun x y -> x y) y~ . If we try to naively
     evaluate the expression, by substituting ~y~ for ~x~, we get the term ~(fun
     y -> y y)~. However, this has different semantics than the original
     expression. A correct substitution would be \linebreak ~(fun z -> y z)~.
     Notice how we had to rename the bound variable.

     Using \debruijn{} indices, there is no longer a need to rename variables.
     Observe how variables form a stack -- first, ~y~ is declared at some point
     earlier in the program. Within the anonymous function, ~x~ and ~y~ are
     bound to new variables, and outside of the function those bindings are
     popped off. \debruijn{} indices represent variables by their position from
     the top of the variable stack.
     
     The OCaml expression from earlier becomes ~(fun . . -> 0 1) 0~ when using
     \debruijn{} indices, assuming that ~y~ was the last variable declared
     before this expression. Notice how the anonymous function does not need to
     name its parameters -- the \debruijn{} indices uniquely identify every
     variable.

     After substitution, the function becomes ~(fun . -> 1 0)~. First, the
     variable ~x~ eliminated, hence removed from the variable stack. That means
     that the ~y~ inside the anonymous function was promoted to the top of the
     stack, so its index decreased -- the ~1~ became a ~0~. Outside the
     function, the variable ~y~ was at the top of the stack, so it had index
     ~0~. As it moved into the function, another variable was pushed onto the
     stack above it, so its index was changed to ~1~.

     By using \debruijn{} indices, the originally difficult problem of renaming
     variables during substitution has become a simple transformation of
     incrementing and decrementing some integers.
*** Rust
    Rust is an imperative programming language designed to eliminate a number of
    memory safety issues encountered in C and C++. This is achieved through its
    ownership system, although that is not exploited by this project. This
    project instead makes use of the procedural-macro system.

    A procedural macro is a program that receives a stream of Rust tokens and
    outputs a different stream of Rust tokens. There are two primary uses for
    procedural macros: to extend existing Rust code; and to add new syntax to
    Rust.

    The primary use of procedural macros is to extend existing Rust code. These
    add additional definitions to Rust data types. For example, if a data type
    is annotated with the ~#[derive(Debug)]~ attribute, then the Rust compiler
    can use a procedural macro to generate code used to print debugging
    information for the data type.

    This project is primarily concerned with the other use of procedural macros
    -- to add new syntax to Rust. If a formal language only uses Rust tokens,
    then strings from that language can be used as arguments to a procedural
    macro. The procedural macro can then parse and process this language however
    it sees fit, before outputting arbitrary Rust source code derived from the
    string.

    One example used in this project is ~quote~. This procedural macro
    transforms Rust source code into an abstract syntax tree representing that
    code. This is used in the back-end of the Chomp parser generator (section
    [[*The Back-End: Code Generation]]) to produce parser code.
** Requirements Analysis
   The primary goal of the project was to provide a parser generator for the
   \ky{} type system. Achieving this goal requires three parts. First, the
   Nibble language has to provide a syntax for \mres{}. Second, the Chomp parser
   generator has to implement the \ky{} type system. Third, the Chomp parser
   generator has to output source code for a parser.

   After completing this primary deliverable, the Nibble language and Chomp
   parser generators could be extended with new features, one after another.
   This lends itself to the spiral development model cite:10.1109/2.59, where
   each new feature undergoes a complete waterfall development cycle -- design,
   implement, integrate, test.

   It is useful to be able to concurrently work on many features at once during
   the design phase, to be able to gauge the difficulty in completing a full
   implementation and to see the ways in which different features can conflict
   with each other. This is only possible with strict version control measures,
   so that each feature remains separate and so that a functional deliverable is
   always available.

   To solve these problems, I used ~git~ for version control. Development of the
   core deliverable took place on the ~master~ branch. Once it was complete, all
   new features were developed on different branches. Several branches were
   added for the exploratory design of each potential feature. Once I decided on
   a particular feature to implement, I proceeded to complete its waterfall
   cycle on its design branch. Then, the ~master~ branch was rebased onto the
   feature branch.

   Many additional features were considered for inclusion in this project as
   stretch requirements. I performed a MoSCoW analysis for each potential
   feature, shown in table [[tbl:moscow]]. This ranked features by the impact on the
   design of the Nibble language and the complexity of implementation in the
   Chomp parser generator.
   
   #+label: tbl:moscow
   #+name: tbl:moscow
   #+attr_latex: :float t :align p{0.2\linewidth}p{0.75\linewidth}
   #+caption: A MoSCoW analysis of features to include in the project.
   | Priority    | Feature                                                          |
   |-------------+------------------------------------------------------------------|
   | Must Have   | Embed \mres{}; Implement \ky{} type system; Generate Rust parser |
   | Should Have | Let statements; Function expressions; Type inference;            |
   | Could Have  | User-defined parser errors; Semantic actions                     |
   | Won't Have  | Lexer                                                            |
   
   One important stage of each waterfall development cycle was testing. Most
   tests for the Chomp parser generator were end-to-end tests. By using the
   visitor pattern, described in section [[*The Visitor Design Pattern]], the
   implementation of an operation is broken down into a different operation for
   each syntax node in the Nibble language. Each of these sub-operations were
   small enough to verify by inspection. Therefore, only large inputs needed
   testing, and the easiest way to provide large test inputs is with end-to-end
   tests.

   A key part of the test suite was AutoNibble, the Chomp-generated parser for
   the Nibble language. If the outputs of AutoNibble and the handwritten parser
   for Nibble used by Chomp's front-end were equal, then it was highly likely
   that Chomp worked correctly.

   When a new feature was added to the project, it could introduce regressions
   in the behaviour of existing end-to-end tests. Once the source of the problem
   was identified, I added a regression unit test to exercise the issue, to
   save time if a future extension reintroduced the problem.

   This project was intended to be a proof-of-concept for parser generators
   based on the \ky{} type system. Therefore, the code was made publicly
   available on both my personal website and on GitHub. It is dual-licensed
   under the MIT and Apache 2.0 licenses, like many Rust projects, such that
   other people can reuse code as part of any project or in any form, as long as
   they include the licenses.
   
** Starting Point
   I closely studied the \ky{} type system before beginning the project.

   I had previous experience with using the Rust language for personal projects.

   The project builds on ideas about formal languages. These have been studied
   in the /Part IA Discrete Maths/ and /Part IB Compiler Construction/ courses.
   I also completed a small personal project on them previously.

   Additionally, the project uses concepts from type systems, covered in the
   /Part IB Semantics of Programming Languages/, /Part II Types/ and /Part II
   Denotational Semantics/ courses.
* Implementation
   
  There are two areas of implementation for this project. The first is the
  design of the Nibble language, which describes formal languages, along with
  the description of the theoretical implementation of two type systems for
  Nibble (sections [[*The Nibble Language]] and [[*Type Systems for Nibble]]). The
  second explores the implementation of the Chomp parser generator, which
  transforms a Nibble expression describing a formal language into Rust source
  code for a parser of that language (sections [[*Chomp Repository Overview]]
  through [[*The Back-End: Code Generation]]).

  Section [[*The Nibble Language]] starts by introducing the syntax and semantics of
  the Nibble language, explaining how it solves the repetition problem of
  \mres{}. Then in section [[*Type Systems for Nibble]], the design of two type
  systems for Nibble are described: \ky{}-\lambda in section [[*The \ky{}-\lambda
  Type System]] and LM in section [[*The LM Type System]].

  Next this chapter moves on to describing the Chomp parser generator, starting
  with the structure of its code repository and overall architecture in section
  [[*Chomp Repository Overview]]. Chomp has an architecture similar to other
  compilers and translators (section [[*Translators]]). The front-, middle- and
  back-ends of Chomp are described in sections [[*The Front-End: Parsing and
  Normalisation]], [[*The Middle-End: Type Inference]], and [[*The Back-End: Code
  Generation]] respectively.

** The Nibble Language
   Nibble is a formal language for describing formal languages -- semantically,
   a Nibble expression represents a formal language. Nibble is designed to be a
   user-friendly alternative to \mres{}. In section [[*\mres{}]], the problem of
   repetition in \mres{} was introduced. Nibble solves this problem by
   introducing let statements and lambda abstractions. The syntax of the Nibble
   language is given using BNF in listing [[lst:nibble-syntax]].
   
   #+label: lst:nibble-syntax
   #+name: lst:nibble-syntax
   #+caption: The syntax of the Nibble language.
   #+begin_src bnf
   <expression> ::= <let-stmt> <expression> | <match-stmt>
   <let-stmt>   ::= "let" <ident> <arg-list> "=" <expr> ";"
   <match-stmt> ::= "match" <expr> ";"

   <expr>    ::= <lambda> | <alt>
   <lambda>  ::= "/" <arg-list> "/" <alt>
   <alt>     ::= <named> | <named> "|" <alt>
   <named>   ::= <cat>   | <cat>   ":" <ident>
   <cat>     ::= <call>  | <call>  "." <cat>
   <call>    ::= <term>  | <term>      <call>
   <term>    ::= <epsilon>
               | <literal>
               | <ident>
               | <fix>
               | "(" <expr> ")"
   <fix>     ::= "!" <term>
   <epsilon> ::= "_"

   <arg-list> ::= "" | <ident> <arg-list>
   <ident>    ::= <letter> | <letter> <ident>
   #+end_src

   For Nibble expressions to be a suitable replacement for \mres{}, Nibble must
   be able to describe the same set of languages. Nibble achieves this by
   directly embedding \mres{}. Listing [[lst:nibble-embeds-mu]] shows how Nibble
   embeds the \mre{} from figure [[fig:mre-sheep]], describing the sheep language.

   #+label: lst:nibble-embeds-mu
   #+name: lst:nibble-embeds-mu
   #+caption: Nibble expressions can embed \mres{}.
   #+begin_src nibble
   match "baa".!(/x/ (_|"a" . x));
   #+end_src

   You may notice that Nibble does not embed the \(\bot\) \mre{}. By itself,
   \(\bot\) has no practical use -- there is no need to parse the empty
   language. When combined with other combinators, \(\bot\) is either an
   annihilator or the identity, demonstrated in figure [[fig:bot-elim]]. This means
   any \mre{} containing \(\bot\) is either semantically equivalent to \(\bot\),
   or semantically equivalent to an \mre{} without \(\bot\).

   #+label: fig:bot-elim
   #+name: fig:bot-elim
   #+caption: Equalities to eliminate \(\bot\) from \mres{}.
   #+begin_figure
   \begin{align*}
     \bot \cdot e &= \bot \\
     e \cdot \bot &= \bot \\
     \bot \vee e &= e \\
     e \vee \bot &= e \\
     \mu x. \bot &= \bot
   \end{align*}
   #+end_figure

   The Nibble language fixes the repetition problem of \mres{}, by introducing
   let statements. Listing [[lst:nibble-let-statement]] demonstrates how Nibble can
   eliminate the simple repetition from figure [[fig:mre-hex-colour]].

   #+label: lst:nibble-let-statement
   #+name: lst:nibble-let-statement
   #+caption: Nibble expressions can introduce variables with let statements.
   #+begin_src nibble
     let hex = "0"|"1"|"2"|"3"|"4"|"5"|"6"|"7"|"8"|"9"
             | "a"|"b"|"c"|"d"|"e"|"f"
             | "A"|"B"|"C"|"D"|"E"|"F";
     match "#" . hex . hex . hex . hex . hex . hex . (_ | hex . hex);
   #+end_src

   A let statement introduces a new variable name, the binding variable, and
   assigns it a Nibble expression, the bound expression. In this case, the
   variable ~hex~ is assigned to a hexadecimal character. The variable can be
   used repeatedly in following statements.

   Whilst let statements can eliminate verbatim repetition, they do not help
   with repetitive patterns, where there are only minor differences between
   different instances of a pattern. The Nibble language handles this with
   lambda abstractions, which are demonstrated in [[lst:nibble-lambda]].

   #+label: lst:nibble-lambda
   #+name: lst:nibble-lambda
   #+caption: Lambda and application expressions add functions to the Nibble
   #+caption: language.
   #+begin_src nibble
     let opt = /x/ _ | x;
     let plus x = !(/plus/ x . opt plus);
     match plus "a" . plus "b" . plus "c";
   #+end_src
   
   There are two ways to introduce a lambda abstraction: either through a lambda
   expression ~/x/ e~, or through a let-lambda statement ~let x y = e~. The
   let-lambda statements are /syntactic sugar/ for a let-statement binding a
   lambda-expression. They are indistinguishable in their semantics and how they
   are type checked: the Nibble expression ~let x y = e~ is equivalent to
   ~let x = /y/ e~.

   Notice how in the second line of the listing, the fixed-point operator ~!~
   takes the lambda expression as an argument. In general, the fixed-point
   operator accepts any expression that is a one-argument first-order function.
   Whilst not demonstrated here, functions can take more than one argument.

** Type Systems for Nibble
   I have designed two type systems for Nibble: \ky{}-\lambda and LM. The
   \ky{}-\lambda type system is a minimal departure from the \ky{} type system
   for \mres{}, which was presented in section [[*The \ky{} Type System]], by
   treating lambda abstractions as \mre{} macros. The LM type system is a
   theoretical system for Nibble, incorporating ideas from the \hm{} type
   system, introduced in section [[*The \hm{} Type System]]. This adds polymorphic
   function types on top of the \ky{} type system.
   
*** The \ky{}-\lambda Type System
    The \ky{}-\lambda type system is a type system for Nibble using the \ky{}
    type system, presented in section [[*The \ky{} Type System]], as the core. By
    treating lambda abstractions as macros for \mres{}, Nibble expressions are
    translated into embedded \mres{}. This \mre{} is type checked using the
    \ky{} type system.

    For a Nibble expression ~e~, the translation of ~e~ is denoted as \(
    \llbracket \texttt{e} \rrbracket \). This is detailed in figure
    [[fig:nibble-translate]] To summarise, translation performs call-by-name
    evaluation of Nibble expressions. The only exception is the fixed-point
    operator, ~!e~. This first translates the argument ~e~. If ~e~ translates to
    ~/x/ f~, then ~f~ is translated, keeping ~x~ free. Otherwise, translation
    fails.

    #+label: fig:nibble-translate
    #+name: fig:nibble-translate
    #+caption: Translation of Nibble expressions
    #+begin_figure
    \centering
    \begin{math}
    \begin{array}{ccc}
      \begin{prooftree}
        \infer0{\llbracket \texttt{\_} \rrbracket = \texttt{\_}}
      \end{prooftree}
      &&
      \begin{prooftree}
        \infer0{\llbracket \texttt{"w''} \rrbracket = \texttt{"w''}}
      \end{prooftree} \\
      && \\
      \begin{prooftree}
        \infer0{\llbracket \texttt{x} \rrbracket = \texttt{x}}
      \end{prooftree}
      && \\
      && \\
      \begin{prooftree}
        \hypo{\llbracket \texttt{e} \rrbracket = \texttt{f}}
        \hypo{\llbracket \texttt{e'} \rrbracket = \texttt{f'}}
        \infer2{\llbracket \texttt{e . e'} \rrbracket = \texttt{f . f'}}
      \end{prooftree}
      &&
      \begin{prooftree}
        \hypo{\llbracket \texttt{e} \rrbracket = \texttt{f}}
        \hypo{\llbracket \texttt{e'} \rrbracket = \texttt{f'}}
        \infer2{\llbracket \texttt{e | e'} \rrbracket = \texttt{f | f'}}
      \end{prooftree} \\
      && \\
      \begin{prooftree}
        \infer0{\llbracket \texttt{/x/ e} \rrbracket = \texttt{/x/ e}}
      \end{prooftree}
      &&
      \begin{prooftree}
        \hypo{\llbracket \texttt{e} \rrbracket = \texttt{/x/ f}}
        \hypo{\llbracket \texttt{f [e'/x]} \rrbracket = \texttt{f'}}
        \infer2{\llbracket \texttt{e e'} \rrbracket = \texttt{f'}}
      \end{prooftree} \\
      && \\
      \begin{prooftree}
        \hypo{\llbracket \texttt{e} \rrbracket = \texttt{/x/ f}}
        \hypo{\llbracket \texttt{f} \rrbracket = \texttt{f'}}
        \infer2{\llbracket \texttt{!e} \rrbracket = \texttt{!(/x/ f')}}
      \end{prooftree}
      &&
      \begin{prooftree}
        \hypo{\llbracket \texttt{e' [e/x]} \rrbracket = \texttt{r}}
        \infer1{\llbracket \texttt{let x = e; e'} \rrbracket = \texttt{r}}
      \end{prooftree} \\
    \end{array}
    \end{math}   
    #+end_figure

    There are some problems with this approach. Firstly, call-by-name evaluation
    of untyped terms is non-terminating. Consider listing [[lst:omega]]. The
    expression ~omega omega~ evaluates to ~omega omega~. Users might be confused
    by the parser generator hanging instead of producing an error. In any case,
    the type system ensures parsers terminate in linear time, which is more
    important in most cases.
   
    #+label: lst:omega
    #+name: lst:omega
    #+caption: An example Nibble expression that does not translate. Evaluating
    #+caption: ~omega omega~ produces ~omega omega~.
    #+begin_src nibble
      let omega x = x x;
      match omega omega;
    #+end_src

    # In fact, the Nibble embeds the lambda calculus. This means that translation,
    # effectively evaluation, of terms can perform arbitrary computation.
    # Therefore, not only is translation of Nibble non-terminating, termination of
    # translation is also undecidable.

    Another issue is that unused expressions are completely ignored. Whilst this
    has some computational benefits, it could cause confusion for users. An
    example is in listing [[lst:kyl-ignores]]. Due to the evaluations strategy,
    because the ill-typed expression ~"a" | "a"~ (it fails the \(\#\)
    constraint) is bound to the unused variable ~unused~, the overall expression
    is well-typed. If someone referenced ~unused~, type checking would
    unexpectedly fail.

    #+label: lst:kyl-ignores
    #+name: lst:kyl-ignores
    #+caption: The \ky{}-\lambda type system ignores expressions bound by unused
    #+caption: variables.
    #+begin_src nibble
      let unused = "a" | "a";
      match "baa";
    #+end_src

*** The LM Type System
    The LM type system is an alternative type system for Nibble. It combines
    features of the \hm{} type system discussed in section [[*The \hm{} Type
    System]] with the core of the \ky{} type system, presented in section [[*The
    \ky{} Type System]]. This allows for expressions bound by let statements to
    have polymorphic types, and removes the need to translate expressions before
    type checking.
    
    Figure [[fig:lm-rules]] shows the typing rules for the LM type system. We
    first talk through the various rules, then show some example inferences.
    Looking at those examples may help with understanding these rules.

    #+label: fig:lm-rules
    #+name: fig:lm-rules
    #+caption: The LM type rules
    #+begin_figure
    \centering
    \begin{prooftree}
      \infer0[LMEps]{\Gamma; \Delta \vdash \texttt{\_} : K(\tau_\epsilon); \emptyset}
    \end{prooftree}
    \qquad
    \begin{prooftree}
      \infer0[LMLit]{\Gamma; \Delta \vdash \texttt{"cw"} : K(\tau_c); \emptyset}
    \end{prooftree}

    \begin{prooftree*}
      \hypo{\sigma = S\rho}
      \hypo{C' = S C}
      \infer2[LMVar]{\Gamma, x : (\rho, C); \Delta \vdash \texttt{x} : \sigma; C'}
    \end{prooftree*}

    \begin{prooftree*}
      \hypo{\Gamma; \Delta \vdash \texttt{e} : K(\tau); C}
      \hypo{\Gamma, \Delta; \cdot \vdash \texttt{e'} : K(\tau'); C'}
      \infer2[LMCat]{\Gamma; \Delta \vdash \texttt{e . e'} : K(\tau \cdot \tau'); C \cup C' \cup \{ \tau \circledast \tau' \}}
    \end{prooftree*}

    \begin{prooftree*}
      \hypo{\Gamma; \Delta \vdash \texttt{e} : K(\tau); C}
      \hypo{\Gamma; \Delta \vdash \texttt{e'} : K(\tau'); C'}
      \infer2[LMAlt]{\Gamma; \Delta \vdash \texttt{e | e'} : K(\tau \vee \tau'); C \cup C' \cup \{ \tau \# \tau' \}}
    \end{prooftree*}

    \begin{prooftree}
      \hypo{\Gamma, x : (\sigma, \emptyset); \Delta \vdash \texttt{e} : \sigma'; C}
      \infer1[LMAbs1]{\Gamma; \Delta \vdash \texttt{/x/ e} : \sigma \to \sigma'; C}
    \end{prooftree}
    \qquad
    \begin{prooftree}
      \hypo{\Gamma; \Delta, x : (\sigma, \emptyset) \vdash \texttt{e} : \sigma'; C}
      \infer1[LMAbs2]{\Gamma; \Delta \vdash \texttt{/x/ e} : \sigma \leadsto \sigma'; C}
    \end{prooftree}

    \begin{prooftree*}
      \hypo{\Gamma; \Delta \vdash \texttt{e} : \sigma \to \sigma'; C}
      \hypo{\Gamma; \Delta \vdash \texttt{e'} : \sigma; C'}
      \infer2[LMApp1]{\Gamma; \Delta \vdash \texttt{e e'} : \sigma'; C \cup C'}
    \end{prooftree*}

    \begin{prooftree*}
      \hypo{\Gamma; \Delta \vdash \texttt{e} : \sigma \leadsto \sigma'; C}
      \hypo{\Gamma, \Delta; \cdot \vdash \texttt{e'} : \sigma; C'}
      \infer2[LMApp2]{\Gamma; \Delta \vdash \texttt{e e'} : \sigma'; C \cup C'}
    \end{prooftree*}

    \begin{prooftree}
      \hypo{\Gamma; \Delta \vdash \texttt{e} : K(\tau) \leadsto K(\tau); C}
      \infer1[LMFix]{\Gamma; \Delta \vdash \texttt{!e} : K(\tau); C}
    \end{prooftree}
    \qquad
    \begin{prooftree}
      \hypo{\Gamma; \Delta \vdash \texttt{e} : \sigma \leadsto \sigma'; C}
      \infer1[LMSub]{\Gamma; \Delta \vdash \texttt{e} : \sigma \to \sigma'; C}
    \end{prooftree}

    \begin{prooftree*}
      \hypo{\Gamma; \Delta \vdash \texttt{e} : \sigma; C}
      \hypo{\Gamma; \Delta, x: (\bar{\sigma}, \bar{C}) \vdash \texttt{e'} : \sigma'; C'}
      \infer2[LMLet]{\Gamma; \Delta \vdash \texttt{let x = e; e'} : \sigma'; C'}
    \end{prooftree*}
    #+end_figure
    
    The variable contexts store both types and constraints. Constraints are
    relations between types in the \ky{} type system. They need to be stored in
    the variable context because these relations are not always decidable for
    type variables. For example, whether \(\alpha \# \tau_c\) holds depends on
    what \alpha is.
    
    Another oddity in the LM typing system is the form of the judgement. We
    extend the \ky{} typing judgement by adding a set of constraints \(C\) to
    the conclusion. The judgement has the form \( \Gamma; \Delta \vdash
    \texttt{e} : \tau ; C \), which can be read: under unguarded variable
    context \Gamma and guarded context \Delta, the Nibble expression ~e~ has the
    type \tau given the constraints in \(C\) hold. Thus, type checking an
    expression becomes a two-step process: infer a type; then check the
    constraints hold.

    The LMEps and LMLit rules are the simplest LM typing rules, being largely
    unchanged from the \ky{} type system. The only differences are that the type
    is wrapped in a \(K\) constructor, to distinguish \ky{} types from function
    types, and they both return an empty set of constraints.

    The LMVar rule is a combination of the variable rules from the \hm{} and
    \ky{} type systems. First, the variable must be unguarded. This is to
    prevent infinite recursion, like in the \ky{} type system. Second, the
    output type and constraints are a specialisation of the type and constraints
    from the context. This is like the \hm{} variable typing rule. We
    specialise the type constraints because they can also include type
    variables.

    LMCat and LMAlt remain similar to the corresponding rules in the \ky{} type
    system. Like the LMEps and LMLit rules, all the types are wrapped in a
    \(K\) constructor. Instead of the constraints appearing in the premise, as
    they do in the \ky{} type system, they are moved to the conclusion. This is
    so they can be checked later when all type variables are instantiated.

    Notice how there are two different typing rules for lambda expressions. This
    is due to the two variable contexts from the \ky{} type system. One function
    type, used by LMAbs1, is for unguarded functions, where the formal parameter
    can be used in an unguarded context. The other function type is used by
    LMAbs2 for guarded functions, where the formal parameter can only be used in
    guarded contexts. Because lambda expressions are monomorphic, type
    constraints pass straight through.

    Again due to the presence of two function types, there are two typing rules
    for application. If the called function is an unguarded function type, then
    the argument is evaluated in the same context, described by the LMApp1 rule.
    If the function is a guarded function, the LMApp2 rule applies and the
    argument is evaluated in an unguarded context -- the function body ensures
    the parameter only appears on the right side of a concatenation, so all
    variables are accessible.

    The LMSub rule allows guarded functions to be used when an unguarded
    function was expected. Krishnaswami and Yallop cite:10.1145/3314221.3314625
    proved the transfer property that makes this safe. This rule is likely only
    useful for type inference.

    The LMFix rule only accepts first-order guarded functions as the argument.
    Whilst fixed-points could accept higher-order functions, doing so would
    allow Nibble expressions that do not translate to \mres{}. To prevent
    unguarded recursion in parsers, the formal parameter must be guarded.

    The LMLet rule is taken from the corresponding \hm{} typing rule almost
    directly. Note that bound variables are always unguarded. This is also the
    only typing rule that adds constraints to variables in the context. A
    consequence of this typing rule is that constraints \(C\) on the bound
    expression are only checked if ~x~ is used in the body.

**** Examples
     
     I will now justify the need for two different function contexts. Consider
     the expression ~/x/ _ | x~, corresponding to an optional ~x~. This
     expression is an essential combinator for real-world languages. ~x~ appears
     unguarded in the expression ~_ | x~, so this lambda expression can have the
     type \(\forall \alpha. \tau_\epsilon \vee \alpha\).
    
     Now consider the expression ~!(/x/ "a" | "<".x.">")~. As a \mre{}, this
     would be represented as \( \mu x. a \vee {<}x{>} \). Recalling the rules from
     the \ky{} typing judgement, \(x\) would be introduced to the guarded
     context. Hence, ~x~ should be introduced to the guarded context too.
     Therefore, this expression has the type \( \mu \alpha. \tau_a \vee \tau_{<}
     \cdot \alpha \cdot \tau_{>} \).
    
** Chomp Repository Overview
   Chomp is a parser generator from the Nibble language to Rust. Chomp is
   implemented in Rust. Table [[tbl:overview]] gives a brief description of the
   repository structure for Chomp.

   #+label: tbl:overview
   #+name: tbl:overview
   #+caption: Brief outline of the code repository structure. All code is 
   #+caption: written in Rust.
   #+attr_latex: :float t
   | Directory              | Description                                    | Lines of Code |
   |------------------------+------------------------------------------------+---------------|
   | ~src/nibble~           | Nibble parser and normalisation                |           738 |
   | ~src/chomp~            | Chomp type inference algorithm                 |          1786 |
   | ~src/lower~            | Parser code generation                         |           403 |
   | ~tests~                | Minimal end-to-end tests of Chomp              |            57 |
   | ~chewed~               | Shared library for all Chomp-generated parsers |           270 |
   | ~chomp-macro~          | Procedural macro interface                     |            42 |
   | ~autonibble/src~       | AutoNibble implementation                      |           489 |
   | ~autonibble/tests~     | Tests for correctness of AutoNibble            |            35 |
   | ~autonibble/benches~   | Benchmarks of AutoNibble                       |            57 |
   | ~chomp-bench/**/json~  | Benchmarks of various parsers for JSON         |           489 |
   | ~chomp-bench/**/arith~ | Benchmarks of various parsers for arithmetic   |           274 |

   Overall, the project consists of a main Chomp generator and four smaller
   supporting libraries. The top-level directory consists of the source code and
   test directory for the main Chomp generator. The ~chewed~, ~chomp-macro~,
   ~autonibble~ and ~chomp-bench~ directories contain code for the supporting
   libraries.
   
   The ~src~ directory contains the code for the main Chomp generator. It is
   split into three parts. The front-end is in the ~src/nibble~ directory. This
   parses an input stream of Nibble expressions and produces an abstract syntax
   tree for it. The middle-end is in the ~src/chomp~ directory. This performs
   type inference using the \ky{}-\lambda type system. It outputs a typed
   \mre{}. The back-end is in the ~src/lower~ directory. It is responsible for
   converting the typed \mre{} into Rust source code for a parser.
   
   The ~tests~ directory contains the code to run a number of end-to-end tests
   for the Chomp generator, making sure certain examples type check correctly.
   Correctness of the generated parsers is left to benchmarking and other test
   code.
   
   All Chomp-generated parsers share some common code, such as the trait
   definition (a type definition like a Java interface) for a parser. This
   common code is kept in the ~chewed~ library. The reason it forms a separate
   library is so that consumers of Chomp only need to include the small ~chewed~
   library with their final binary, instead of the relatively large Chomp
   library.
   
   To help make Chomp easier for developers to include in their projects, a
   procedural macro interface was created. Due to current limitations in Rust,
   this interface has its own, small library in the ~chomp-macro~ directory.
   
   The success criterion for this project required bootstrapping the Nibble
   language -- using a Chomp-generated parser to parse Nibble expressions. This
   parser, dubbed /AutoNibble/, is in the ~autonibble~ directory. This directory
   includes some simple tests of the correctness of AutoNibble, as well as some
   benchmarks to compare its performance against the handwritten parser used by
   Chomp's front-end.
   
   Finally, there is the ~chomp-bench~ directory. This is a small library for
   comparing the performance of Chomp-generated parsers against handwritten
   parsers and ~lalrpop~-generated parsers, which is an external parser
   generator for Rust. Ideally, this library would be part of ~chomp-macro~.
   However, limitations in the build system for ~lalrpop~ makes this impossible.
** The Front-End: Parsing and Normalisation
   The front-end of Chomp is responsible for converting the input stream of
   characters representing a Nibble expression into an abstract syntax tree.
   This occurs in three stages: lexing, which splits the characters in the input
   stream into different tokens; parsing, which transforms the token stream into
   a concrete syntax tree; and normalisation, which converts this concrete
   syntax tree into an abstract syntax tree.
   
   In Chomp, the lexing is performed by the external ~syn~ library. This can
   convert streams of characters into tokens from the Rust language. Nibble uses
   a subset of the tokens found in Rust, so lexing into Rust tokens makes the
   parser simpler. It also makes integration with procedural macros
   significantly easier, because procedural macros receive a stream of Rust
   tokens as input.
   
   The parser in Chomp also uses the ~syn~ library. It provides lightweight
   interface to parse some often-used data structures. For example, it provides
   the ~Punctuated<T, P>~ type, which represents a list of values of type ~T~,
   separated by values of type ~P~. 
   
   Listing [[lst:parse-term]] shows how Chomp parses a Nibble term. This function
   makes use of Rust's type inference and trait systems to call one of four
   different functions, all written as ~input.parse()~. By checking what the
   next input token is, it is possible to determine exactly what type of term
   can appear.
   
   #+label: lst:parse-term
   #+name: lst:parse-term
   #+caption: Rust code snippet that parses a Nibble term. An enum is like an 
   #+caption: OCaml datatype.
   #+begin_src rust
     pub enum Term {
         Epsilon(Epsilon),
         Ident(Ident),
         Literal(Literal),
         Fix(Fix),
         Parens(ParenExpression),
     }

     impl Parse for Term {
         fn parse(input: ParseStream<'_>) -> syn::Result<Self> {
             let lookahead = input.lookahead1();

             if lookahead.peek(Token![_]) {
                 input.parse().map(Self::Epsilon)
             } else if lookahead.peek(LitStr) {
                 input.parse().map(Self::Literal)
             } else if lookahead.peek(Token![!]) {
                 input.parse().map(Self::Fix)
             } else if lookahead.peek(Paren) {
                 input.parse().map(Self::Parens)
             } else if lookahead.peek(Ident::peek_any) {
                 input.call(Ident::parse_any).map(Self::Ident)
             } else {
                 Err(lookahead.error())
             }
         }
     }
   #+end_src
   
   The front-end finishes with normalisation, which converts the concrete syntax
   tree into an abstract syntax tree. This occurs in two stages that occur
   simultaneously. First, syntactic sugar is expanded. Second, variable names
   are converted to \debruijn{} indices (introduced in section [[*\debruijn{}
   Indices]]).
   
   In section [[*The Nibble Language]], let-lambda statements were introduced as
   syntactic sugar for a let statement binding a lambda expression. During
   normalisation, let-lambda statements are converted into this expanded form,
   instead of keeping them around as another node type in the abstract syntax
   tree. This reduces complexity in later stages of Chomp.
   
   The other part of normalisation is conversion to \debruijn{} indices. Most of
   this work is achieved by the ~Context~ struct, shown in listing
   [[lst:parse-ctx]]. As discussed in section [[*\debruijn{} Indices]], when \debruijn{}
   indices were introduced, variables in lexically-scoped languages form a
   stack. The most-recently declared variable is at the top of the stack. New
   variables are pushed on top of the stack, and popped off when they leave
   scope.
   
   #+label: lst:parse-ctx
   #+name: lst:parse-ctx
   #+caption: Rust code snippet for conversion to \debruijn{} indices.
   #+begin_src rust
     pub struct Context {
         bindings: Vec<Name>,
     }

     impl Context {
         /// Get the De Bruijn index of `name`, if it is defined.
         pub fn lookup(&self, name: &Name) -> Option<usize> {
             self.bindings
                 .iter()
                 .rev()
                 .enumerate()
                 .find(|(_, n)| *n == name)
                 .map(|(idx, _)| idx)
         }

         /// Permanently add the variable `name` to the top of the stack.
         pub fn push_variable(&mut self, name: Name) {
             self.bindings.push(name);
         }

         /// Call `f` with the variable `name` pushed to the top of the stack.
         pub fn with_variable<F: FnOnce(&mut Self) -> R, R>(
             &mut self, 
             name: Name, 
             f: F,
         ) -> R {
             self.bindings.push(name);
             let res = f(self);
             self.bindings.pop();
             res
         }
     }
   #+end_src
   
   There are two ways to introduce variables in Nibble. The binding variables
   from let statements are in scope for the rest of the Nibble expression. The
   formal parameters of lambda expressions are in scope only for the body of the
   lambda expression. This is reflected by the two different ways to push
   variables to the stack.
   
   ~push_variable~ adds a variable onto the stack of ~bindings~ in a ~Context~
   permanently. The API provides no way to remove variables. This is called
   by let statements, after converting the bound expression but before
   converting their body. The ~with_variable~ method pushes a variable onto the
   stack only for the duration of a call to the function ~f~. This is used by
   lambda expressions, where ~f~ will convert the lambda body.
   
   The ~lookup~ method does all of the heavy-lifting. It numbers each member of
   the stack starting from the top. Then still from the top, it returns the
   index of the first item with a matching name. If no such item exists, then
   an error is returned.

** The Middle-End: Type Inference
   The middle-end of Chomp performs type inference using the \ky{}-\lambda type
   system, to produce a typed \mre{}. First, the abstract syntax tree computed
   by the front-end is translated. Next, its type is inferred using the \ky{}
   type system, and the output expression is built. Before going into detail
   over how these two stages work, the visitor design pattern is described, as
   it is used to implement both of them.
   
*** The Visitor Design Pattern
    Both translation and the type inference algorithm use the visitor design
    pattern. This design pattern separates algorithms from the data structures
    they operate on. The visitor design pattern follows the open/closed
    principle -- the data structure is closed for modification, but the design
    pattern makes it open for new algorithms.
    
    Figure [[fig:visitor-uml]] shows a UML diagram for the visitor design pattern.
    The ~Visitor~ interface requires implementors to handle each type of object
    in the ~Visitable~ data type. The ~Visitable~ data type then only needs one
    generic dispatch method to implement all the algorithms that use this
    pattern. 
    
    #+name: src:visitor-uml
    #+begin_src dot :file ./images/uml.png :cache yes
      digraph {
        nodesep = 1.5;

        node [
          shape = record;
          width = 2;
        ];
        edge [
          dir = back;
        ];

        expression [ 
          label = "{\<\<enumeration\>\>\nVisitable|Variant1\lVariant2\lVariant3\l...}";
        ];
        visitor [
          label = "{\<\<interface\>\>\nVisitor|+ visit_one(Variant1)\l+ visit_two(Variant2)\l+ visit_three(Variant3)\l...}";
        ];
        translation    [ label = "{VisitorA}" ];
        substitution [ label = "{VisitorB}" ];

        expression -> visitor   [ style = dashed; constraint = false ];
        visitor -> translation    [ arrowtail = onormal ];
        visitor -> substitution [ arrowtail = onormal ];
      }
    #+end_src
    
    #+label: fig:visitor-uml
    #+name: fig:visitor-uml
    #+caption: A UML diagram depicting the visitor design pattern.
    #+results: src:visitor-uml
    [[./images/uml.png]]
    
    Listing [[lst:visitor-defs]] show how this pattern is implemented in Chomp. The
    ~Visitor~ trait includes a function signature for each type of
    abstract-syntax-tree node. ~NamedExpression~ then uses pattern matching to
    dispatch the appropriate method.
    
    #+label: lst:visitor-defs
    #+name: lst:visitor-defs
    #+caption: Rust code snippet showing the implementation of the visitor 
    #+caption: design pattern.
    #+begin_src rust
      pub type NameSpan<'a> = (Option<&'a Name>, Option<Span>);
      pub trait Visitor {
          type Out;
          fn visit_epsilon(&mut self, namespan: NameSpan, eps: &Epsilon) -> Self::Out;
          fn visit_literal(&mut self, namespan: NameSpan, lit: &Literal) -> Self::Out;
          fn visit_cat(&mut self, namespan: NameSpan, cat: &Cat) -> Self::Out;
          fn visit_alt(&mut self, namespan: NameSpan, alt: &Alt) -> Self::Out;
          fn visit_fix(&mut self, namespan: NameSpan, fix: &Fix) -> Self::Out;
          fn visit_var(&mut self, namespan: NameSpan, var: &Variable) -> Self::Out;
          fn visit_call(&mut self, namespan: NameSpan, call: &Call) -> Self::Out;
          fn visit_lambda(&mut self, namespan: NameSpan, lambda: &Lambda) -> Self::Out;
          fn visit_let(&mut self, namespan: NameSpan, stmt: &Let) -> Self::Out;
      }

      impl NamedExpression {
          pub fn visit<V : Visitor>(&self, visitor: &mut V) -> <V as Visitor>::Out {
              let namespan = (self.name.as_ref(), self.span);
              match &self.expr {
                  Self::Epsilon(e) => visitor.visit_epsilon(namespan, e),
                  Self::Literal(l) => visitor.visit_literal(namespan, l),
                  Self::Cat(c) => visitor.visit_cat(namespan, c),
                  Self::Alt(a) => visitor.visit_alt(namespan, a),
                  Self::Fix(f) => visitor.visit_fix(namespan, f),
                  Self::Variable(v) => visitor.visit_variable(namespan, v),
                  Self::Call(c) => visitor.visit_call(namespan, c),
                  Self::Lambda(l) => visitor.visit_lambda(namespan, l),
                  Self::Let(l) => visitor.visit_let(namespan, l),
              }
          }
      }
    #+end_src
    
    An alternative to the visitor pattern in Rust is to define algorithms as
    traits directly on ~NamedExpression~. However, this would require making the
    representation of ~NamedExpression~ public, so that it can be fully
    unwrapped. Visitors would also be more tightly coupled to the implementation
    of ~NamedExpression~. Changing the representation of ~NamedExpression~ would
    then be more difficult in future.
*** Translation
    The first step in using the \ky{}-\lambda type system is translation of the
    expression. Recall from section [[*The \ky{}-\lambda Type System]] how
    translation of Nibble is essentially call-by-name evaluation. This is
    implemented using a number of visitors.
    
    The outer-most visitor is the ~Reduce~ visitor (a misnomer). This is what
    drives the translation. It searches for let statements and application
    expressions to translate, without stepping into lambda expressions. For the
    let and application expressions, it performs the appropriate substitution
    and then translates the result.
    
    The substitution is performed by another visitor, ~Substitute~. Given an
    expression and a \debruijn{} index, it searches for uses of that index and
    replaces them with the expression. In section [[*\debruijn{} Indices]], we noted
    that indices of free variables change inside of lambda bodies. The target
    index is shifted inside of lambda bodies, and free variables in the
    substitute need to be renamed.
    
    The final visitor for translation changes the \debruijn{} indices of free
    variables in an expression.
    
*** Inference
    Type inference is the second part of the \ky{}-\lambda type system. This
    uses the typing rules of the \ky{} type system to infer the type of
    expressions. It returns a \mre{} annotated with types.

    Recall how Nibble has some constructs that are not in \mres{}, namely let
    statements, application expressions and lambda expressions. The translation
    in the previous section eliminates all let statements and application
    expressions. However, it does not eliminate lone lambda expressions.
    Therefore, if the type inference algorithm reaches a lambda expression, it
    fails.
    
    By looking back to the \ky{} typing rules (figure [[fig:ky-rules]]), the type
    inference rules for other Nibble expressions can be derived. Inference for
    epsilon expressions is trivial -- always the type \tau_\epsilon. By
    combining the KYCat rule with the KYChar rule, a literal expression ~"cw"~
    can be shown to always has type \tau_c.
    
    By implementing types using ~BTreeSet~ from the Rust standard library to
    store the first and flast sets, alternation becomes simple. First, the type
    of each sub-expression is inferred recursively. Next, to check that the
    \(\#\) constraint holds, the ~intersection~ function from the Rust standard
    library is used. Finally, the ~append~ function is used to compute the new
    type.
    
    Concatenation is nearly identical to alternation. There are two differences.
    First, the constraint is slightly different, although the checks are almost
    identical. The bigger difference is that the suffix of a concatenation needs
    to use an unguarded context for type inference.
    
    The last Nibble expressions to consider are fixed-point expressions and
    variables. Variables are a simple lookup in the type context. The type for
    fixed-point expressions are found using iteration. Initially, the type is
    assumed to be \(\tau_\bot\). This guess is then added to the variable
    context and used to infer the type of the fixed-point body. This is repeated
    until the initial guess and next inferred types are equal, or there is a
    type error. Listing [[lst:infer-fix]] shows the type inference process.
    
    #+label: lst:infer-fix
    #+name: lst:infer-fix
    #+caption: Rust code snippet for type inference for fixed-point expressions.
    #+begin_src rust
      fn fold_fix(&mut self, fix: Fix) -> Result<_, _> {
          let mut ty = Type::default();
          let inner = fix.inner.try_into_lambda()?.get_body();

          loop {
              // ? at end exits function if there is a type error.
              let res = self.context.with_variable_type(ty.clone(), |context| {
                  inner.clone().fold(&mut TypeInfer { context })
              })?;

              let next = res.get_type();

              if next == ty {
                  return Ok(/* ... */);
              }

              ty = next.clone();
          }
      }
    #+end_src
    
    Listing [[lst:type-context]] shows the type context used during type inference.
    Its design is similar to the naming context used in the front-end, shown in
    listing [[lst:parse-ctx]]. There are two ~with~ methods: ~with_variable_type~
    for introducing new variables; and ~with_unguard~ for moving the guarded
    context into the unguarded one. These call their function arguments after
    pushing some data on a stack, and then pop the data off before returning.
    Like the front-end context, most of the work is performed by the
    ~get_variable_type~ method.
    
    #+label: lst:type-context
    #+name: lst:type-context
    #+caption: Rust code snippet showing the type context used by type 
    #+caption: inference.
    #+begin_src rust
      pub struct Context {
          vars: Vec<Type>,
          unguard_points: Vec<usize>,
      }

      impl Context {
          pub fn with_unguard<F: FnOnce(&mut Self) -> R, R>(&mut self, f: F) -> R {
              self.unguard_points.push(self.vars.len());
              let res = f(self);
              self.unguard_points.pop();
              res
          }

          pub fn get_variable_type(
              &self, 
              var: Variable,
          ) -> Result<&Type, GetVariableError> {
              self.vars
                  .iter()
                  .nth_back(var.index)
                  .ok_or(GetVariableError::FreeVariable)
                  .and_then(|ty| {
                      if self.unguard_points.last().unwrap_or(&0) + var.index
                          >= self.vars.len()
                      {
                          Ok(ty)
                      } else {
                          Err(GetVariableError::GuardedVariable)
                      }
                  })
          }

          pub fn with_variable_type<F: FnOnce(&mut Self) -> R, R>(
              &mut self, 
              ty: Type, 
              f: F,
          ) -> R {
              self.vars.push(ty);
              let res = f(self);
              self.vars.pop();
              res
          }
      }
    #+end_src
    
    Because the abstract syntax tree of Nibble uses \debruijn{} indices to
    represent variables, the variable context can be represented by a pair of
    stacks. The ~vars~ field stores the stack of types in the context. The type
    of a variable is then the item in ~vars~ indexed from the top of the stack.
    For most type systems, this completes the lookup function. However, the
    \ky{} type system has both a guarded and unguarded context. Which variables
    are unguarded is kept track of by ~unguard_points~, which stores the total
    number of variables that are unguarded. A simple arithmetic check then
    determines whether a variable is unguarded.
        
** The Back-End: Code Generation
   Code generation is the final step in Chomp. The typed \mre{} computed by the
   middle-end is converted into Rust code for both data types and parser
   implementations. A separate library called ~chewed~ contains the definitions
   of some data types and the parsing trait used by all Chomp-generated parsers.
   
   The substitutions performed while translating the original Nibble expression
   results in a large amount of duplication of sub-expressions. In an attempt to
   reduce the amount of generated code, and to make integrating a
   Chomp-generated parser into a project easier, the back-end attempts to
   eliminate work generating code for duplicate expressions using interning.
   
   This interning occurs in three phases. First, each \mre{} is mapped to a
   /handle/. This is done structurally, such that two identical \mres{} receive
   the same handle. Next, the set of all handles that can be reached from the
   top-level \mre{} handle are computed. After this, code is generated only for
   the \mres{} that were reached. In practice, the first and last phases are
   computed in one pass, and the second step collates the necessary generated
   code.
   
   Mapping expressions to handles uses many caches, one for each type of
   expression. First, the handles of all sub-expressions are found. Next, the
   cache corresponding to the expression type is checked, using the
   sub-expression handles as keys. If the cache does not contain a handle for a
   key, a new one is generated.
   
   Finding the set of reachable handles uses well-known graph algorithms.
   
   The generated code takes the form of a stream of Rust source tokens. Creating
   such streams by hand would be tedious and error prone, given the complexity
   of parts of the generated code. Instead, a procedural macro from the ~quote~
   library is used to allow the output token stream to be written literally. The
   procedural macro converts the literal Rust code into a token stream
   describing it.

   The form of generated code depends on the \mre{}. \epsilon is translated to
   the ~Epsilon~ type in the ~chewed~ library, on the assumption that almost all
   Nibble expressions will include an epsilon expression.
   
   Literals are each translated to a unique unit struct. By using unique types,
   no data needs to be stored with the type to determine what literal it is.
   This means that the Rust compiler can eliminate these literal types, keeping
   only the side effects of their computation.
   
   Concatenations are quite simple in terms of code generation. Each
   concatenation is translated into a struct, where each sub-expression is a
   different field. To parse the concatenation, each field is parsed in turn.
   
   The most challenging expression form to translate into Rust code is
   alternation. Alternations are represented by enumerations, where each
   constructor corresponds to a different alternative. When parsing an
   alternation, which alternative to try and parse depends on the next character
   of input and the first sets of each alternative. Example output code is shown
   in listing [[lst:gen-alt]].
   
   #+label: lst:gen-alt
   #+name: lst:gen-alt
   #+caption: Chomp-generated Rust code for parsing an alternation.
   #+begin_src rust
     // Parser for `match (_|"a")|("b"|"B")|"c";`
     enum Ast {
         Branch1(/*...*/); //   _|"a"
         Branch2(/*...*/); // "b"|"B"
         Branch3(/*...*/); // "c"
     }
     impl Parse for Ast {
         fn take<P: Parser + ?Sized>(input: &mut P) -> Result<Self, TakeError> {
             match input.peek() {
                 Some('a')   => Ok(Self::Branch1(input.take()?)),
                 Some('b')
                 | Some('B') => Ok(Self::Branch2(input.take()?)),
                 Some('c')   => Ok(Self::Branch3(input.take()?)),
                 // Because `_|"a"` contains the empty string,
                 // we can always take a member of `_|"a"`.
                 _           => Ok(Self::Branch1(input.take()?))
             }
         }
     }
   #+end_src
   
   Fixed-point expressions are implemented as type aliases. Variables then refer
   to the type of the declaring fixed point.
   
** Summary
   This chapter began by introducing the Nibble language for describing formal
   languages. Two type systems for Nibble were introduced -- the \ky{}-\lambda
   type system and the LM type system.

   The \ky{}-\lambda type system treats Nibble constructs as macros for \mres{},
   before deferring to the \ky{} type system. This causes type checking to be
   non-terminating, and well-typed Nibble to contain some ill-typed fragments.

   To address these issues, I detailed the LM type system. By combining the
   \hm{} type system with the \ky{} type system, it gives Nibble expressions
   polymorphic types. To account for type variables, the constraints from the
   \ky{} type system were moved from the premise to the conclusion of the typing
   rules, to be solved separately.
   
   The chapter started discussing the Chomp parser generator, which uses the
   \ky{}-\lambda type system. Chomp is both implemented in and targets Rust to
   exploit the procedural-macro system. Like most translators, Chomp is split
   into three "ends".
   
   The front-end of the Chomp generator takes a concrete stream of characters
   and produces an abstract syntax tree of a Nibble expression. An external
   lexer converts the character stream to a sequence of Rust tokens. A parser
   then converts the Rust tokens into a concrete syntax tree. A normalisation
   step then expands syntactic sugar and introduces \debruijn{} indices.
   
   The middle-end is responsible for type inference using the \ky{}-\lambda type
   system. First, the Nibble expression represented by the abstract syntax tree
   is translated to a \mre{} with call-by-name evaluation. Next, the \ky{} type
   system is used to infer the type of this \mre{}.
   
   Finally, the back-end performs code generation. The type-annotated \mre{} is
   expanded into a sequence of Rust tokens describing a parser for its language.
   This process utilises interning to eliminate the replication of expressions
   introduced by the earlier translation.

* Evaluation
  I now evaluate whether the implementation has fulfilled the goals of the
  project. In section [[*Meeting the Success Criterion]] I demonstrate the success
  criterion was satisfied. Following this, I compare the benefits of using
  Nibble to BNF in section [[*Comparing Nibble and BNF]]. Section [[*Integrating Chomp]]
  contrasts integrating Chomp into a project with integrating ~lalrpop~, a
  parser generator for Rust using BNF-inspired syntax. I pay particular
  attention to the impact of the \ky{} type system on describing formal
  languages in Nibble. Finally, in section [[*Performance of Chomp-generated
  Parsers]] I analyse the results of benchmarks to determine the run-time cost of
  using Chomp-generated parsers.
  
** Meeting the Success Criterion
   The success criterion required that the Chomp parser generator: "can generate
   a parser implementation for [the Nibble language] that produces identical
   output to the hand-written Nibble parser". I start by considering whether I
   have produced the requisite deliverables, then move on to checking I have
   fulfilled the success criterion.
   
   I described the Nibble language in section [[*The Nibble Language]], which is a
   new language for describing formal languages. By embedding \mres{} in Nibble,
   Nibble provides a new way of describing \mres{}. The introduction of let
   statements and lambda expressions shifts Nibble away from its mathematical
   roots in \mres{} towards a more programmatic style.
   
   Whilst not explicitly stated in the proposal, it was required to design an
   appropriate type system for Nibble that was compatible with the \ky{} type
   system. I have achieved this in two different ways. The \ky{}-\lambda type
   system described in section [[*The \ky{}-\lambda Type System]] describes the
   minimal changes to the \ky{} type system necessary for use with Nibble.
   Section [[*The LM Type System]] describes the LM type system, which is a fresh
   type system directly on Nibble compatible with the \ky{} type system.
   
   Another core requirement was the creation of Chomp -- a parser generator
   taking Nibble as input. Chapter [[*Implementation]] discusses the implementation
   of Chomp, including the three parts described in the original implementation.
   
   The rest of this section is concerned with the bootstrapping of Chomp
   required to satisfy the success criterion. All of this code is in the
   ~autonibble~ directory of the repository. 
   
   First, Nibble is sufficiently expressive to be able to describe itself. This
   is demonstrated in the ~autonibble/src/lib.rs~ file. This is a significant
   result -- Nibble syntax is similar to that of many programming languages used
   in industry. Further, the Chomp generator can process this self-description
   to produce the AutoNibble parser.
   
   The rest of the code in ~autonibble/src/lib.rs~ is there to make AutoNibble a
   complete front-end for Chomp. It performs the same normalisation process on
   AutoNibble's concrete syntax tree as Chomp does on its own concrete syntax
   tree.
   
   Finally, I directly compare the outputs of the full AutoNibble front-end and
   Chomp's front-end. This is performed on the test cases present in the
   ~autonibble/tests/compare/~ directory. 
   
** Comparing Nibble and BNF
   Untyped Nibble and BNF, introduced in section [[*Backus-Naur Form]], are both
   languages for describing formal languages. By comparing the features and
   ergonomics of Nibble and BNF, I can conclude whether the Nibble language has
   the potential to join BNF as an effective way to describe formal languages.
   
   First, I explore the classes of formal languages that BNF and Nibble can
   describe. As stated in section [[*Backus-Naur Form]], BNF can describe all
   context-free languages. Nibble likely also only accepts context-free
   languages, despite translation being able to perform arbitrary computation.
   
   A fundamental principle of programming is DRY -- don't repeat yourself.
   Duplicating code makes maintenance more difficult. A conscious effort has to
   be made to keep all the duplicates in sync, and any bug found in one
   duplicate will be present in all others. Nibble avoids replication using let
   statements for verbatim copies, and lambda expressions for parametric copies.
   
   In contrast, whilst a new top-level form in BNF can eliminate verbatim and
   some cases of parametric repetition, other forms of parametric replication
   cannot be removed. Figure [[fig:dry-example]] shows how Nibble can remove
   replication of list definitions which cannot be removed from BNF. Whilst the
   BNF forms ~<xs>~ and ~<ys>~ have the same shape, there is no way to abstract
   that out. On the other hand, the Nibble variable ~list~ provides a parametric
   list definition. 
   
   #+label: fig:dry-example
   #+name: fig:dry-example
   #+caption: Comparison of BNF and the Nibble language, demonstrating that 
   #+caption: Nibble can abstract away patterns such as lists.
   #+begin_figure
     \begin{minted}{bnf}
       // BNF
       <xs>    ::= "x" | "x" <xs>
       <ys>    ::= "y" | "y" <ys>
       <start> ::= <xs> <ys>
     \end{minted}
     \medskip
     \begin{minted}{nibble.py -x}
       // Nibble
       let list inner = !(/list/ inner . (_ | list));
       match list "x" . list "y";
     \end{minted}
   #+end_figure
   
   Another difference between BNF and Nibble is the variable scope rules. The
   mutually-recursive global scope of BNF allows other forms to be referenced
   without qualification from anywhere. Nibble uses much more restrictive
   non-recursive lexical scoping rules -- only variables defined previously can
   be used. This is shown in figure [[fig:scope-example]]. In BNF, the ~<term>~ form
   can directly refer to the ~<expr>~ form, even though it is defined later.
   Conversely, Nibble requires ~term~ to be a function with ~expr~ as a
   parameter. Also note that Nibble makes recursion explicit using the fixed
   point, compared to BNF's implicit recursion.
   
   #+label: fig:scope-example
   #+name: fig:scope-example
   #+caption: Comparison of BNF and the Nibble language demonstrating the 
   #+caption: difference in variable scope rules.
   #+begin_figure
     \begin{minted}{bnf}
       // BNF
       <term> ::= <number> | "(" <expr> ")"
       <expr> ::= <term>   | <term> + <expr>
     \end{minted}
     \medskip
     \begin{minted}{nibble.py -x}
       // Nibble
       let term expr = number | "(" . expr . ")";
       let expr      = !(/expr/ term expr . (_ | ("+" | "-") . expr));
     \end{minted}
   #+end_figure
  
** Integrating Chomp
   Whilst Nibble is interesting in its own right, most of its use in practice
   depends on Chomp. Here, I compare Chomp and \ky{}-\lambda-typed Nibble
   against ~lalrpop~, a parser generator for Rust using BNF-inspired syntax. We
   first discuss the language classes accepted by both \ky{}-typed Nibble and
   ~lalrpop~, and the consequences on the form of descriptions. Then I look at
   how to integrate both Chomp and ~lalrpop~ into a project build system.

   Due to limitations of the \ky{} type system, \ky{}-\lambda-typed Nibble can
   only describe LL languages. Meanwhile, ~lalrpop~ can describe the wider class
   of LR languages.
   
   One practical example of these differences in allowed language descriptions
   is shown in figure [[fig:compare-number]]. This shows how to parse an
   optionally-signed number in both ~lalrpop~ and \ky{}-\lambda-typed Nibble.
   ~lalrpop~ has no problems with having the optional sign as a prefix to the
   number. On the other hand, the \ky{}-\lambda typing rules use the
   \(\circledast\) constraint to prevent the prefix of a concatenation from
   containing the empty string. This means ~number~ must be used twice -- once
   for unsigned numbers, and another for signed numbers.
   
   #+label: fig:compare-number
   #+name: fig:compare-number
   #+caption: Comparison of ~lalrpop~ and the Chomp parser generator
   #+caption: demonstrating  that ~lalrpop~ can accept optional prefixes before 
   #+caption: a string.
   #+begin_figure
     \begin{minted}{rust}
       // lalrpop
       Sign : bool = {
           "+" => true,
           "-" => false,
       }
       SignedNumber = <s : Sign?> <n : Number>;
     \end{minted}
     \medskip
     \begin{minted}{nibble.py -x}
       // Chomp
       let signed_number = number | ("+"|"-") . number;
     \end{minted}
   #+end_figure
   
   Another major difference between ~lalrpop~ and \ky{}-\lambda-typed Nibble is
   that the \ky{}-\lambda type system only accepts left-factored expressions.
   Figure [[fig:left-factor]] shows how ~lalrpop~ and \ky{}-\lambda-typed Nibble
   would describe the small language consisting of the three strings "bat",
   "band" and "brook". In ~lalrpop~, all three strings appear as literals.
   However, \ky-\lambda-typed Nibble factors out the common prefixes.
  
   #+label: fig:left-factor
   #+name: fig:left-factor
   #+caption: Comparison of ~lalrpop~ and the Chomp parser generator that 
   #+caption: demonstrates that ~lalrpop~ does not need left-factoring.
   #+begin_figure
   \begin{minted}{rust}
     // lalrpop
     Language = {
         "bat",
         "band",
         "brook",
     }
   \end{minted}
   \medskip
   \begin{minted}{nibble.py -x}
     // Chomp
     match "b" . ("a" . ("t" | "nd") | "rook");
   \end{minted}
   #+end_figure
   
   The Nibble has to be left-factored to satisfy the \(\#\) constraint -- each
   alternative must have a disjoint \(\textsc{First}\) set. Consider listing
   [[lst:unfactored]], which shows the output parser implementation for the
   unfactored expression. Each alternative has the same condition of
   ~Some('b')~. As there is no backtracking, parsing this unfactored expression
   would not perform as expected.
   
   #+label: lst:unfactored
   #+name: lst:unfactored
   #+caption: Chomp-generated Rust code assuming left-factoring was unnecessary. 
   #+caption: Notice how the conditions for three of the branches are all 
   #+caption: ~Some('b')~.
   #+begin_src rust
     impl Parse for Ast {
         fn take<P: Parser + ?Sized>(input: &mut P) -> Result<Self, TakeError> {
             match input.peek() {
                 // "bat"
                 Some('b') => Ok(Self::Bat(input.take()?),
                 // "band"
                 Some('b') => Ok(Self::Band(input.take()?),
                 // "brook"
                 Some('b') => Ok(Self::Brook(input.take()?),
                 // Errors
                 None      => Err(TakeError::EndOfInput(/*...*/)
                 Some(c)   => Err(TakeError::BadBranch(/*...*/)),
             }
         }
     }
   #+end_src
   
   Left factoring causes many problems. Firstly, the resulting expression is
   more difficult to read. This hinders developers trying to understand what
   language the expression is describing. Secondly, searching for a literal can
   now fail. When searching for a word, users start at the left and work towards
   the right. However, left-factoring separates the left-most characters from
   the rest of a literal. More generally, left factoring obfuscates the
   semantics of an expression. 
   
   Additionally, left factoring introduces replication. Consider a programming
   language like Java. Two common statements are variable assignments and if
   statements. Variables can be any sequence of lowercase characters that is not
   a keyword. Figure [[fig:java-factor]] shows fragments of ~lalrpop~ and
   \ky{}-\lambda-typed Nibble that match these two types of statements. The
   ~lalrpop~ fragment is as expected -- ~"if"~ does one thing and variables do
   another. The Nibble fragment introduces a lot of repetition to avoid
   left-factoring. 
   
   #+label: fig:java-factor
   #+name: fig:java-factor
   #+caption: Comparison of ~lalrpop~ and the Chomp parser generator showing 
   #+caption: Chomp's left-factoring for a Java-like language.
   #+begin_figure
   \begin{minted}{rust}
   // lalrpop
   Statement = {
       "if" "(" <e: Expression> ")" "{" <then: Statements> "}",
       <var: r"[a-z]+"> "=" <e : Expression> ";" => {
           if !var.is_keyword() { 
               // all good
           } else {
               // error!
           }
   }
   \end{minted}
   \medskip
   \begin{minted}{nibble.py -x}
   // Chomp
   let if_body = "(" . expr . ")" . opt ws . "{" . stmts . "}";
   let assign_body = opt ws . "=" . expr . ";";
   let stmt = 
     "i" . ( "f" . ( if_body 
                   | plus a_to_z . assign_body)
           | assign_body 
           | ("a"|/* ... */|"e"|"g"|/* ... */|"z") . star a_to_z . assign_body
           )
   | ("a"|/* ... */|"h"|"j"|/* ... */|"z") . star a_to_z . assign_body
   ;
   \end{minted}
   #+end_figure
   
   ~lalrpop~ avoids the left factoring problem via two mechanisms. The first is
   that it accepts a larger range of language descriptions, as described above.
   Secondly, it has a lexing stage. A lexer splits the input character stream
   into different tokens, which are then given to the parser. The lexer decides
   whether a particular sequence of characters is a keyword or just a regular
   name.
   
   Chomp does not use a lexer. Whilst lexers are useful to avoid left factoring,
   they are not necessary. Throughout the project, a more powerful type system
   was judged to be a more useful addition to Chomp. Given a sufficiently smart
   type system, the left factoring problem can be eliminated entirely.
   
   Another feature of ~lalrpop~ not present in Nibble is the use of semantic
   actions. These are small computations performed by the parser during parsing,
   often used to build the abstract syntax tree. 

   Chomp can bypass semantic actions using Rust's trait system. All data types
   used by Chomp-generated parsers are made publicly accessible. Developers
   using Chomp can define trait implementations for these data types, giving
   them arbitrary properties. This is how AutoNibble converts its concrete
   syntax tree into an abstract syntax tree for the Nibble language -- it uses
   the same ~Convert~ trait used in the front-end of Chomp.
   
   Chomp is easier to integrate into projects than ~lalrpop~. Chomp uses Rust's
   procedural-macro system to transform a Nibble expression embedded in a Rust
   file into a Chomp-generated parser. ~lalrpop~ instead requires an external
   build script to first convert ~lalrpop~ descriptions into Rust source code,
   which has to be explicitly included in the project.
   
   Finally, Chomp computes explicit types for Nibble expressions. This has
   benefits in terms of error reporting over ~lalrpop~. Under the \ky{} type
   system, an \mre{} is only rejected if one of the \(\circledast\) or \(\#\)
   constraints does not hold. The types of the relevant sub expressions then
   trace the origin of the problem. Using ~lalrpop~, parser generation only
   fails if there is a problem generating the output parser.
  
** Performance of Chomp-generated Parsers
   One claim made by Krishnaswami and Yallop cite:10.1145/3314221.3314625 was
   that the parsing algorithm they presented produced linear parsers. After a
   discussion on the benchmark methodology, I test whether this claim extends to
   Chomp and Chomp-generated parsers and compare the performance of
   Chomp-generated parsers to ~lalrpop~ and handwritten parsers.

*** Methodology
    Benchmarks were performed using the ~criterion~ library for Rust. By
    analysing the timing data produced by ~criterion~, it is possible to estimate
    how long it takes a function to run and the variance of that measurement. To
    estimate the asymptotic behaviour of a parser on the size of the input,
    benchmarks are performed for many different input lengths.

    First, a description of how ~criterion~ measures performance. There are two
    stages it takes to benchmark every function: warm-up and measurement. The
    warm-up stage prepares the system for executing the desired function. The
    function is executed once, then twice, then four times and so on, until the
    total warm-up time exceeds three seconds.

    Measurements are split into samples. Each sample consists of a number of
    iterations, or executions, of the function. ~criterion~ measures the
    duration of each of one hundred samples. For the \(n\)th sample, the number
    of iterations \( I_n = n d \), where \(d\) is a scaling factor chosen so
    that all one hundred samples are collected in approximately thirty seconds.
    \(d\) is calculated by the number of iterations and duration of the warm-up
    period.

    Assume that a sample measurement \( T_n \) is given by the equation \( T_n =
    M + n t + \epsilon \), where \(M\) is a constant measurement error, \(t\) is
    the wall-time of one function iteration, and \epsilon is a normally
    distributed error. By performing linear regression on the sample
    measurements with respect to the sample size, you can compute \(\bar{t}\)
    and \(\bar{\sigma^2}\), an estimate for \(t\) and the variance of the
    estimate.

    Due to the deterministic nature of parsers, and the number of iterations
    performed during benchmarking, wall-time is a suitable proxy measurement for
    processor time. The primary source of non-determinism in measurements will
    be hardware interrupts, which were minimised by running benchmarks on an
    idle system.

    Only one benchmark was performed at a time. Every benchmark used a single
    thread of execution. The measurements were collected on a desktop computer
    with the following specifications:
    
    * Processor: AMD Ryzen 5 3600 @ \SI{4.2}{\giga\hertz}
    * Memory: \SI{16}{\gibi\byte} DIMM DDR4 \SI{2400}{\mega\hertz} RAM
    * OS: Arch Linux 5.11
    * Rust Version: 1.51.0
    
*** Performance of AutoNibble
    AutoNibble is the most complex example of Nibble that exists, and hence
    produces the most complex Chomp-generated parser. Roughly eighty lines of
    Nibble are translated into over 4800 lines of Rust source code. I compare
    the performance of AutoNibble to the Nibble parser in Chomp's front-end.

    Figure [[fig:autonibble-vs-chomp]] shows the benchmark results for AutoNibble
    and the handwritten Nibble parser in Chomp. The benchmarks were computed on
    Nibble expressions of various lengths, ranging from 12 to 3096 bytes.

    Because AutoNibble and Chomp's parser produce different concrete syntax
    trees, the benchmarks include normalisation to the Nibble abstract syntax
    tree, as detailed in section [[*The Front-End: Parsing and Normalisation]].
    
    #+label: fig:autonibble-vs-chomp
    #+name: fig:autonibble-vs-chomp
    #+caption: Graph of wall-time to parse a Nibble expression against the
    #+caption: length of the Nibble expression in bytes. This compares the
    #+caption: handwritten Nibble parser used by the Chomp parser generator
    #+caption: against AutoNibble.
    [[./images/autonibble.png]]
  
    The trend in the data for both AutoNibble and the handwritten Nibble parser
    suggests that both parsers operate in linear time. Note that even the
    largest input is quite small, so behaviour might change for bigger inputs.

    This graph also suggests that AutoNibble outperforms the handwritten parser.
    This is an encouraging result, because it suggests Chomp-generated parsers
    can be used in some practical applications to improve performance.

*** Performance against ~lalrpop~
    Whilst AutoNibble is the most complex example of Nibble, using Nibble as the
    benchmark language has some issues. There are very few examples of Nibble,
    and the complexity of writing descriptions of the Nibble language for other
    parser generators seems daunting, especially given how the language was
    continuously evolving. Instead, Chomp-generated, handwritten and ~lalrpop~
    parsers were created for two widely-used languages: JSON and arithmetic.

    Figures [[fig:bench-json]] and [[fig:bench-arith]] show the benchmark results for
    JSON and arithmetic respectively. For JSON, each parser produced a Rust
    datatype corresponding to the JSON value. The input data ranges from roughly
    one hundred bytes in length to 28000 bytes, and was taken from a weather
    API. For arithmetic, the result of evaluating the expression was computed.
    The length of data goes from 100 bytes up to 32000 and was randomly
    generated in advance of writing the benchmark.

    #+label: fig:bench-json
    #+name: fig:bench-json
    #+caption: Graph of wall-time to parse a JSON value string against the
    #+caption: length of the string in bytes. This compares a Chomp-generated
    #+caption: parser, a ~lalrpop~-generated parser and a handwritten parser.
    [[./images/json.png]]

    #+label: fig:bench-arith
    #+name: fig:bench-arith
    #+caption: Graph of wall-time to parse and evaluate an arithmetic expression
    #+caption: against the length of the arithmetic expression in bytes. This 
    #+caption: compares a Chomp-generated parser, a ~lalrpop~-generated parser 
    #+caption: and a handwritten parser.
    [[./images/arith.png]]

    In both cases, the Chomp-generated parsers appear to run in linear time.
    This gives further evidence that the parsers truly do have linear
    performance.

    In these two tests, the handwritten parser outperforms both generated
    parsers. The handwritten parser can immediately produce values of the
    appropriate type. On the other hand, the generated parsers first produce a
    concrete syntax tree before converting it to the correct type.
    Chomp-generated parsers create the full tree and are then converted.
    ~lalrpop~ parsers create fragments of concrete syntax trees before
    performing conversion.

    The performance of ~lalrpop~- and Chomp-generated parsers is comparable,
    with the Chomp-generated parser outperforming the ~lalrpop~ parser for
    arithmetic expressions. Therefore, if the usability issues described in
    section [[*Integrating Chomp]] are resolved, the Chomp generator could compete
    with traditional parser generators in general use.
    
* Conclusion
  This project was a success -- I completed the success criterion of
  implementing AutoNibble and completed an extension of adding functions to the
  Nibble language.
  
  My project set out to develop a parser generator based on \mres{}. By creating
  the Nibble language, I designed an ergonomic way to describe \mres{} (section
  [[*The Nibble Language]]). I then designed two type systems for the Nibble
  language, which extend the \ky{} type system to the syntax of Nibble (section
  [[*Type Systems for Nibble]]). One of these type systems was implemented in the
  Chomp parser generator (section [[*Chomp Repository Overview]]), to produce
  parsers implemented in Rust.

  I proved that the Nibble language is capable of being able to describe itself.
  Further, using the Chomp parser generator to create the AutoNibble parser, I
  found that AutoNibble outperforms a hand-written parser. I also found that
  Chomp-generated parsers have comparable performance to traditionally-generated
  parsers.
  
** Lessons Learned
   The time table proposed in my original project proposal was quite optimistic.
   I underestimated the effect that external time pressures -- such as
   supervisions during Lent term -- would have on the time I could spend on this
   project. Fortunately I had allocated plenty of slack time in the proposed
   schedule. However, a more balanced schedule would have been the better
   option.

   The code structure used by the Chomp parser generator has three separate
   modules for the front-, middle-, and back-ends. As described in section
   [[*Translators]], this is the architecture typically used by translators in
   industry. When the interface to each end is stable, there is little coupling
   between the three ends. However, because the Nibble language and the Chomp
   parser generator are both brand-new, the interface for each end was
   constantly evolving. Every new feature changed one of these critical
   interfaces, so almost every file had to be modified. More research should
   have been spent trying to find an architecture that could withstand the fast
   development cycle used by the project.

   The end-to-end tests for this project were written relatively early on in the
   evolution of the Nibble language and the Chomp parser generator. Several
   times, these tests and benchmarks had to be almost completely rewritten to
   account for changes in the syntax of Nibble or the structure of
   Chomp-generated parsers. Writing more unit tests could have reduced the
   number of necessary end-to-end tests, and, since unit tests are faster to
   change than end-to-end tests, more time could have been spent adding new
   features instead of maintaining tests.
   
** Future Work
   The LM Type system generates the potential for lots of future work. First,
   there is no formal proof that it is a correct type system. Whilst the
   modifications made to the \hm{} and \ky{} type systems are small, these
   changes could require imaginative solutions to prove soundness and
   completeness properties. Formal proofs of these properties are necessary to
   make sure the LM type system achieves what it sets out to complete.
    
   Secondly, the Chomp parser generator could be modified to use the LM type
   system. Even without proof, a practical implementation can provide evidence
   for the claims made by the LM type system. It can also help to justify some
   design decisions of the LM type system, such as the choice to use structural
   unification for types. Work on this implementation can proceed alongside a
   proof for the type system. Using a dependant-type system such as Agda could
   allow for the proof and implementation to be tightly coupled, in the sense
   that changes to one necessitates changes to the other.
    
   I conjectured that the LM type system does not accept expressions for a
   larger class of languages than the \ky{} type system does. Unfortunately both
   type systems reject some simple expressions that can be parsed in linear
   time, for example the \mre{} \((\epsilon \vee a) \cdot b \), representing an
   optional prefix symbol \(a\) before the symbol \(b\). Searching for a type
   system for \mres{} that permits accepts a wider range of expressions would
   make writing well-typed Nibble easier.
    
   In section [[*The \ky{}-\lambda Type System]], I discussed how translation of
   Nibble expressions can perform arbitrary computations. In contrast, the LM
   type system performs no such computation, because of the syntactic nature of
   the type system. The Nibble language and the LM type system could be extended
   to support additional data types, such as natural numbers. This would make it
   possible to express a richer set of parser combinators in Nibble.
   
#+latex: %TC:ignore
* References
  \printbibliography[heading=none]{}

\appendix

* Project Proposal
  \begin{refsection}
  \input{proposal.tex}
  \end{refsection}
  
# * Pink Book
# ** Introduction [0/2]
#    * [ ] Clear motivation
#    * [ ] Justifies potential benefits of success
# ** Preparation [0/4]
#    * [ ] Good or excellent requirements analysis
#    * [ ] Justified and documented selection of suitable tools
#    * [ ] Good engineering approach
#    * [ ] Clear presentation of challenging background beyond Part IB
# ** Implementation [0/7]
#    * [ ] Contribution to the field
#    * [ ] Application of extra-curricular reading
#    * [ ] Original interpretation of previous work
#    * [ ] Challenging goals and substantial deliverables
#    * [ ] Selection and application of appropriate mathematical and engineering techniques
#    * [ ] Clear and justified repository overview
#    * [ ] At most minor faults in execution or understanding
# ** Evaluation [0/3]
#    * [ ] Clearly presented argument demonstrating success criteria met
#    * [ ] Excellent evidence of critical thought and interpretation of results
#    * [ ] Substantiate any claims of success, improvements or novelty
# ** Conclusion [0/3]
#    * [ ] Provide an effective summary of work completed
#    * [ ] Good future work
#    * [ ] Personal reflection on the lessons learned

#+latex: %TC:endignore
#  LocalWords:  AutoNibble
