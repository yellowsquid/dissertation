#+latex_class: dissertation
#+latex_class_options: [12pt,a4paper,twoside,openright]
#+latex_header: \usepackage[hyperref=true,url=true,backend=biber,natbib=true]{biblatex}
#+latex_header: \usepackage[margin=1in]{geometry}
#+latex_header: \usepackage{booktabs,ebproof,parskip,stmaryrd}
#+latex_header: \addbibresource{diss.bib}

# math operators
#+latex_header: \DeclareMathOperator{\True}{true}
#+latex_header: \DeclareMathOperator{\False}{false}
#+latex_header: \DeclareMathOperator{\If}{if}
#+latex_header: \DeclareMathOperator{\Then}{then}
#+latex_header: \DeclareMathOperator{\Else}{else}
#+latex_header: \DeclareMathOperator{\Let}{let}
#+latex_header: \DeclareMathOperator{\In}{in}
#+latex_header: \DeclareMathOperator{\Null}{null}
#+latex_header: \DeclareMathOperator{\First}{first}
#+latex_header: \DeclareMathOperator{\Flast}{flast}

# try to avoid widows and orphans
#+latex_header: \raggedbottom
#+latex_header: \sloppy
#+latex_header: \clubpenalty1000%
#+latex_header: \widowpenalty1000%

# add more header depths
#+options: H:6

list-of-figures:nil

\pagestyle{headings}
* Introduction
  A /language/ is a set of strings over some alphabet of symbols. For example, a
  dictionary enumerates a language over written characters. Spoken English is a
  language, where the alphabet consists of phonemes. Programming languages are
  over an alphabet of ASCII or Unicode characters. IP packets are languages over
  bytes. The language of a decision problem is the set of valid inputs. Every
  computer science problem can be transformed into a question about languages.

  Whilst linear strings are helpful for data storage and transmission, they have
  limited use for other algorithms. Strings are used to encode other non-linear
  data. A /parser/ is a function that takes a language string and decodes it to
  return the underlying data. Parsers should be fast; why spend time decoding
  data when there is useful computation to be done? Unfortunately, hand writing
  efficient parsers is repetitive, with lots of boiler plate, and requires
  careful consideration of the order of operations.

  A /parser generator/ is a curried function taking a description of a language
  and returning a parser. If the target language is known ahead of time, then
  the parser can be computed ahead of time. Automated parser generation raises
  the level of abstraction for writing parsers, allowing people to quickly
  design a language so they can focus on solving difficult problems elsewhere.

  The de facto way to describe a language is with a /formal grammar/. Introduced
  by Chomsky cite:null, a grammar is a finite set of string rewriting rules.
  These rules are often given to the parser generator in Backus-Naur form (BNF).
  This form of parser generation has a couple of problems in practice. Firstly,
  BNF uses a single global mutually-recursive namespace. This is incompatible
  with modern programming languages that almost exclusively use lexical scoping.
  Secondly, there is no standard BNF format. Therefore a grammar cannot be
  easily shared between different projects. Finally, introducing an additional
  compilation step for parser generation is often difficult, sometimes needing
  to completely overhaul the build system being used.

  These issues, combined with a trend towards more powerful type systems, have
  led to an increase in use of /parser combinators/. Parser combinators are
  higher-order functions that take parsers and return a new parser. By using
  regular functions instead of parser generators, problems with variable scoping
  and compilation are completely bypassed.

  Unfortunately, the ease of use of parser combinators comes with a price. Most
  implementations of parser combinators use backtracking, which can lead to
  exponential worst-case parse time. Fortunately, a recent result by
  Krishnaswami and Yallop cite:null found a type system (the /KY type system/)
  for some primitive parser combinators that only accepts linear-time
  deterministic parsers.
  
** Project Overview
   Some parser combinators can be defined by composing smaller combinators
   together. The KY type system handles this by evaluating the higher-order
   combinators until they reach the base combinators. In section [[*The LM Type
   System]] I form the /LM type system/ by extending the KY type system to handle
   higher-order combinators directly.

   Next, I design /Nibble/ in section ref:null. Nibble is a new DSL for
   describing languages, inspired by combinators used in the LM type system.
   Nibble should be able to represent the same languages as BNF.
   
   In section ref:null, I implement /Chomp/, a typed parser generator for
   Nibble. Chomp uses the LM type system to ensure its input is suitable for
   transformation into a recursive-descent parser. Chomp is implemented in Rust,
   and produces Rust source code as output. This output is a /chewed parser/.
   
   I end by evaluating the performance of chewed parsers. Critically in section
   ref:null we show that chewed parsers operate in linear time. Section ref:null
   demonstrates that chewed parsers have comparable performance to other
   generated parsers. Finally section ref:null demonstrates that Nibble is
   potentially usable in practice.
* Preparation
** Background
   This subsection starts with the definition of formal languages, generators
   and parsers. Understanding these definitions is essential for understanding
   the rest of this dissertation. Next, it discusses formal grammars. Formal
   grammars are the traditional way to study context-free languages *(why have
   them?)*. Following this, it describes parser combinators and an algebraic
   interpretation of them. This algebraic interpretation is the core of the KY
   type system and the extended LM type system. This subsection then goes on to
   describe the KY type system. Finally, we discuss a general Hindley-Milner
   type system, which heavily inspired the final form of the LM type system.
   
*** Formal Languages
    Given an alphabet, \( \Sigma \), a /language/
    \( L \subseteq \mathcal{P}(\Sigma^*) \) is a set of strings over this
    alphabet.

    Take a set \( D \) of /derivations/. A function \( g : D \to \Sigma^* \) is
    a /language generator/, with generated language
    \( L[g] = \{ w \in \Sigma^* \mid \exists d \in D. g(d) = w \} \).

    If \( g \) is an injection, then the generator is /unambiguous/. Every
    string in \( L[g] \) will have a unique derivation.

    Take a reversed function \( p : L[g] \to D \). \( p \) is a /parser/ of
    \( g \) if \( p \) is a right inverse of \( g \). In general, many such
    parsers could exist. Figure [[fig:ambiguous-parser]] shows a trivial example.
    The set of parsers for a generator is denoted \( P[g] \). When \( g \) is
    unambiguous, then \( g \) is both injective and surjective over \( L[g] \),
    hence \( p \) is unique.

    # Define a /prefix parser/ of \( g \),
    # \( p' : \Sigma^* \rightharpoonup D \times \Sigma^* \), to be a partial
    # function such that when \( p'(w) = d, w' \), then \( g(d) w' = w \), and
    # \( g(d) \) is the longest prefix of \( w \) in \( L[g] \).

    #+label: fig:ambiguous-parser
    #+name: fig:ambiguous-parser
    #+caption: A generated language with two parsers.
    #+begin_figure
    \[ D = \{ 0 , 1 \} \]
    \[ g(d) = a \]
    \begin{align*}
      p_1(a) &= 0 \\
      p_2(a) &= 1
    \end{align*}
    \[ g(p_1(a)) = a = g(p_2(a)) \]
    #+end_figure

    For a set of /language descriptions/, \( \mathcal{D} \), define an indexed
    set of generators, \( \mathcal{G} \). A /parser generator/ is a function
    \( \mathcal{P} \) such that
    \( \mathcal{P}(d) \in P[\mathcal{G}_d] \). For any valid language
    description, a parser generator then produces a parser for that language.
*** Formal Grammars
    Formal grammars are a set of language descriptions. Introduce a set of
    /non-terminal symbols/ \( N \). Distinguish a /start symbol/ \( S \in N \).
    Let \( V = \Sigma \uplus N \) be the /vocabulary/ of a grammar.

    A /production rule/ is a pair, \( u \mapsto v \), where \( u \in V^*NV^* \)
    and \( v \in (V/S)^* \). The relation \( wuw' \Mapsto wvw' \) is an
    /application/ of this production rule.

    A grammar \( G \) is a set of production rules. A sequence of applications
    \( S \Mapsto^* w \) is a derivation if \( w \in \Sigma^* \). The generator
    for \( G \) returns these \( w \). An example derivation is given in figure
    [[fig:grammar-example]].

    #+label: fig:grammar-example
    #+name: fig:grammar-example
    #+caption: An example grammar derivation.
    #+begin_figure
    \begin{align*}
      S &\mapsto aX \\
      X &\mapsto aX\\
      aX &\mapsto Xb \\
      X &\mapsto c
    \end{align*}
    \[ S \Mapsto aX \Mapsto aaX \Mapsto aXb \Mapsto acb \]
    #+end_figure

    These /unrestricted grammars/ correspond to recursively enumerable languages
    cite:null. Whilst any string in the language is accepted, rejecting strings
    is undecidable. Chomsky cite:null introduced a hierarchy of constrained
    grammars. Adding more constraints to production rules reduces the
    computational complexity of parsers, at the cost of reduced expressive
    power.

    Context-free grammars have rules of the form \( A \mapsto v \), where
    \( A \in N \). This transforms derivations into trees, with non-terminal
    internal nodes and alphabet strings as leaves.

    /Context-free grammars/ are the smallest class of grammars in the Chomsky
    hierarchy that include paired delimiters. The set of languages they
    represent are called /context-free languages/. Unfortunately, algorithms
    that parse general context-free grammars, such as Earley and CYK, have
    super-linear time complexity. *NOTE: why is this bad?*

    Chomsky cite:null found that context-free grammars can be parsed by
    nondeterministic push-down automata -- finite state machines with a stack.
    Restricting this to deterministic finite automata leads to /deterministic
    context-free grammars/. Their languages can be parsed in linear time, and
    are unambiguous.

    There are generally two approaches to parsing deterministic context-free
    grammars: top-down and bottom-up. Both of these methods are typically
    restricted to one symbol of /lookahead/. This means only one symbol of the
    input is visible at a time, and once the input is advanced it cannot be
    reversed.

    Top-down parsers, or left-most derivation parsers, start at the root of the
    derivation tree and recursively parse each non-terminal. Parsers like this
    one exclude grammars with /left-recursion/; rules of the form
    \( A \mapsto Av \). With only one symbol of lookahead, it is impossible to
    determine how deep the derivation needs to be.

    Bottom-up parsers (right-most derivation parsers) start at the leaves of the
    derivation tree. This eliminates the left-recursion problem, as the tree is
    only built up to the minimum necessary height.
*** Parser Combinators
    A /generator combinator/ is a higher-order language generator. They take
    some number of generators and generator combinators, and produce a new
    generator or generator combinator. A /parser combinator/ is likewise a
    higher-order parser.
    
    Mathematical analysis of arbitrary generator combinators is infeasible --
    they are arbitrary functions, after all. By restricting the combinators used
    to the set composing some primitive combinators, it is possible to introduce
    an algebra to describe them. Figure [[fig:mu-reg-def]] details one such algebra,
    named \(\mu\)-regular expressions.
    
    #+label: fig:mu-reg-def
    #+name: fig:mu-reg-def
    #+caption: \(\mu\)-regular expressions and their derivations.
    #+begin_figure
      *TODO: alignment of derivations is a little wonky*
      \[
        e = \bot
          \mid \epsilon
          \mid c
          \mid e \cdot e
          \mid e \vee e
          \mid \mu x. e
          \mid x
      \]
      
      \centering
      \bigskip
      \begin{math}
      \begin{array}{ccc}
        \begin{prooftree}
           \infer0[DEps]{\epsilon &\Mapsto \epsilon}
        \end{prooftree}
        & \qquad &
        \begin{prooftree}
           \infer0[DLit]{c &\Mapsto c}
        \end{prooftree}
        \\
        & \qquad & \\
        \begin{prooftree}
           \hypo{e &\Mapsto w}
           \infer1[DVeeL]{e \vee e' &\Mapsto w}
        \end{prooftree}
        & \qquad &
        \begin{prooftree}
           \hypo{e &\Mapsto w}
           \infer1[DVeeR]{e' \vee e &\Mapsto w}
        \end{prooftree}
        \\
        & \qquad & \\
        \begin{prooftree}
           \hypo{e &\Mapsto w}
           \hypo{e' &\Mapsto w'}
           \infer2[DCat]{e \cdot e' &\Mapsto ww'}
        \end{prooftree}
        & \qquad &
        \begin{prooftree}
           \hypo{e [ \mu x . e / x ] &\Mapsto w}
           \infer1[DFix]{\mu x . e &\Mapsto w}
        \end{prooftree}
      \end{array}
      \end{math}
    #+end_figure

    There are three first-order language generators: \(\bot\) for the empty
    language, \(\epsilon\) for the language of the empty string only, and
    \( c \) for a language containing the single-symbol string \( c \) only.

    There are two second-order combinators. Concatenation, \( g \cdot g' \)
    takes words from \( g \) and concatenates them with words from \( g' \).
    Alternation, \( g \vee g' \), forms the union of the languages \( g \) and
    \( g' \).

    Finally, there is the least-fixed-point combinator \(\mu g\). This is the
    union \( \bigcup_{n\in\mathbb{N}} g^n(\bot) \), assuming \( g \) is
    monotone. It is the fixed point as \( g (\mu g) = \mu g \).

    To complete the definition of \(\mu\)-regular expressions as language
    generators, figure [[fig:mu-reg-def]] also shows the derivation relation. Leiß
    cite:null found that \(\mu\)-regular expressions describe context-free
    languages. This means that for any \(\mu\)-regular expression, there is a
    context-free grammar with the same language, and vice versa. One consequence
    of this means that general \(\mu\)-regular expressions take super-linear
    time to parse.

    Context-free grammars resolve the parse complexity problem by a
    transformation into a push-down automaton. The algebraic nature of
    \(\mu\)-regular expressions lends itself to a type system instead.
    
*** KY Type System
    *Note: All definitions are taken from cite:null. To what extent do they need
    citations?*

    The KY type system is a type judgement for \(\mu\)-regular expressions. If
    an expression is well typed, then there exists a top-down parser for the
    language of the expression.
    
    There are three properties of languages that are particularly interesting,
    named \( \Null \), \( \First \) and \( \Flast \). Their definitions are in
    figure [[fig:lang-props]]. To summarise, a langauge \( L \) is \( \Null \) when
    it contains the empty string. The \( \First \) set is the set of symbols
    starting strings in \( L \), and the \( \Flast \) set is the set of symbols
    that immediately follow strings in \( L \) to make a bigger string in
    \( L \).
    
    #+label: fig:lang-props
    #+name: fig:lang-props
    #+caption: Definitions of \( \Null \), \( \First \) and \( \Flast \)
    #+begin_figure
      \begin{gather*}
        \Null L \iff \epsilon \in L \\
        \begin{align*}
          \First L &= \{ c \in \Sigma \mid \exists w \in \Sigma^*.\, cw \in L \} \\
          \Flast L &=
             \{ c \in \Sigma
             \mid \exists w \in \Sigma^+, w' \in \Sigma^*.\,
               w \in L \wedge wcw' \in L
             \}
        \end{align*}
      \end{gather*}
    #+end_figure
    
    A /KY type/ \( \tau \) is a record \( \{\textsc{Null} \in \mathbb{B} ,
    \textsc{First} \subseteq \Sigma , \textsc{Flast} \subseteq \Sigma \}\). A
    language /satisfies/ a type, \( L \vDash \tau \), when \( \Null L \le
    \tau.\textsc{Null} \wedge \First L \subseteq \tau.\textsc{First} \wedge
    \Flast L \subseteq \tau.\textsc{Flast} \). This definition means that a type
    always over-approximates a language's properties. As types are triples of
    values, they can be manipulated by functions. Figure [[fig:mu-type]] shows some
    basic types and some operations on them. It also describes two relations on
    types, used by the typing judgement.
    
    #+label: fig:mu-type
    #+name: fig:mu-type
    #+caption: Some KY types and operations and relations on them
    #+begin_figure
    \[ b \Rightarrow s = \If b \Then s \Else \emptyset \]
    \begin{align*}
      \tau_{\bot} &= ( \False, \emptyset, \emptyset ) \\
      \tau_{\epsilon} &= ( \True, \emptyset, \emptyset ) \\
      \tau_{c} &= ( \False, \{ c \} , \emptyset )
    \end{align*}
    \begin{align*}
      \tau \vee \tau' &= \left\{ \begin{array}{rl}
           \textsc{Null} = &\tau.\textsc{Null} \vee \tau'.\textsc{Null} \\
           \textsc{First} = &\tau.\textsc{First} \cup \tau'.\textsc{First} \\
           \textsc{Flast} = &\tau.\textsc{Flast} \cup \tau'.\textsc{Flast}
         \end{array}\right\} \\
      \tau \cdot \tau' &= \left\{ \begin{array}{rl}
           \textsc{Null} = &\tau.\textsc{Null} \wedge \tau'.\textsc{Null} \\
           \textsc{First} = &\tau.\textsc{First} \cup (\tau.\textsc{Null} \Rightarrow \tau'.\textsc{First}) \\
           \textsc{Flast} = &\tau'.\textsc{Flast} \cup (\tau'.\textsc{Null} \Rightarrow \tau'.\textsc{First} \cup \tau.\textsc{Flast})
         \end{array}\right\}
    \end{align*}
    \begin{align*}
      \tau \circledast \tau' &= (\tau.\textsc{Flast} \cap \tau'.\textsc{First} = \emptyset) \wedge \neg \tau.\textsc{Null} \\
      \tau \# \tau' &= (\tau.\textsc{First} \cap \tau'.\textsc{First} = \emptyset) \wedge \neg (\tau.\textsc{Null} \wedge \tau'.\textsc{Null})
    \end{align*}
    #+end_figure

    Since the aim is to build a top-down parser, an expression cannot be left
    recursive. The KY type system achieves this using two /variable contexts/. A
    variable context is a map from variables to type. One of the variable
    contexts is /unguarded/, meaning that variables can be used freely. The
    other context is /guarded/, meaning variables can only be used on the right
    side of a concatenation.

    Figure [[fig:mu-judge]] gives the full typing judgement of the KY type system.
    Of particular note, the TFix rule assumes \( x \) is guarded in the
    hypothesis, the TCat rule shifts the guarded context into the unguarded one
    for the right side, and the TVar rule can only reference unguarded
    variables. Krishnaswami and Yallop showed cite:null that is an expression
    has a complete typing judgement when the two variable contexts are empty, it
    is possible to compute a parser for the language of that expression.
    
    #+label: fig:mu-judge
    #+name: fig:mu-judge
    #+caption: KY typing judgement
    #+begin_figure
    \begin{math}
    \begin{array}{ccc}
      \begin{prooftree}
        \infer0[TBot]{\Gamma; \Delta &\vdash \bot : \tau_{\bot}}
      \end{prooftree}
      & \qquad &
      \\
      & \qquad &
      \begin{prooftree}
        \infer0[TEps]{\Gamma; \Delta &\vdash \epsilon : \tau_{\epsilon}}
      \end{prooftree}
      \\
      \begin{prooftree}
        \infer0[TChar]{\Gamma; \Delta &\vdash [ c ] : \tau_c}
      \end{prooftree}
      & \qquad &
      \\
      & \qquad &
      \begin{prooftree}
        \infer0[TVar]{\Gamma, x : \tau; \Delta &\vdash x : \tau}
      \end{prooftree}
      \\
      \begin{prooftree}
        \hypo{\Gamma; \Delta &\vdash e : \tau} 
        \hypo{\Gamma; \Delta &\vdash e' : \tau'} 
        \hypo{\tau &\# \tau'}
        \infer3[TVee]{\Gamma; \Delta &\vdash e \vee e' : \tau \vee \tau'}
      \end{prooftree}
      & \qquad &
      \\
      & \qquad &
      \begin{prooftree}
        \hypo{\Gamma; \Delta &\vdash e : \tau} 
        \hypo{\Gamma, \Delta; \cdot &\vdash e' : \tau'} 
        \hypo{\tau &\circledast \tau'}
        \infer3[TCat]{\Gamma; \Delta &\vdash e \cdot e' : \tau \cdot \tau'}
      \end{prooftree}
      \\
      \begin{prooftree}
        \hypo{\Gamma; \Delta, x : \tau &\vdash e : \tau} 
        \infer1[TFix]{\Gamma; \Delta &\vdash \mu x. e : \tau}
      \end{prooftree}
      & \qquad &
    \end{array}
    \end{math}
    #+end_figure
    
*** Hindley-Milner Type System
    *TODO: Proof read*
    
    The simply-typed lambda calculus (STLC) is possibly the simplest possible
    type system, consisting of ground terms and functions only. System F is an
    extension of the STLC, adding /polymorphism/, where values can have multiple
    types.

    /Type inference/ is the assignment of types to expressions such that the
    expression type checks. Whilst there are arguments for and against type
    inference, when types are difficult to express, the option to elide them is
    helpful. Unfortunately, type inference for System F is undecidable
    cite:null.

    To overcome this problem, Hindley and later Milner described a type system
    with /parametric polymorphism/. Values either have a monotype, or a
    polytype. A monotype is a regular STLC type. A polytype is an abstraction
    over a monotype by adding in type variables, which are placeholders for
    arbitrary monotypes. For example, the generic identity function has the
    polytype \( \forall \alpha. \alpha \to \alpha \).

    A key part of the HM type system is /specialisation/. This is the
    instantiation of one or more free variables in a polytype. The relation
    \( \sigma \sqsubseteq \sigma' \) holds if \(\sigma\) can specialise to
    \(\sigma'\).

    The syntax and type judgement for the HM type system is given in figure
    [[fig:hm-type]]. Notice how HMVar specialises types. Conversely, only HMLet can
    /generalise/ types -- monotypes with free variables become polytypes.

    #+label: fig:hm-type
    #+name: fig:hm-type
    #+caption: HM syntax and typing judgement
    #+begin_figure
    \begin{align*}
      e &= x \mid e e \mid \lambda x. e \mid \Let x = e \In e \\
      \tau &= \alpha \mid \tau \to \tau \\
      \sigma &= \tau \mid \forall \alpha. \sigma
    \end{align*}
    \begin{math}
    \begin{array}{ccc}
    \begin{prooftree}
      \hypo{\sigma \sqsubseteq \tau}
      \infer1[HMVar]{\Gamma, x : \sigma \vdash x : \tau}
    \end{prooftree}
    & \qquad &
    \begin{prooftree}
      \hypo{\Gamma \vdash e : \tau \to \tau'}
      \hypo{\Gamma \vdash e' : \tau}
      \infer2[HMApp]{\Gamma \vdash e e' : \tau'}
    \end{prooftree}
    \\ & \qquad & \\
    \begin{prooftree}
      \hypo{\Gamma, x : \tau \vdash e : \tau'}
      \infer1[HMAbs]{\Gamma \vdash \lambda x. e : \tau \to \tau'}
    \end{prooftree}
    & \qquad &
    \begin{prooftree}
      \hypo{\Gamma \vdash e : \tau}
      \hypo{\Gamma, x : \forall \alpha. \tau \vdash e' : \tau'}
      \infer2[HMLet]{\Gamma \vdash \Let x = e \In e' : \tau'}
    \end{prooftree}
    \end{array}
    \end{math}
    #+end_figure

    By restricting introduction of polymorphism to \( \Let \) statements only,
    type inference is not only possible, but is nearly linear is almost all
    cases. The inference algorithm, called the /J algorithm/ is usually given in
    tree form, as in figure [[fig:hm-infer]]. 

    #+label: fig:hm-infer
    #+name: fig:hm-infer
    #+caption: The J algorithm
    #+begin_figure
    \begin{prooftree*}
      \hypo{\tau = inst(\sigma)}
      \infer1[JVar]{\Gamma, x : \sigma \vdash x : \tau}
    \end{prooftree*}
    \begin{prooftree*}
      \hypo{\Gamma \vdash e : \tau}
      \hypo{\Gamma \vdash e' : \tau'}
      \hypo{\tau'' = newvar()}
      \hypo{unify(\tau, \tau' \to \tau'')}
      \infer4[JApp]{\Gamma \vdash e e' : \tau''}
    \end{prooftree*}
    \begin{prooftree*}
      \hypo{\tau = newvar()}
      \hypo{\Gamma, x : \tau \vdash e : \tau'}
      \infer2[JAbs]{\Gamma \vdash \lambda x. e : \tau \to \tau'}
    \end{prooftree*}
    \begin{prooftree*}
      \hypo{\Gamma \vdash e : \tau}
      \hypo{\Gamma, x : \forall \alpha. \tau \vdash e' : \tau'}
      \infer2[JLet]{\Gamma \vdash \Let x = e \In e' : \tau'}
    \end{prooftree*}
    #+end_figure

    Instead of performing specialisation, JVar instead returns a general
    instance of a polytype. All the bound type variables are instantiated by a
    fresh generic type instance.

    Specialisation is then performed by JApp. The \(unify\) function coerces
    both arguments to their join, or type inference fails if the join doesn't
    exist. Recall that the join of two values is the least value greater than
    them both. Therefore \(unify\) performs the least amount of specialisation
    to give both arguments the same shape.

    Variations of the HM type sytem are used by many functional programming
    languages, such as ML and Haskell cite:null. 
** Requirements Analysis
   My core deliverable focused on implementing the KY type system. Having a well
   typed language description is nearly useless without a way to parse the
   language. Hence another core component was to /output a chewed parser/. These
   two components could then be used to create a parser from any Nibble
   description.

   One major feature of parser combinators is their composition into
   higher-order combinators. The KY type system cannot directly type check these
   higher-order combinators and must first perform some evaluation down to
   combinators represented by \(\mu\)-regular expressions. This can lead to an
   exponential increase in size of \(mu\)-regular expressions. I attempt to
   eliminate this issue by /exploring adding functions and lambda expressions/
   to \(\mu\)-regular expressions and to Nibble.

   There are many other ways Nibble could be extended. *TODO: list them*
** Starting Point
   I closely studied the KY type system before beginning the project. I did not
   begin any work on possible extensions to it.

   The project builds on ideas about formal languages. These have been studied
   in the /Part IA Discrete Maths/ and /Part IB Compiler Construction/ courses.
   I also did a small personal project on them during the summer of 2018.

   Additionally, the project uses concepts from type systems, covered in the
   /Part IB Semantics of Programming Languages/, /Part II Types/ and /Part II
   Denotational Semantics/ courses.
** Software Engineering
*** Project Management
    After successful development of an initial core, extensions to a programming
    language naturally tend themselves to an iterative approach. Whilst you are
    mindful of future extensions, you work towards successful implementation of
    one at a time.

    This lends itself to the spiral development model. Each component follows a
    waterfall development cycle --- design, implementation, integration and
    testing --- and no two components are developed concurrently.

    *NOTE: the rest of this section could be cut*

    This model has several other benefits. At the end of each cycle, there is a
    functional deliverable. This means that even when there are unexpected
    delays in implementing a component, there is still a functional product to
    fall back on.

    Additionally, there is a lot of flexibility in what components are
    implemented and in what order. As you work on a product, you come to better
    understand what features can be added and the cost of doing so. *TODO: More
    words here*
*** Version Control
    I used git as a version-control and revision history system. New features
    were developed on individual branches. Upon completion, they were merged
    with the main branch.

    The git repository was mirrored on both a privately-owned server and GitHub.
    Regular commits and pushes ensured that very little data was lost if there
    was an issue with my device.

    The project is dual-licensed under the MIT and Apache 2.0 licenses, as is
    common for projects written in Rust. These are permissive licenses that
    encourage development whilst limiting personal liability.
*** Development Tools
    The standard Rust build system is called Cargo. It provides an easy way to
    run several kinds of checks against the whole code base. In particular
    clippy is a static analysis tool that highlights some style improvements and
    common bugs. Also, rustfmt was regularly used to consistently format code.
    
    Some tests were performed using Rust's built-in test harness. This allows
    the user to write unit tests anywhere. It also provides a method of
    performing integration tests.

    Benchmarks were written using  criterion. This micro-benchmarking library
    measures the performance of a function by measuring thousands of iterations.
    It also provides some simple statistical analysis and comparisons between
    functions.
* Implementation
** The LM Type System
   *NOTE: need beta reductions from the start. Makes the whole section more
   complex.*
   
   This subsection has three parts. The first introduces a minimal way to add
   lambda expressions to the KY type system. The second introduces function
   types. Finally, the full LM type system is described, by adding polymorphism.
*** Function Expressions.
    The simplest way to introduce function expressions is using /macros/. These
    are syntactic substitutions of the source code. Figure [[fig:func-exp]]
    introduces the necessary syntax, derivation and typing rule to fully extend
    the KY type system with function expressions.

    #+name: fig:func-exp
    #+label: fig:func-exp
    #+caption: KY type system with function expressions
    #+begin_figure
      \[ e = \ldots \mid \lambda x. e \mid e e \]

      \centering
      \begin{prooftree}
        \hypo{e [ e' / x ] \Mapsto w}
        \infer1{(\lambda x. e) e' \Mapsto w}
      \end{prooftree}
      \qquad
      \begin{prooftree}
        \hypo{\Gamma; \Delta \vdash e [ e' / x ] : \tau}
        \infer1{\Gamma; \Delta \vdash (\lambda x. e) e' : \tau}
      \end{prooftree}
    #+end_figure

    Macros have some advantages and disadvantages. Because they are only a
    syntactic substitution, they are very easy to implement. They also help to
    eliminate repetition in expressions. However, type checking is slightly less
    efficient, because there first has to be a macro expansion stage. Finally,
    it is dubious to call macros a change to the type system, given they can be
    completely eliminated before type checking begins.
*** Function Types
    Unlike most type systems, which have a single variable context, the KY type
    system has two. Hence there needs to be two different function types. Figure
    [[fig:func-type]] shows the extended definition of types, as well as derivations
    and typing rules for functions.
    
    #+name: fig:func-type
    #+label: fig:func-type
    #+caption: KY type system with function types
    #+begin_figure
    *TODO: Complete figure*
    #+end_figure

    *TODO: remarks on rules*

    Unlike macros, function types only need to be checked once. This can lead to
    a significant performance improvement. Unfortunately this type system is
    monomorphic. Take the expression \(\lambda f. f \, (a \cdot f \, b)\). Here
    \( f \) has a single type \(\tau \to \tau'\), but is applied to arguments of
    two different types. Because \( \tau \) can be at most one of those types,
    this expression fails to type check.

    Adding implicit polymorphism would solve this failing of the typing
    judgement. Unfortunately, just like System F, implicit polymorphism would
    likely make inference undecidable. A more intelligent solution needs to be
    found.
*** Polymorphism
    *TODO: figures maybe in an appendix?*
    
    The full LM type system uses a HM-style type system as a base, extended with
    a constraint system. Figure [[fig:lm-type]] defines both monotypes \( \sigma \)
    and polytypes \( \rho \).

    #+label: fig:lm-type
    #+name: fig:lm-type
    #+caption: LM types
    #+begin_figure
    *TODO: define \(\sqsubseteq\)*
    \begin{align*}
      \tau &= \tau_\bot \mid \tau_\epsilon \mid \tau_c \mid \tau \cdot \tau \mid \tau \vee \tau \mid \alpha \\
      \sigma &= \tau \mid \sigma \overset{C}{\to} \sigma \mid \sigma \overset{C}{\leadsto} \sigma \\
      \rho &= \sigma \mid \forall \alpha. \rho
    \end{align*}
    #+end_figure

    Figure [[fig:lm-expr]] gives the syntax and derivation relation of the
    \(\lambda\mu\)-regular expressions used by the LM type system. With the
    introduction of functions, simple substitution alone is not sufficient for
    derivations. First, terms must be \(\beta\)-expanded until a primitive
    first- or second-operation combinator is at the root.
    
    #+label: fig:lm-expr
    #+name: fig:lm-expr
    #+caption: \(\lambda\mu\)-expressions, \(\beta\) equivalence and derivations
    #+begin_figure
    \[
      e = \bot \mid \epsilon \mid c \mid x \mid e \cdot e \mid e \vee e \mid \mu e \mid \lambda x. e \mid \Lambda x. e \mid e \, e \mid e \sim e \mid \Let x = e \In e \mid \Let' x = e \In e
    \]
    
    \centering
    \begin{math}
    \begin{array}{ccc}
      \begin{prooftree}
        \infer0{e &\approx_\beta e}
      \end{prooftree}
      & \qquad &
      \begin{prooftree}
        \hypo{e &\approx_\beta \mu e'}
        \infer1{e &\approx_\beta e' \sim \mu e'}
      \end{prooftree}
      \\ & \qquad & \\
      \begin{prooftree}
        \hypo{e &\approx_\beta e'}
        \infer1{e \, e'' &\approx_\beta e' \, e''}
      \end{prooftree}
      & \qquad &
      \begin{prooftree}
        \hypo{e &\approx_\beta e'}
        \infer1{e \sim e'' &\approx_\beta e' \sim e''}
      \end{prooftree}
      \\ & \qquad & \\
      \begin{prooftree}
        \hypo{e &\approx_\beta (\lambda x. e') e''}
        \infer1{e &\approx_\beta e' [e'' / x]}
      \end{prooftree}
      & \qquad &
      \begin{prooftree}
        \hypo{e &\approx_\beta (\Lambda x. e') \sim e''}
        \infer1{e &\approx_\beta e' [e'' / x]}
      \end{prooftree}
      \\ & \qquad & \\
      \begin{prooftree}
        \hypo{e &\approx_\beta \Let x = e' \In e''}
        \infer1{e &\approx_\beta e'' [e' / x]}
      \end{prooftree}
      & \qquad &
      \begin{prooftree}
        \hypo{e &\approx_\beta \Let' x = e' \In e''}
        \infer1{e &\approx_\beta e'' [e' / x]}
      \end{prooftree}
      \\ & \qquad & \\
    \end{array}
    \end{math}

    \begin{math}
    \begin{array}{ccc}
      \begin{prooftree}
        \infer0{\epsilon &\Mapsto \epsilon}
      \end{prooftree}
      & \qquad &
      \begin{prooftree}
        \infer0{c &\Mapsto c}
      \end{prooftree}
      \\ & \qquad & \\
      \begin{prooftree}
        \hypo{e &\Mapsto w}
        \infer1{e \vee e' &\Mapsto w}
      \end{prooftree}
      & \qquad &
      \begin{prooftree}
        \hypo{e' &\Mapsto w}
        \infer1{e \vee e' &\Mapsto w}
      \end{prooftree}
      \\ & \qquad & \\
      \begin{prooftree}
        \hypo{e &\Mapsto w}
        \hypo{e' &\Mapsto w'}
        \infer2{e \cdot e' &\Mapsto ww'}
      \end{prooftree}
      & \qquad &
      \begin{prooftree}
        \hypo{e &\approx_\beta e'}
        \hypo{e' &\Mapsto w}
        \infer2{e &\Mapsto w}
      \end{prooftree}
    \end{array}
    \end{math}
    #+end_figure

    Finally, there is the LM typing judgement,
    \( \Gamma; \Delta \vdash e : \sigma; C \), where \( C \) meaning that under
    context \( \Gamma; \Delta \), expression \( e \) has monotype \( \sigma \)
    when the constraints \( C \) are all met. The definition is in [[fig:lm-judge]].
    
    #+label: fig:lm-judge
    #+name: fig:lm-judge
    #+caption: LM typing judgement
    #+begin_figure
    *TODO: be more formal about \( \forall \alpha. \sigma\)*
    \centering
    \begin{math}
    \begin{array}{ccc}
      \begin{prooftree}
        \infer0{\Gamma; \Delta \vdash \bot : \tau_\bot ; \emptyset}
      \end{prooftree}
      & \qquad &
      \begin{prooftree}
        \infer0{\Gamma; \Delta \vdash \epsilon : \tau_\epsilon ; \emptyset}
      \end{prooftree}
      \\ & \qquad & \\
      \begin{prooftree}
        \infer0{\Gamma; \Delta \vdash c : \tau_c ; \emptyset}
      \end{prooftree}
      & \qquad &
      \begin{prooftree}
        \hypo{\rho \sqsubseteq \sigma}
        \infer1{\Gamma, x : \rho; \Delta \vdash x : \sigma ; \emptyset}
      \end{prooftree}
      \\ & \qquad & \\
      \begin{prooftree}
        \hypo{\Gamma; \Delta \vdash e : \tau ; C}
        \hypo{\Gamma, \Delta; \cdot \vdash e' : \tau' ; C'}
        \infer2{\Gamma; \Delta \vdash e \cdot e' : \tau \cdot \tau' ; C \cup C' \cup \{\tau \circledast \tau'\}}
      \end{prooftree}
      & \qquad &
      \begin{prooftree}
        \hypo{\Gamma; \Delta \vdash e : \tau ; C}
        \hypo{\Gamma; \Delta \vdash e' : \tau' ; C'}
        \infer2{\Gamma; \Delta \vdash e \vee e' : \tau \vee \tau' ; C \cup C' \cup \{\tau \# \tau'\}}
      \end{prooftree}
      \\ & \qquad & \\
      \begin{prooftree}
        \hypo{\Gamma, x : \sigma; \Delta \vdash e : \sigma' ; C}
        \infer1{\Gamma; \Delta \vdash \lambda x. e : \sigma \overset{C}{\to} \sigma' ; \emptyset}
      \end{prooftree}
      & \qquad &
      \begin{prooftree}
        \hypo{\Gamma; \Delta, x : \sigma \vdash e : \sigma' ; C}
        \infer1{\Gamma; \Delta \vdash \Lambda x. e : \sigma \overset{C}{\leadsto} \sigma' ; \emptyset}
      \end{prooftree}
      \\ & \qquad & \\
      \begin{prooftree}
        \hypo{\Gamma; \Delta \vdash e : \sigma \overset{C''}{\to} \sigma' ; C}
        \hypo{\Gamma; \Delta \vdash e' : \sigma ; C'}
        \infer2{\Gamma; \Delta \vdash e \, e' : \sigma' ; C \cup C' \cup C''}
      \end{prooftree}
      & \qquad &
      \begin{prooftree}
        \hypo{\Gamma; \Delta \vdash e : \sigma \overset{C''}{\leadsto} \sigma' ; C}
        \hypo{\Gamma, \Delta; \cdot \vdash e' : \sigma ; C'}
        \infer2{\Gamma; \Delta \vdash e \sim e' : \sigma' ; C \cup C' \cup C''}
      \end{prooftree}
      \\ & \qquad & \\
      \begin{prooftree}
        \hypo{\Gamma; \Delta \vdash e : \sigma ; C}
        \hypo{\Gamma, x : \forall \alpha. \sigma; \Delta \vdash e' : \sigma' ; C'}
        \infer2{\Gamma; \Delta \vdash \Let x = e \In e' : \sigma' ; C \cup C'}
      \end{prooftree}
      & \qquad &
      \begin{prooftree}
        \hypo{\Gamma, \Delta; \cdot \vdash e : \sigma ; C}
        \hypo{\Gamma, x : \forall \alpha. \sigma; \Delta \vdash e' : \sigma' ; C'}
        \infer2{\Gamma; \Delta \vdash \Let' x = e \In e' : \sigma' ; C \cup C'}
      \end{prooftree}
      \\ & \qquad & \\
      \begin{prooftree}
        \hypo{\Gamma; \Delta \vdash e : \tau \overset{C'}{\leadsto} \tau ; C}
        \infer1{\Gamma; \Delta \vdash \mu e : \tau ; C \cup C'}
      \end{prooftree}
      & \qquad &
    \end{array}
    \end{math}
    #+end_figure

    Note that typing rules for elements taken from \(\mu\)-regular expressions
    remain largely the same. One change is that fixed points now take an
    explicit function instead of implicitly defining one. The second change is
    that concatenation and alternation move the type constraint from the premise
    to the overall constraint set. The final change is that variable references
    now use specialisation.
    
    *TODO: why two lets? can it be only one?*
    
    *TODO: comment on other rules*

    *TODO: prove if there's time*

    I conjecture that for all expressions \( e \), if \( \cdot; \cdot \vdash e :
    \tau ; C \) and constraints \( C \) are met, then I can generate a
    deterministic linear-time parser for the language described by \( e \).
** Nibble
   *TODO: implement language changes in Nibble*

   Figure [[fig:nibble-ex]] shows how Nibble can describe basic arithmetic.

   *TODO: decide whether to describe current state or future goal*

   #+label: fig:nibble-ex
   #+name: fig:nibble-ex
   #+caption: Nibble code describing basic arithmetic
   #+begin_src rust
          // TODO: update with syntax change
          let opt(x) = _ : None | x : Some;
          let star(x) = [rec](opt(x.rec));
          let plus(x) = (x : First).(star(x) : Rest);

          let ws = star(" ");
          let list(x, p) = (x : First).(star(p.ws.x) : Rest);

          let number = plus("0" | "1" | "2"| "3" | "4" | "5" | "6" | "7" | "8" | "9");
          let term(e) =
             (( number : Pos
              | "-".ws.number : Neg
              | "(".ws.(e : Inner).")" : Parens)
              : RawTerm).ws;
          let prod(e) = list(term(e), "*"|"/");
          let expr(e) = list(prod(e), "+"|"-");
          let arith = [e](expr(e));

          match [rec]((" ".rec : Base | arith) : Ast);
   #+end_src

   There are some differences between Nibble and \(\lambda\mu\)-regular
   expressions. First, \(\bot\) is not at all present in Nibble. Being able to
   match an empty language is not a useful feature in practice.

   Additionally, let expressions can only appear at the top level -- a let
   expression cannot appear within any other expression. *Why?*. Let expressions
   also have some syntactic sugar to make it easier to define functions.
   *Explain.*
   
** Chomp
  *TODO: very much a sketch*
*** Libraries and Frameworks
    * Why did I use Rust?
      * Familiar with the language
      * ~proc_macro~ takes arbitrary embedded code and replaces with arbitrary
        source code
    * Why did I use ~proc_macro2~?
      * Can build Rust token stream for output code generation
      * Compatible with built-in ~proc_macro~ system for easy staging
    * Why did I use ~syn~?
      * Parser framework for a ~proc_macro~ token stream
      * Chomp can reuse parser components from Rust language
    * Why did I use ~quote~?
      * Standard way to output a token stream
      * Takes source code and converts into token stream constructor at compile
        time.
*** Interesting Components
**** Variable Context
**** Visitor Pattern
**** Code Generation
* Evaluation
  *TODO: very much a sketch*
** Validity
   * Chewed parsers fulfil Nibble description
     * Proof or bootstrapping evidence
** Qualitative
   * Nibble is easier to write than BNF
     * Very subjective
     * Left factoring makes things quite difficult
       * Just as hard for LL BNF
     * Different types of functions could be confusing
     * Lack of lexer!
   * Nibble is easier to read than BNF
     * Very subjective
     * More control over alternation position makes things easier
     * Lack of lexer!
   * Chomp is easier to integrate
     * Than parser combinators
       * Requires thought
     * Than traditional parser generators
       * Maybe
       * Harder to add to project
       * Easier to write supporting code
     * Than hand-made parsers
       * No
       * No additional components
       * Easier to write supporting code
       * Harder to write parser elements
** Quantitative
   * Chewed parsers have linear time complexity
     * True
     * Include graph
   * Chewed parser performance
     * Not slower than parser combinators
       * Write benchmark first
     * Not slower than traditional parser generators
       * Comparable
       * Have JSON benchmarks
     * Not slower than hand-made parsers
       * False
       * Have JSON benchmarks
   * Chewed parser compilation
     * Not slower than parser combinators
       * Write external benchmark
     * Not slower than traditional parser generators
       * Write external benchmark
     * Not slower than hand-made parsers 
       * Write external benchmark
* Conclusion
* References
  \printbibliography[heading=none]{}
