#+latex_class: dissertation
#+latex_class_options: [12pt,a4paper,twoside,openright]
#+latex_header: \usepackage[hyperref=true,url=true,backend=biber,natbib=true]{biblatex}
#+latex_header: \usepackage[margin=1in]{geometry}
#+latex_header: \usepackage{booktabs,parskip,stmaryrd}
#+latex_header: \addbibresource{diss.bib}
#+options: H:6

\pagestyle{headings}
* Introduction
  A language is a set of strings over an alphabet. They can be described by a
  set of production rules, which describe how to transform semantic ideas into
  syntactic strings. The exact rules used to create a string is called its
  derivation.

  There are different classes of language based on the complexity of their
  rules. The two most-studied are regular languages and context-free languages.
  Regular languages are the simplest class of practical languages. Context-free
  languages are a larger superset.

  A parser is a function that transforms a string into its derivation. Parsers
  are used by nearly all interpreters and compilers to convert the linear text
  presented by the file system into the abstract structure of the source code.

  Generating parsers for programming languages has been done in practice for
  over 60 years cite:null. Writing the hundreds of lines of necessary to
  describe how to parse some input requires a huge amount of repetition and
  boiler-plate code. Parser generators can express the same logic in only a few
  tens of production rules, which they expand into full parser source code.

  Modern parser generators transform production rules of context-free grammars into
  deterministic push-down automata. This allows creation of parsers that run in
  linear time cite:null, with high-quality error reporting cite:null.

  With the increasing popularity of more powerful type systems, another approach
  to parsing is becoming increasingly common. Parser combinators are
  higher-order functions that take a number of parsing functions and combine
  them into a new, more complex parsing function. Unfortunately, most parser
  combinators rely on some form of backtracking before they can reject an input
  string.

  Regular expressions are a concise way to describe regular languages cite:null,
  equivalent to using production rules. \(\mu\)-regular expressions
  cite:10.1007/BFb0023771 are a small extension of regular expressions. They are
  a concise way to describe context-free languages, also equivalent to using
  production rules cite:null.

  Krishnaswami and Yallop cite:10.1145/3314221.3314625 designed a type system
  (henceforth known as the /KY type system/) for \(\mu\)-regular expressions
  that only accepts languages that can be parsed in linear time. They also
  implemented a parser generator using this type system in OCaml, and provided
  evidence its performance is comparable to other parser generators.
  
** Project Overview
   I start in section ref:null by defining an extension of \(\mu\)-regular
   expressions adding functions. We extend type safety and semantics results
   from Krishnaswami and Yallop cite:10.1145/3314221.3314625 to these
   /\(\lambda\mu\)-regular expressions/. The extended type system will be called
   the /LM type system/.

   Next, I design /Nibble/ in section ref:null. Nibble is a new DSL for
   context-free languages. The syntax is based on \(\lambda\mu\)-regular
   expressions.
   
   In section ref:null, I implement /Chomp/, a typed parser generator for
   Nibble. Chomp uses the LM type system to ensure its input is suitable for
   transformation into a recursive-descent parser. Chomp is implemented in Rust,
   and produces Rust source code as output. This output is a /chewed parser/.
   
   I end by evaluating the performance of chewed parsers. Critically in section
   ref:null we show that chewed parsers operate in linear time. Section ref:null
   demonstrates that chewed parsers have comparable performance to other
   generated parsers. Finally section ref:null demonstrates that Nibble is
   potentially usable in practice.
* Preparation
** Background
   *TODO: some preamble*
   
*** Formal Languages
    Given an alphabet \( \Sigma \), a language \( L \) is a set
    \( L \subseteq \Sigma^* \). There are three properties of particular
    interest to us: Null, First and Flast.

    A language \( L \) is Null if it contains the empty string. The First set of
    \( L \) is the set of characters that can start a string in \( L \). Given a
    string \( u \in L \), a character \( x \) is in the Flast set of \( L \) if
    there is some string \( v \) such that \( uxv \in L \). Formal definitions
    are given in figure [[fig:lang-prop-def]].

    #+label: fig:lang-prop-def
    #+caption: Definitions of the Null, First and Flast properties of language \(L\).
    #+begin_figure
    \begin{align*}
      \mathrm{Null}(L) &\iff \epsilon \in L \\
      \mathrm{First}(L) &= \{ x \in \Sigma
                           \mid \exists w \in \Sigma^*. xw \in L
                           \} \\
      \mathrm{Flast}(L) &= \{ x \in \Sigma
                           \mid \exists w \in \Sigma^* / \epsilon,
                                \exists w' \in Sigma^*. w \in L \wedge
                                                        wxw' \in L
                           \}
    \end{align*}
    #+end_figure

**** Chomsky Hierarchy
     *TODO: needs citations*
     
     Introduce a set \( N \) of non-terminal symbols. Distinguish one symbol
     \( S \in N \) as the start symbol. A production rule \( u \to v \) is a
     rewriting rule, where \( u \in (\Sigma \uplus N)^* N (\Sigma \uplus N)^*\)
     and \( v \in (\Sigma / S \uplus N)^* \).

     A set of production rules forms a grammar, \( G \).

     From the start symbol, the string is successively rewritten using the
     grammar. For a rule \( u \to v \), a string \( wuw' \) can be rewritten as
     \( wvw' \). This repeats until there are no more non-terminal symbols in
     the string. The final string is in the language generated by the grammar,
     \( L(G) \) and the sequence of rule applications is the derivation.

     Chomsky cite:null found restricting the form of strings \( u \) and \( v \)
     in production rules results in different language classes. These language
     classes are shown in table [[tab:chomsky]].

     #+label: tab:chomsky
     #+caption: Chomsky Hierarchy of grammars
     #+attr_latex: :float t :booktabs :align lccl
     | Grammar | \(u \in\)                                     | \(v \in\)                   | Languages              |
     |---------+-----------------------------------------------+-----------------------------+------------------------|
     | Type-0  | \((\Sigma \uplus N)^* N (\Sigma \uplus N)^*\) | \((\Sigma / S \uplus N)^*\) | Recursively enumerable |
     | Type-1  | \((\Sigma \uplus N)^* N (\Sigma \uplus N)^*\) | \((\Sigma / S \uplus N)^+\) | Context-sensitive      |
     | Type-2  | \(N\)                                         | \((\Sigma / S \uplus N)^*\) | Context-free           |
     | Type-3  | \(N\)                                         | \(N \cup N(\Sigma / S)\)    | Regular                |

**** \(\mu\)-Regular Expressions
     Figure [[fig:regex]] gives the syntax and denotation of regular
     expressions. Their denotation is how they are interpreted as a language.
     *Someone* cite:null proved there is an equivalence between Type-0 grammars
     and regular expressions.

     #+label: fig:regex
     #+caption: Syntax and denotation of regular expressions
     #+begin_figure
       \[
         e ::= \bot \mid \epsilon \mid c \mid e + e \mid e \cdot e \mid e^*
       \]
       \begin{align*}
         &\llbracket \bot \rrbracket &&= \emptyset \\
         &\llbracket \epsilon \rrbracket &&= \{ \epsilon \} \\
         &\llbracket c \rrbracket &&= \{ c \} \\
         &\llbracket e + e' \rrbracket &&=
           \llbracket e \rrbracket \cup \llbracket e' \rrbracket \\
         &\llbracket e \cdot e' \rrbracket &&=
           concat(\llbracket e \rrbracket, \llbracket e' \rrbracket) \\
         &\llbracket e^* \rrbracket &&=
           star(\llbracket e \rrbracket)
       \end{align*}
       \begin{align*}
         star(X) &= \bigcup_{i \in \mathbb{N}} L_i \text{ where }
           \begin{array}{ll}
             L_0 & = \{ \epsilon \} \\
             L_{n+1} & = concat(X, L_n)
           \end{array} \\
         concat(X, Y) &= \{ ww' \in \Sigma^* \mid \exists w \in X, w' \in Y \}
       \end{align*}
     #+end_figure

     \(\mu\)-regular expressions make two modifications. The first is the
     introduction of variables. The second is a generalisation of the Kleene
     star \( e^* \) to fixed point expressions \( \mu x. e \). Because of
     the addition of variables, the denotation is now a function from a variable
     map to languages. The new syntax and denotation is in figure [[fig:mu-regex]].

     #+label: fig:mu-regex
     #+caption: Syntax and denotation of \(\mu\)-regular expressions
     #+begin_figure
       \[
         g ::= \bot \mid \epsilon \mid c \mid x \mid g + g \mid g \cdot g \mid \mu x. g
       \]
       \begin{align*}
         &\llbracket \bot \rrbracket \gamma &&= \emptyset \\
         &\llbracket \epsilon \rrbracket \gamma &&= \{ \epsilon \} \\
         &\llbracket c \rrbracket \gamma &&= \{ c \} \\
         &\llbracket x \rrbracket \gamma &&= \gamma(x) \\
         &\llbracket g + g' \rrbracket \gamma &&=
           \llbracket g \rrbracket \gamma \cup
           \llbracket g' \rrbracket \gamma \\
         &\llbracket g \cdot g' \rrbracket \gamma &&=
           \{ ww' \in \Sigma^*
           \mid \exists w \in \llbracket g \rrbracket \gamma,
                        w' \in \llbracket g' \rrbracket \gamma
           \} \\
         &\llbracket \mu x. g \rrbracket \gamma &&=
           fix(\lambda X. \llbracket g \rrbracket (\gamma, X/x))
       \end{align*}
       \[
         fix(f) = \bigcup_{n \in \mathbb{N}} f^n (\bot)
       \]
     #+end_figure
     
     Leiß cite:null proved that \( e^* = \mu x. \epsilon + e \cdot x \), hence
     \(\mu\)-regular expressions can represent all regular languages. *Someone*
     cite:null further showed that \(\mu\)-regular expressions are equivalent to
     Type-1 grammars.

*** KY Type System
    * key language properties
    * type definition
    * type satisfiability
    * variable context
    * type judgement
   
*** Parser Generators

**** State Machines
    
     * machines for each hierarchy

**** \(LR(\kappa)\)
     * problem with cfl
     # * LR(κ) solution

**** Parser Combinators
     * describe combinators
     * describe problems

*** Formal Languages
    *NOTE: This could possibly benefit from a couple of figures.*
    
    *NOTE: Much of the second half is just a summary of Krishnaswami and
    Yallop.*

    A language is a set of strings. Some notable examples are: the empty
    language, \( \emptyset \), containing no strings; the language containing
    only the empty string, \( \{ \epsilon \} \); and the universal language, \(
    U \), containing all strings.
    
    A /recogniser/ is an algorithm that determines whether a given string is in
    a language. We can group languages by the minimum computational complexity
    of a recogniser. For example, a language is /regular/ if it can be
    recognised in constant space.

    A /regular expression/ is a description of a /regular language/ cite:null.
    Different expressions can be composed by three operators: concatenation,
    alternation and Kleene star. The regular expression \( baaa* \) describes a
    language used by sheep, and \( ((\epsilon|b)ooga)* \) a language used by
    stereotypical cave-people.

    Whilst regular expressions are useful, they have many limitations. For use
    in artificial languages, the most important is that regular expressions
    cannot match parentheses. That requires /context-free languages/.
    Recognisers for context-free languages need linear space and polynomial
    time.

    Leiß cite:10.1007/BFb0023771 showed that much like how regular expressions
    describe regular languages cite:null, /\(\mu\)-regular expressions/ describe
    context-free languages. These are regular expressions extended with a
    fixed-point operator. This allows for expressions such as
    \( \mu\alpha.([\alpha]|a) \),
    which is the language for the strings ~a~, ~[a]~, ~[[a]]~, ...

    Leiß cite:10.1007/BFb0023771 also found that fixed-point operators can
    replace the need for the Kleene star operator.
    \( a* = \mu\alpha.\epsilon|a\alpha \)

    Krishnaswami and Yallop cite:10.1145/3314221.3314625 found there are three
    properties of languages we find important, dubbed Null, First and Flast. A
    language \( L \) is Null if \( \epsilon \in L \). A character is in the
    First set of a language if there is a string in the language starting with
    that character. For example, \( ((\epsilon|b)ooga)* \) has First set
    \( \{ o, b \} \).

    The Flast set of a language is harder to define. Suppose there is a string
    \( u \in L \). If there is some string \( v \) and character \( x \) such
    that \( uxv \in L \), then \( x \) is in the Flast set of \( L \).
    Intuitively, it is the set of characters that follow an accepted string and
    can make a new string.
    
*** Parsing Techniques
    A /parser/ is a type of recogniser, which also extracts structure from the
    input string. This structure is usually dubbed the /concrete parse tree/.
    Given a \(\mu\)-regular expression, a parser for its language could give a
    tree following the structure of the expression. *EXAMPLE?* 

**** Backus-Naur Form
     Variations of the Backus-Naur form (BNF) are the standard way to describe
     context-free languages. It describes a context-free language from its
     production rules.

     *TODO: insert bnf example*
     
     Figure ref:null shows an example of BNF code. Strings of characters, such
     as ~"+"~, represent literal input strings. These are called terminal
     symbols.

     Names surrounded by angle brackets, such as ~<term>~, are called
     non-terminal symbols. They represent places where a production rule will be
     substituted, instead of any literal input. Every non-terminal is assigned a
     set of production rules, separated by ~|~ characters.

     Other syntax is much like regular expressions. There are various standard
     and non-standard extensions to this core syntax.

     One criticism of BNF is that the production rules are part of a single,
     mutually-recursive namespace. In our example, ~<expr>~ refers to ~<term>~
     which refers to ~<expr>~. This single-namespace creates hidden dependencies
     between different production rules, which can lead to surprising behaviour
     when one is modified.

     Someone *NOTE: find out who* cite:null showed that production rules used by
     BNF and \(\mu\)-regular expressions are equivalent.
     
**** Parser Combinators
     Consider concatenation and alternation. They each combine a number of
     smaller expressions into one bigger expression. A parser for them would
     combine a number of smaller parsers into one bigger parser. A construct
     that combines or extends the behaviour of smaller parsers is called a
     /parser combinator/.

     Let's think about fixed-point expressions. They don't take a parser as an
     argument. Instead, the argument is itself a parser combinator. This makes
     the fixed-point operator a /higher-order parser combinator/, mapping a
     parser combinator into a parser. Much like there is a hierarchy of function
     orders, we find there is a similar hierarchy for parser combinators.

     In general, parser combinators require some form of backtracking. Take for
     example the regular expression \(apple|aardvark\). When we see an ~a~, we
     don't know whether this the start of ~apple~ or ~aardvark~. This
     non-determinism is why backtracking is essential.
     
**** Recursive Descent
     Recursive descent parsers are the "natural" solution to parsing. cite:null
     If you ask a student with no knowledge about parsers to construct a parser,
     chances are they will build a recursive descent parser.

     Recursive descent parsing is a top-down technique. You start with the
     highest structural elements and work down to the small details. Parser
     combinators are a form of recursive descent parser.

     An interesting sub-class of recursive-descent languages are the
     \(LL(\kappa)\) languages. These are languages that can be parsed by
     recursive descent without backtracking cite:null. This is the class of
     languages that are well-typed under the KY type system.

**** Recursive Ascent
     Whilst recursive descent work from the top down, recursive ascent parsers
     work from the bottom upwards cite:null. Starting from the smallest details,
     you build your way up to a final complete structure.

     Recursive ascent parsers are typically implemented as push-down automata
     cite:null. They keep a stack of parse results and use a large state table
     to determine what action to perform.

     One of the most widely-used recursive-ascent language classes is the
     \(LR(\kappa)\) class of languages. This is what is used by most of the
     popular parser generators cite:null. This is because \(LR(\kappa)\)
     languages are non-backtracking.

*** KY Type System
    Under the KY type system, the /denotation/ of an expression is the language
    it represents . This is shown in figure ref:null.

    *TODO: Add denotation figure*
    
    A /type/ in the KY type system is a triple of three values: \textsc{Null},
    \textsc{First} and \textsc{Flast}. Two important types are \( \bot= \{
    \textsc{Null} = \mathbf{false}, \textsc{First} = \emptyset, \textsc{Flast} =
    \emptyset \} \) and \( \epsilon = \{ \textsc{Null} = \mathbf{true},
    \textsc{First} = \emptyset, \textsc{Flast} = \emptyset \} \).
    
    A language /satisfies/ a type if the type over-approximates the language
    properties. For example, \( \emptyset \) satisfies all types, because it has
    the most restrictive properties.

    Next, we describe guarded and unguarded variables. In most programming
    languages, Once a variable is defined, it can be used anywhere. This is not
    the case in this type system. When a fixed point introduces a variable, it
    cannot be used immediately; it is /guarded/. The variable can only be used
    when it becomes /unguarded/, which happens on the right side of
    concatenations.

    In the expression \( \mu\alpha.\alpha x \), \( \alpha \) is guarded, so
    cannot be referenced. However, in the expression \(
    \mu\alpha.\epsilon|a\alpha \), it appears on the right of a concatenation,
    so it is unguarded.

    A /variable context/ is a pair of maps from variables to types. The two maps
    correspond to guarded and unguarded variables. Because of this, a variable
    can only appear in one of the two maps at a time.
    
    A typing judgement is a relation between variable contexts, expressions, and
    types. \( \Gamma, \Delta \vdash e : \tau \) can be read: expression \( e \)
    has type \( \tau \) in context \( \Gamma, \Delta \). Krishnaswami and Yallop
    cite:10.1145/3314221.3314625 found that if an expression and type are
    related by the KY typing judgement in figure cite:null, then the language of
    the expression satisfies the type.
    
    *TODO: insert type rules figure*
    
    Krishnaswami and Yallop cite:10.1145/3314221.3314625 found there are three
    sources of back-tracking for parser combinators: sequential non-determinism,
    disjunctive non-determinism and non-left-factoring.

    Consider the regular expression \(a*a*\). Given a string such as ~aaaa~,
    there are five ways to parse it. This ambiguity is /sequential
    non-determinism/. In general, it occurs when there are multiple ways to
    split a string to parse a concatenation.

    Now look at \((aa* | a*a)\). Given the string ~aaaa~, each alternative can
    parse it in only one way. However, both alternatives can parse the string.
    /disjunctive non-determinism/ is when multiple alternatives can parse the
    same string.

    Finally, think about the earlier \(apple|aardvark\) example. This is not
    disjunctive non-determinism because the two alternatives have disjoint
    languages. However, assume a parser can only see one character from the
    input string at a time. If we can only see the ~a~ symbol, it is impossible
    to know which alternative to take. This expression is not /left-factored/
    --- the two alternatives share a prefix.
** Requirements Analysis
   My core deliverable focused on implementing the KY type system. Having a well
   typed language description is nearly useless without a way to parse the
   language. Hence another core component was /output of a chewed parser/. These
   two components could then be used to create a parser from any Nibble
   description.

   Whilst sufficient to describe any context-free language, \(\mu\)-regular
   expressions are awfully verbose. One stretch requirement was to introduce
   /lambda expressions and named functions/. This would allow users of Nibble to
   extract common patterns, greatly reducing the amount of Nibble they would
   need to write.

   There are many other ways Nibble could be extended. *TODO: list them*
** Starting Point
   I closely studied the KY type system before beginning the project. I did not
   begin any work on possible extensions to it.

   The project builds on ideas about formal languages. These have been studied
   in the /Part IA Discrete Maths/ and /Part IB Compiler Construction/ courses.
   I also did a small personal project on them during the summer of 2018.

   Additionally, the project uses concepts from type systems, covered in the
   /Part IB Semantics of Programming Languages/, /Part II Types/ and /Part II
   Denotational Semantics/ courses.
** Software Engineering
*** Project Management
    After successful development of an initial core, extensions to a programming
    language naturally tend themselves to an iterative approach. Whilst you are
    mindful of future extensions, you work towards successful implementation of
    one at a time.

    This lends itself to the spiral development model. Each component follows a
    waterfall development cycle --- design, implementation, integration and
    testing --- and no two components are developed concurrently.

    *NOTE: the rest of this section could be cut*

    This model has several other benefits. At the end of each cycle, there is a
    functional deliverable. This means that even when there are unexpected
    delays in implementing a component, there is still a functional product to
    fall back on.

    Additionally, there is a lot of flexibility in what components are
    implemented and in what order. As you work on a product, you come to better
    understand what features can be added and the cost of doing so. *TODO: More
    words here*
*** Version Control
    I used git as a version-control and revision history system. New features
    were developed on individual branches. Upon completion, they were merged
    with the main branch.

    The git repository was mirrored on both a privately-owned server and GitHub.
    Regular commits and pushes ensured that very little data was lost if there
    was an issue with my device.

    The project is dual-licensed under the MIT and Apache 2.0 licenses, as is
    common for projects written in Rust. These are permissive licenses that
    encourage development whilst limiting personal liability.
*** Development Tools
    The standard Rust build system is called Cargo. It allows an easy way to run
    several kinds of checks against the whole code base. In particular  clippy 
    is a static analysis tool that some style improvements and common bugs.
    Also,  rustfmt  was regularly used to consistently format code.
    
    Tests were performed using Rust's built-in test harness. This allows the
    user to write unit tests anywhere. It also provides a method of performing
    integration tests.

    Benchmarks were written using  criterion . This micro-benchmarking library
    measures the performance of a function by measuring thousands of iterations.
    It also provides some simple statistical analysis and comparisons between
    functions.

**** NOTE: Comments on tests
     I don't have enough/any unit tests. All my tests come from some simple
     integration tests. I am relying on AutoChomp being so complex and fragile
     that any errors in the type system would break it entirely.
* Implementation
** \(\lambda\mu\)-Regular Expressions
    *EXT: describe \(\lambda\mu\)-regular expressions*
* Evaluation
* Conclusion
* Glossary
  # - chewed parser: output parser from Chomp
  # - Chomp: a parser generator for Nibble using the LM type system
  # - derivation: set of production rules to produce a string
  # - First: \( First(L) = \{ x \mid \exists w \in \Sigma^*. xw \in L \} \)
  # - Flast:
  #   \( First(L) = \{ x \mid \exists w \in \Sigma^*/\emptyset, w\' \in \Sigma^*. w \in L \wedge wxw' \in L \} \)
  # - grammar: set of production rules
  # - KY type system: type system for \(\mu\)-regular expressions
  # - language: set of strings over an alphabet
  # - LM type system: type system for \(\lambda\mu\)-regular expressions
  # - Nibble: DSL for \(\lambda\mu\)-regular expressions
  # - non-terminal: another set of symbols
  # - Null: \( Null(L) \iff \emptyset \in L \)
  # - parser: function from string to derivation
  # - parser combinator: higher-order function from parser to parser
  # - parser generator: program from production rules to parser source code
  # - production rule: transform semantic idea into syntactic string
  # - regular expression: concise description of regular language
  # - start symbol: distinguished non-terminal
  # - \(\lambda\mu\)-regular expression: extension of \(\mu\)-regular expression
  #   with functions
  # - \(\mu\)-regular expression: concise description of context-free language
* References
  \printbibliography[heading=none]{}
