#+latex_class: dissertation
#+latex_class_options: [12pt,a4paper,twoside,openright]
#+latex_header: \usepackage[hyperref=true,url=true,backend=biber,natbib=true]{biblatex}
#+latex_header: \usepackage[vmargin=2cm,hmargin=1in]{geometry}
#+latex_header: \usepackage[chapter]{minted}
#+latex_header: \usepackage[binary-units]{siunitx}
#+latex_header: \usepackage{booktabs,ebproof,parskip,standalone,stmaryrd,syntax}
#+latex_header: \addbibresource{diss.bib}


# math operators
#+latex_header: \DeclareMathOperator{\True}{true}
#+latex_header: \DeclareMathOperator{\False}{false}
#+latex_header: \DeclareMathOperator{\If}{if}
#+latex_header: \DeclareMathOperator{\Then}{then}
#+latex_header: \DeclareMathOperator{\Else}{else}
#+latex_header: \DeclareMathOperator{\Let}{let}
#+latex_header: \DeclareMathOperator{\In}{in}
#+latex_header: \DeclareMathOperator{\Null}{null}
#+latex_header: \DeclareMathOperator{\First}{first}
#+latex_header: \DeclareMathOperator{\Flast}{flast}

# shorthand
#+latex_header: \newcommand\mre{\(\mu\)-regular expression}
#+latex_header: \newcommand\mres{\(\mu\)-regular expressions}
#+latex_header: \newcommand\ky{KY}
#+latex_header: \newcommand\hm{Hindley-Milner}

# try to avoid widows and orphans
#+latex_header: \raggedbottom
#+latex_header: \sloppy
#+latex_header: \clubpenalty1000%
#+latex_header: \widowpenalty1000%

# Other options
#+options: toc:nil H:6

# Word count
#+name: word-count
#+begin_src shell :exports none
  tmp="$(mktemp)"
  sed -e '/begin{minted}/,/end{minted}/d' diss.tex >"$tmp"
  texcount -sum -1 "$tmp"
  rm "$tmp"
#+end_src

#+RESULTS: word-count
: 11102

#+begin_src emacs-lisp :exports none
  (defun tables-recalc (backend)
    (org-table-recalculate-buffer-tables))

  (add-hook 'org-export-before-processing-hook #'tables-recalc)
#+end_src

#+RESULTS:
| tables-recalc |

#+latex: %TC:ignore
# ##############################################################################
# Title
\pagestyle{empty}
\rightline{\LARGE\bf Greg Brown}

\vspace*{60mm}
\begin{center}
\Huge
{\bf A Typed, Algebraic Parser Generator} \\[5mm]
Computer Science Tripos -- Part II \\[5mm]
Queens' College \\[5mm]
\today
\end{center}
  
# ##############################################################################
# Declaration of Originality 
\pagebreak{}

** Declaration
   :PROPERTIES:
   :UNNUMBERED: notoc
   :END:

   I, Greg Brown of Queens' College, being a candidate for Part II of the
   Computer Science Tripos, hereby declare that this dissertation and the work
   described in it are my own work, unaided except as may be specified below,
   and that the dissertation does not contain material that has already been
   used to any substantial extent for a comparable purpose.

   I am content for my dissertation to be made available to the students and
   staff of the University.

   \bigskip
   \leftline{Signed Greg Brown}
   
   \medskip
   \leftline{Date \today}

# ##############################################################################
# Proforma
* Proforma
  :PROPERTIES:
  :UNNUMBERED: notoc
  :END:
  
  \pagestyle{plain}
  \pagenumbering{roman}

  | \large Candidate Number:   | \large 2374B                                                    |
  | \large Project Title:      | \large *A Typed, Algebraic Parser Generator*                    |
  | \large Examination:        | \large *Computer Science Tripos -- Part II, 2021*               |
  | \large Word Count:         | #ERROR                                                          |
  | \large Line Count:         | \large 4758[fn:: Calculated using ~scc~, ignoring test inputs.] |
  | \large Project Originator: | \large The dissertation author                                  |
  | \large Supervisor:         | \large Prof. Alan Mycroft                                       |
  #+TBLFM: @4$2='(concat "\\large " (org-sbe "word-count") "[fn\:\: Calculated using ~texcount~.]")

** Original Aims of the Project

   The core aims were to design a language /Nibble/ to describe formal
   languages, and then implement the \ky{} type system in a parser generator
   /Chomp/. Unlike traditional parser generators, which view formal languages as
   automata, Chomp instead takes the approach that languages form an algebra.
   Nibble and parsers generated by Chomp would be compared against the
   traditional analogues, evaluating whether this new approach to parsing has
   benefits for language designers, conceptually or computationally. One
   extension was to add functions Nibble and the \ky{} type system, reducing
   repetition in Nibble and redundant computations in Chomp.
   
** Work Completed
   Exceeded success criterion. Nibble describes formal languages with function
   expressions. Two different type systems were developed for Nibble, including
   one with polymorphic types. Chomp can generate parsers from Nibble. These
   parsers have comparable performance to traditionally-generated parsers, and
   in some cases outperform handwritten parsers.
   
** Special Difficulties
   None.

# ##############################################################################
# Contents

#+toc: headlines 2
list-of-figures:nil
#+toc: listings
# #+toc: tables

# ##############################################################################
# Disertation Body
#+latex: %TC:endignore
* Introduction
  \pagestyle{headings}
  \pagenumbering{arabic}
  
  A /formal language/ is a set of strings over some alphabet of symbols. For
  example, a dictionary enumerates a language over written characters. Spoken
  English is a language, where the alphabet consists of phonemes. Programming
  languages are over an alphabet of ASCII or Unicode characters. IP packets are
  languages over bytes. The language of a decision problem is the set of valid
  inputs.
  
  Whilst linear strings are helpful for data storage and transmission, they have
  limited use for other algorithms. Strings are used to encode other non-linear
  data. A /parser/ is a function that takes a language string and decodes it to
  return the underlying data. Parsers should be fast; why spend time decoding
  data when there is useful computation to be done? Unfortunately, hand writing
  efficient parsers is repetitive, with lots of boiler plate, and requires
  careful consideration of the order of operations.

  To help solve this problem, people created /parser generators/ -- programs
  that take descriptions of languages and output source code for a parser.
  Typical efficient parser generators operate by parsing languages with a finite
  state machine. By storing the state transition table in memory, the run-time
  cost of a parser is minimal.

  As optimising compilers become more powerful, more complex source code can be
  compiled down to equally-efficient machine code. This allows us to approach
  parsing from a different direction, without incurring a performance penalty.
  Instead of parsing formal languages using finite state machine, we can
  interpret languages as algebraic expressions. One example algebra is \mres{},
  which were described by Leiß cite:10.1007/BFb0023771. We can then produce an
  intuitive function-based parsing algorithm based on these \mres{}, as opposed
  to the opaque parsing tables used in traditional parser generators.

  Krishnaswami and Yallop took this approach in a recent paper
  cite:10.1145/3314221.3314625. One key contribution the pair made was the
  development of a type system (the \ky{} type system) for \mres{}. If a \mre{}
  was well-typed, then they could produce a linear-time parser for it. They
  produced a implementation of the \ky{} type system using parser combinators,
  which are higher order functions that combine zero or more parsers together to
  make a new parser.
  
** Project Overview
  This project seeks to produce a parser generator based on \mres{}. It makes
  the following contributions:
  
   * I design the /Nibble/ language to describe parsers, based on \mres{}
     (section [[*The Nibble Language]]). This language adds functions and let
     bindings to \mres{}, making it more ergonomic. I determine that Nibble is
     as easy to use as BNF, meaning future parser generators could use
     Nibble-like languages as input.

   * I describe *two* type systems for the Nibble language, extending the \ky{}
     type system (section [[*Type Systems for Nibble]]). Both type systems have
     simple inference algorithms, eliminating the need for type annotations in
     the Nibble language.

   * I implement the /Chomp/ parser generator, which produces Rust source code
     for parsers from a Nibble description (section [[*Chomp Repository Overview]].
     Chomp uses one of the type systems I described to ensure Chomp-generated
     parsers run in linear time (section [[*Performance of Chewed Parsers]]).

   * I demonstrate the Nibble language and Chomp parser generator are suitable
     for use in complex projects by creating /AutoNibble/ (section [[*Meeting the
     Success Criterion]]). AutoNibble is a Chomp-generated parser for the Nibble
     language. AutoNibble outperforms a handwritten parser for the Nibble
     language (section [[*Performance of AutoNibble]]).

* Preparation
  We start this section by describing the wider computer science necessary to
  understand the rest of this dissertation. Next, we discuss the requirements
  for the Nibble language and the Chomp parser generator, and the software
  engineering techniques used to achieve them. Finally, we mention the starting
  point of the project.
  
** Background
   This section starts with a recap on formal languages, from the perspective of
   formal grammars and finite automata. Next we cover \mres{} and the \ky{} type
   system, which view languages as algebraic objects. We then skip over to
   discuss translators, in particular the architecture they typically use.
   Finally, we discuss the features of Rust used by the implementation of the
   Chomp parser generator.
   
*** Formal Languages
    A formal language is a set of strings over some finite alphabet. For
    example, written English words are a formal language over the English
    alphabet, spoken sentences are a formal language over phonemes, and
    programming languages such as Rust are formal languages over Unicode
    characters.

    Most useful formal languages have some structure to them, where every string
    has a derivation that describes this structure. Parsing is the task of
    computing a derivation from a string. Consider the following example. Sheep
    can only say "baa" followed by some number of additional "a"s. The
    derivations for this sheep language could be the natural numbers. A parser
    would count the total number of "a"s, and subtract two. Notice how a
    derivation has no connection to the meaning, or semantics, of a string.

    A parser generator is a program that takes a description of a formal
    language and produces a parser for it. Because all of the strings in a
    language can be generated from a derivation, and a parser finds a derivation
    for a given string, a parser generator only needs to receive a description
    of the form of derivations to be able to generate a parser.
    # *TODO: maybe example?*

**** The Chomsky Hierarchy
     Traditionally, languages have been specified using formal grammars. We
     extend the original alphabet with some additional /non-terminal/ symbols.
     One of these is the start symbol, \(S\). To create a string in the language
     of a grammar, we start with the string consisting of the start symbol.
     Then, we repeatedly apply string rewriting rules called /production rules/
     until there are no more non-terminal symbols. Every production rule must
     consume at least one non-terminal, although they can produce many more.
     
     An example grammar, and the derivation of a string in the grammar, are
     shown in figure [[fig:grammar-sheep]]. This grammar describes the language
     used by sheep. The start symbol gives us the prefix "baa", and a looping
     non-terminal \(A\). \(A\) either pushes an "a" symbol before it, or
     removes itself from the string.

     #+label: fig:grammar-sheep
     #+name: fig:grammar-sheep
     #+caption: An example formal grammar and a derivation.
     #+begin_figure
     \begin{align*}
       S &\Mapsto baaA \\
       A &\Mapsto aA \\
       A &\Mapsto \epsilon
     \end{align*}
     \[
       S \Mapsto baaA \Mapsto baaaA \Mapsto baaaaA \Mapsto baaaa
     \]
     #+end_figure

     Chomsky cite:10.1016/S0019-99585990362-6 detailed a classification of
     formal grammars depending on the form of the production rules: type 0
     through type 3. The smaller the number, the less restricted the rules are,
     and the larger the class of possible languages. Chomsky further discovered
     that each class can be parsed by a different form of finite automata.

     The languages of type 2 grammars are commonly called /context-free
     languages/. These are the most-restrictive grammars in the hierarchy that
     have matched delimiters, which are essential for programming languages.
     These grammars take polynomial time to parse in general. Fortunately, there
     are some sub-classes of context-free languages that can be parsed in linear
     time. The most common of these are LL and LR languages, covered in the Part
     IB Compiler Construction course.
     
**** BNF
     BNF is a formal language to describe grammars. Its syntax is designed to
     resemble the production rules of the mathematical definition. Literal
     symbols are surrounded by quotes. Non-terminal symbols are surrounded by
     angle brackets. Figure [[lst:bnf-sheep]] shows a BNF description of the sheep
     language. The ~<start>~ form corresponds to rules for the \(S\)
     non-terminal. Similarly, the ~<loop>~ form corresponds to the \(A\)
     non-terminal.

     #+label: lst:bnf-sheep
     #+name: lst:bnf-sheep
     #+caption: An example BNF description.
     #+begin_src bnf
     <start> ::= "baa" <loop>
     <loop>  ::= "" | "a" <loop>
     #+end_src

     BNF has a single global namespace. When a form is declared, it can be used
     anywhere else in the description. For example, ~<loop>~ is used before its
     declaration. BNF uses mutually-recursive scope -- different forms can refer
     to themselves in a cycle.
*** \mres{}
    As an alternative to viewing languages as described by grammars, languages
    are also algebraic objects. This was the viewpoint considered by Leiß when
    they described \mres{} cite:10.1007/BFb0023771, described in figure
    [[fig:mre-syntax]].
    
    #+label: fig:mre-syntax
    #+name: fig:mre-syntax
    #+caption: The syntax of \mres{}
    #+begin_figure
      \[
        e = \bot
          \mid \epsilon
          \mid c
          \mid e \cdot e
          \mid e \vee e
          \mid \mu x. e
          \mid x
      \]
    #+end_figure

    There are three constant languages: \(\bot\) for the empty language,
    \(\epsilon\) for the language of the empty string only, and \( c \) for a
    language containing the single-symbol string \( c \) only.

    There are combined with two binary operators. Concatenation, \( g \cdot g'
    \) takes strings from \(g\) and concatenates them with strings from \(g'\).
    Alternation, \( g \vee g' \), forms the union of the languages \(g\) and
    \(g'\). For brevity, we sometimes use juxtaposition instead of the
    concatenation operator.

    Finally, there is the least-fixed-point operator, \(\mu \alpha. g(\alpha)\).
    This finds the smallest language for \alpha that contains all the strings in
    \(g(\alpha)\).
    
    Finally, there is the least-fixed-point combinator \(\mu g\). This is the
    union \( \bigcup_{n\in\mathbb{N}} g^n(\bot) \), assuming \( g \) is
    monotone. It is the fixed point as \( g (\mu g) = \mu g \).

    Figure [[fig:mre-sheep]] shows an example \mre{}, again describing the sheep
    language. Like the BNF example (figure [[lst:bnf-sheep]]), we start with the
    constant prefix \(baa\). Next we have the fixed point expression. This
    is the union of the empty string and the symbol \(a\) followed by the
    fixed point expression -- a string of zero or more "a" symbols.

    #+label: fig:mre-sheep
    #+name: fig:mre-sheep
    #+caption: An example \mre{}.
    #+begin_figure
    \[
      baa \cdot \mu \alpha. (\epsilon \vert a \cdot \alpha)
    \]
    #+end_figure
    
    Leiß cite:10.1007/BFb0023771 found that \mres{} describe the full set of
    context-free languages only. This means that for every \mre{}, there is a
    BNF description for the same language.

    # *TODO: be consistent with BNF terms.*
    
    Unlike BNF, where alternatives are split into many small, reusable rules,
    \mres{} are always part of one long expression. This has problems for
    readability, especially for some repetitive real-world languages. See figure
    [[fig:mre-hex-colour]] which gives a \mre{} for describing a colour in
    hexadecimal ~#RRGGBBAA~ format, where the alpha component is optional. The
    whole list of hexadecimal digits is listed eight times.
    
    #+label: fig:mre-hex-colour
    #+name: fig:mre-hex-colour
    #+caption: \mres{} can contain a lot of repetition. The full list of
    #+caption: hexadecimal digits must be listed eight times.
    #+begin_figure
    \[
      \# \cdot (0 \vert \cdots \vert F) \cdot (0 \vert \cdots \vert F) \cdot (0 \vert \cdots \vert F) \cdot (0 \vert \cdots \vert F) \cdot (0 \vert \cdots \vert F) \cdot (0 \vert \cdots \vert F) \cdot (\epsilon \vert (0 \vert \cdots \vert F) \cdot (0 \vert \cdots \vert F))
    \]
    #+end_figure
   
**** The \ky{} Type System
     The \ky{} type system is a type judgement for \mres{}. If an expression is
     well typed, then there exists a linear-time parser for the language of the
     expression.
    
     There are three properties of languages that are particularly interesting,
     named \( \Null \), \( \First \) and \( \Flast \). Their definitions are in
     figure [[fig:lang-props]]. To summarise, a langauge \( L \) is \( \Null \) when
     it contains the empty string. The \( \First \) set is the set of symbols
     starting strings in \( L \), and the \( \Flast \) set is the set of symbols
     that immediately follow strings in \( L \) to make a bigger string in \( L
     \).
    
     #+label: fig:lang-props
     #+name: fig:lang-props
     #+caption: Definitions of the \( \Null \), \( \First \) and \( \Flast \)
     #+caption: properties of languages.
     #+begin_figure
       \begin{gather*}
         \Null L \iff \epsilon \in L \\
         \begin{align*}
           \First L &= \{ c \in \Sigma \mid \exists w \in \Sigma^*.\, cw \in L \} \\
           \Flast L &=
              \{ c \in \Sigma
              \mid \exists w \in \Sigma^+, w' \in \Sigma^*.\,
                w \in L \wedge wcw' \in L
              \}
         \end{align*}
       \end{gather*}
     #+end_figure
    
     A /\ky{} type/ \( \tau \) is a record \( \{\textsc{Null} \in \mathbb{B} ,
     \textsc{First} \subseteq \Sigma , \textsc{Flast} \subseteq \Sigma \}\). As
     types are triples of values, they can be manipulated by functions. Figure
     [[fig:mre-type]] shows some basic types and some operations on them. It also
     describes two constraints on types, used by the typing judgement.
    
     #+label: fig:mre-type
     #+name: fig:mre-type
     #+caption: The \ky{} types, two binary operations on them, and the two
     #+caption: constraints \(\circledast\) and \(\#\).
     #+begin_figure
     \[ b \Rightarrow s = \If b \Then s \Else \emptyset \]
     \begin{align*}
       \tau_{\bot} &= ( \False, \emptyset, \emptyset ) \\
       \tau_{\epsilon} &= ( \True, \emptyset, \emptyset ) \\
       \tau_{c} &= ( \False, \{ c \} , \emptyset )
     \end{align*}
     \begin{align*}
       \tau \vee \tau' &= \left\{ \begin{array}{rl}
            \textsc{Null} = &\tau.\textsc{Null} \vee \tau'.\textsc{Null} \\
            \textsc{First} = &\tau.\textsc{First} \cup \tau'.\textsc{First} \\
            \textsc{Flast} = &\tau.\textsc{Flast} \cup \tau'.\textsc{Flast}
          \end{array}\right\} \\
       \tau \cdot \tau' &= \left\{ \begin{array}{rl}
            \textsc{Null} = &\tau.\textsc{Null} \wedge \tau'.\textsc{Null} \\
            \textsc{First} = &\tau.\textsc{First} \cup (\tau.\textsc{Null} \Rightarrow \tau'.\textsc{First}) \\
            \textsc{Flast} = &\tau'.\textsc{Flast} \cup (\tau'.\textsc{Null} \Rightarrow \tau'.\textsc{First} \cup \tau.\textsc{Flast})
          \end{array}\right\}
     \end{align*}
     \begin{align*}
       \tau \circledast \tau' &= (\tau.\textsc{Flast} \cap \tau'.\textsc{First} = \emptyset) \wedge \neg \tau.\textsc{Null} \\
       \tau \# \tau' &= (\tau.\textsc{First} \cap \tau'.\textsc{First} = \emptyset) \wedge \neg (\tau.\textsc{Null} \wedge \tau'.\textsc{Null})
     \end{align*}
     #+end_figure

     Unlike most type systems, the \ky{} type system uses two variable contexts.
     This is so all fixed-point variables occur after a concatenation, to avoid
     a problem with recursion in the parser implementation. If a variable is
     /unguarded/, it can be used anywhere. Otherwise, if the variable is
     /guarded/, it cannot be used freely.
     
     Figure [[fig:ky-rules]] gives the full typing judgement of the \ky{} type
     system. Of particular note, the fixed-point rule assumes \( x \) is guarded
     in the hypothesis, the concatenation rule shifts the guarded context into
     the unguarded one for the right side, and the variable rule can only
     reference unguarded variables. Krishnaswami and Yallop showed
     cite:10.1145/3314221.3314625 that is an expression has a complete typing
     judgement when the two variable contexts are empty, it is possible to
     compute a parser for the language of that expression.
    
     #+label: fig:ky-rules
     #+name: fig:ky-rules
     #+caption: The \ky{} typing judgement.
     #+begin_figure
     \centering
     \begin{math}
     \begin{array}{ccc}
       \begin{prooftree}
         \infer0{\Gamma; \Delta &\vdash \bot : \tau_{\bot}}
       \end{prooftree}
       & \qquad &
       \begin{prooftree}
         \infer0{\Gamma; \Delta &\vdash \epsilon : \tau_{\epsilon}}
       \end{prooftree}
       \\
       & \qquad &
       \\
       \begin{prooftree}
         \infer0{\Gamma; \Delta &\vdash [ c ] : \tau_c}
       \end{prooftree}
       & \qquad &
       \begin{prooftree}
         \infer0{\Gamma, x : \tau; \Delta &\vdash x : \tau}
       \end{prooftree}
       \\
       & \qquad &
       \\
       \begin{prooftree}
         \hypo{\Gamma; \Delta &\vdash e : \tau} 
         \hypo{\Gamma; \Delta &\vdash e' : \tau'} 
         \hypo{\tau \# \tau'}
         \infer3{\Gamma; \Delta &\vdash e \vee e' : \tau \vee \tau'}
       \end{prooftree}
       & \qquad &
       \begin{prooftree}
         \hypo{\Gamma; \Delta &\vdash e : \tau} 
         \hypo{\Gamma, \Delta; \cdot &\vdash e' : \tau'} 
         \hypo{\tau \circledast \tau'}
         \infer3{\Gamma; \Delta &\vdash e \cdot e' : \tau \cdot \tau'}
       \end{prooftree}
       \\
       & \qquad &
       \\
       \begin{prooftree}
         \hypo{\Gamma; \Delta, x : \tau &\vdash e : \tau} 
         \infer1{\Gamma; \Delta &\vdash \mu x. e : \tau}
       \end{prooftree}
       & \qquad &
     \end{array}
     \end{math}
     #+end_figure
*** The \hm{} Type System
    The simply-typed lambda calculus is possibly the simplest possible
    type system, consisting of ground terms and functions only. System F extends
    the lambda calculus by adding /polymorphism/, where values can have multiple
    types. Unfortunately, type inference, the problem of assigning types to
    expressions, is undecidable for System F cite:10.1109/LICS.1994.316068.

    To overcome this problem, Hindley and later Milner described a different
    type system with decidable inference. Like System F, it has polymorphic
    types and type variables. The difference is that only let expressions can
    have polymorphic types. The typing rules are detailed in figure [[fig:hm-type]].

     #+label: fig:hm-type
     #+name: fig:hm-type
     #+caption: The \hm{} typing judgement.
     #+begin_figure
     \centering
     \begin{math}
     \begin{array}{ccc}
     \begin{prooftree}
       \hypo{\sigma \sqsubseteq \tau}
       \infer1{\Gamma, x : \sigma \vdash x : \tau}
     \end{prooftree}
     & \qquad &
     \begin{prooftree}
       \hypo{\Gamma \vdash e : \tau \to \tau'}
       \hypo{\Gamma \vdash e' : \tau}
       \infer2{\Gamma \vdash e e' : \tau'}
     \end{prooftree}
     \\ & \qquad & \\
     \begin{prooftree}
       \hypo{\Gamma, x : \tau \vdash e : \tau'}
       \infer1{\Gamma \vdash \lambda x. e : \tau \to \tau'}
     \end{prooftree}
     & \qquad &
     \begin{prooftree}
       \hypo{\Gamma \vdash e : \tau}
       \hypo{\Gamma, x : \forall \alpha. \tau \vdash e' : \tau'}
       \infer2{\Gamma \vdash \Let x = e \In e' : \tau'}
     \end{prooftree}
     \end{array}
     \end{math}
     #+end_figure
     
    A key part of the \hm{} type system is /specialisation/. This is the
    instantiation of one or more free variables in a polymorphic type. The
    relation \( \sigma \sqsubseteq \sigma' \) holds if \(\sigma\) can specialise
    to \(\sigma'\), and is used in the variable rule.

    Conversely, the let typing rule /generalises/ types. First, the bound
    expression is type checked. Then, all the free type variables are bound by
    the universal quantification. Finally, the body is type checked with this
    new expression.

    # *TODO: maybe example.*

*** Translators
    Translators are programs that transform one formal language into another
    whilst preserving the semantics. A familiar example are natural language
    translators, which map sentences from one language into another whilst
    preserving the meaning. An example from computer science are compilers,
    which translate source code into machine code, such that when they are both
    executed the result is the same.

    Translators consist of three different phases, named "ends". The front-end
    parses the source language into a source derivation. The middle-end
    transforms the source derivation into a target derivation, preserving the
    semantics. Finally, the back-end generates the target language from the
    target derivation.

    We will consider two examples: compilers and parser generators. The
    front-end of a compiler parses the source code into an abstract syntax tree.
    Functional languages typically introduce De Bruijn indices at this stage,
    which are introduced in section [[*De Bruijn Indices]]. The middle-end has two
    roles. The first is to type check the abstract syntax tree. If this fails,
    then the compiler cannot guarantee that language semantics are preserved, so
    execution stops. Otherwise, the middle end produces intermediate code -- the
    derivation for machine code. Finally the back-end uses the intermediate
    representation to produce machine code. In some cases, the back-end is
    itself a translator.

    Parser generators are another example of translators. The front-end of a
    parser generator receives a DSL for describing formal languages. This is
    parsed into an abstract syntax tree for the DSL. The middle-end then
    attempts to produce an abstract description for the parsing algorithm. Some
    parser generators produce action tables you would use to describe Turing
    machines. Others create decision trees. In any case, if the generator cannot
    produce an algorithm that matches the described formal language then the
    translator produces an error. Finally, the back-end uses the abstract
    algorithm description to generate source code for the target programming
    language.

    Often, the most complex part of a translator is the middle-end. By creating
    a strong separation between the three phases, the front-end and back-ends
    can be easily modified or replaced to accept different source languages or
    output different target languages respectively. For example, the back-end of
    many compilers can produce machine code for different instruction sets and
    platforms. Clang and GCC, two popular C compilers, can both act as
    front-ends to the GCC compiler.

**** De Bruijn Indices
     Most functional programming languages have a property called
     \alpha-renaming. To summarise, given any program, if you rename all
     occurrences of any variable then the semantics of the program do not
     change. De Bruijn indices exploit the lexical scoping features of
     functional programming languages to provide all variables with a name that
     is invariant under \alpha-renaming. This can simplify many algorithms, such
     as substitution.

     Consider the OCaml expression ~(fun x y -> x y) y~ . If we try to naively
     evaluate the expression, by substituting ~y~ for ~x~, we get the term ~(fun
     y -> y y)~. However, this has different semantics than the original
     expression. A correct substitution would be ~(fun z -> y z)~. Notice how we
     had to rename the bound variable.

     Using De Bruijn indices, we no longer need to rename variables. Observe how
     variables form a stack -- first, we declare ~y~ earlier in the program.
     Within the anonymous function, we bind ~x~ and ~y~ to new variables, and
     outside of the function those bindings are popped off. De Bruijn indices
     represent variables by their position from the top of the variable stack.
     
     The OCaml expression from earlier becomes ~(fun . . -> 0 1) 0~ when using
     De Bruijn indices, assuming that ~y~ was the last variable declared before
     this expression. Notice how the anonymous function does not need to name
     its parameters -- the De Bruijn indices uniquely identify every variable.

     After substitution, we have the function ~(fun . -> 1 0)~. First, the
     variable ~x~ eliminated, hence removed from the variable stack. That means
     that the ~y~ inside the anonymous function was promoted to the top of the
     stack, so its index decreased -- the ~1~ became a ~0~. Outside the
     function, the variable ~y~ was at the top of the stack, so it had index
     ~0~. As it moved into the function, another variable was pushed onto the
     stack above it, so its index was changed to ~1~.

     By using De Bruijn indices, the originally difficult problem of renaming
     variables during substitution has become a simple transformation of
     incrementing and decrementing some integers.
*** Rust
    # *TODO: brief introduction of Rust.*
**** Traits
     Rust provides traits to use as an abstract interface for data types. A
     trait defines a set of methods that implementors have to define. This is
     analogous to interfaces in Java. The biggest difference between Java
     interfaces and Rust traits is that Rust's traits are implemented
     externally -- every trait implementation has its own block.

     Also like Java interfaces, Rust traits can be used in type bounds for
     generics. An example is in figure [[fig:trait-example]]. This defines a trait
     ~Parse~, with method ~take~. All implementors of this trait have to
     define the ~take~ function. The ~take~ function takes a value generic type,
     which much implement the ~Parser~ trait.

     #+label: fig:trait-example
     #+name: fig:trait-example
     #+caption: An example trait definition.
     #+begin_src rust
       trait Parse {
         fn take<P : Parser>(input: &mut P) -> Self;
       }
     #+end_src
**** Procedural Macros
     A procedural macro is a program that receives a stream of Rust tokens and
     outputs a different stream of Rust tokens. There are two primary uses for
     procedural macros: to extend existing Rust code; and to add new syntax to
     Rust.

     The primary use of procedural macros is to extend existing Rust code. These
     add additional definitions to Rust data types. For example, if a data type
     is annotated with the ~#[derive(Debug)]~ attribute, then an implementation
     of the ~Debug~ trait is generated for that data type.

     # *TODO: worded badly.*
     
     This project is primarily concerned with the other use of procedural macros
     -- to add new syntax to Rust. It is possible to embed a DSL within Rust,
     and using procedural macros, produce Rust source code doing any operations
     you like.

     One example used in this project is ~quote~. This procedural macro
     transforms Rust source code into an abstract syntax tree representing that
     code. This is used in the back-end of the Chomp parser generator (section
     [[*The Back-End: Code Generation]]) to produce parser code.
** Requirements Analysis
   The primary goal of the project was to provide a parser generator for the
   \ky{} type system. Achieving this goal requires three parts. First, the
   Nibble language has to provide a syntax for \mres{}. Second, the Chomp parser
   generator has to implement the \ky{} type system. Third, the Chomp parser
   generator has to output source code for a parser.

   After completing this primary deliverable, the Nibble language and Chomp
   parser generators could be extended with new features, one after another.
   This lends itself to the spiral development model, where each new feature
   undergoes a complete waterfall development cycle -- design, implement,
   integrate, test.

   It is useful to be able to concurrently work on many features at once during
   the design phase, to be able to gauge the difficulty in completing a full
   implementation and to see the ways in which different features can conflict
   with each other. This is only possible with strict version control measures,
   so that each feature remains separate and so that a functional deliverable is
   always accessible.

   To solve these problems, I used ~git~ for version control. Development of the
   core deliverable took place on the ~master~ branch. Once it was complete, all
   new features were developed on different branches. Several branches were
   added for the exploratory design of each potential feature. Once I decided on
   a particular feature to implement, I proceeded to complete its waterfall
   cycle on its design branch. Then, the ~master~ branch was rebased onto the
   feature branch.

   Many additional features were considered for inclusion in this project as
   stretch requirements. I performed a MoSCoW analysis for each potential
   feature, shown in table [[tbl:moscow]]. This ranked features by the impact on the
   design of the Nibble language and the complexity of implementation in the
   Chomp parser generator.
   
   #+label: tbl:moscow
   #+name: tbl:moscow
   #+attr_latex: :align p{0.2\linewidth}p{0.75\linewidth}
   #+caption: A MoSCoW analysis of features to include in the project.
   | Priority    | Feature                                                          |
   |-------------+------------------------------------------------------------------|
   | Must Have   | Embed \mres{}; Implement \ky{} type system; Generate Rust parser |
   | Should Have | Let statements; Function expressions; Type inference;            |
   | Could Have  | User-defined parser errors; Semantic actions                     |
   | Won't Have  | Lexer                                                            |
   
   One important stage of each waterfall development cycle was testing. Most
   tests for the Chomp parser generator were end-to-end tests. By using the
   visitor pattern, described in section [[*The Visitor Design Pattern]], the
   implementation of an operation is broken down into a different operation for
   each syntax node in the Nibble language. Each of these sub-operations were
   small enough to verify by inspection. Therefore, only large inputs needed
   testing, and the easiest way to provide large test inputs is with end-to-end
   tests.

   A key part of the test suite was AutoNibble, the Chomp-generated parser for
   the Nibble language. If the outputs of AutoNibble and the handwritten parser
   for the Nibble language used by Chomp were equal, then it was highly likely
   that Chomp worked correctly.

   When a new feature was added to the project, it could introduce regressions
   in the behaviour of existing end-to-end tests. Once the source of the problem
   was identified, I added a regression unit test to exercise the issue, to
   save time if a future extension reintroduced the problem.

   This project was intended to be a proof-of-concept for parser generators
   based on the \ky{} type system. Therefore, the code was made publicly
   available on both my personal website and on GitHub. It is dual-licensed
   under the MIT and Apache 2.0 licenses, like many Rust projects, such that
   other people can reuse code as part of any project or in any form, as long as
   they include the licenses.
  
# *** Development Tools
#     The standard Rust build system is called Cargo. It provides an easy way to
#     run several kinds of checks against the whole code base. In particular
#     clippy is a static analysis tool that highlights some style improvements and
#     common bugs. Also, rustfmt was regularly used to consistently format code.
    
#     Some tests were performed using Rust's built-in test harness. This allows
#     the user to write unit tests anywhere. It also provides a method of
#     performing integration tests.

#     Benchmarks were written using  criterion. This micro-benchmarking library
#     measures the performance of a function by measuring thousands of iterations.
#     It also provides some simple statistical analysis and comparisons between
#     functions.

** Starting Point
   I closely studied the \ky{} type system before beginning the project. I did
   not begin any work on possible extensions to it.

   I had previous experience with using the Rust language for personal projects.

   The project builds on ideas about formal languages. These have been studied
   in the /Part IA Discrete Maths/ and /Part IB Compiler Construction/ courses.
   I also did a small personal project on them during the summer of 2018.

   Additionally, the project uses concepts from type systems, covered in the
   /Part IB Semantics of Programming Languages/, /Part II Types/ and /Part II
   Denotational Semantics/ courses.
* Implementation
   
  There are two areas of implementation for this project. The first is the
  design of Nibble, which is a DSL for describing formal languages made to fix
  the problems of \mres{} as explored in section [[*\mres{}]]. It also describes the
  theoretical implementation of two type systems for Nibble. The second explores
  the implementation of Chomp, a parser generator implemented in Rust. Chomp
  takes Nibble expressions and, for well-typed expressions, produces Rust source
  code for a parser of the formal language described by the Nibble expression.

  We start in section [[*The Nibble Language]] by introducing the syntax and
  semantics of Nibble, explaining how it solves the problems of \mres{}. Then in
  section [[*Type Systems for Nibble]], we describe the design of two type systems
  for Nibble: \ky{}-\lambda in section [[*The \ky{}-\lambda Type System]] and LM in
  section [[*The LM Type System]].

  Next we move on to describing Chomp, starting with the structure of its code
  repository and overall architecture in section [[*Chomp Repository Overview]].
  Chomp has an architecture similar to other compilers and translators (section
  [[*Translators]]). The front-, middle- and back-ends of Chomp are described in
  sections [[*The Front-End: Parsing and Normalisation]], [[*The Middle-End: Type
  Inference]], and [[*The Back-End: Code Generation]] respectively.

** The Nibble Language
   Nibble is a DSL for describing formal languages. Semantically, a Nibble
   expression represents a formal language. Nibble is designed to be a
   user-friendly alternative to \mres{}. In section [[*\mres{}]], we discussed some
   issues with \mres{} that made them impractical to describe non-trivial
   languages. Nibble solves these problems by introducing let statements and
   lambda abstractions.

   For Nibble expressions to be a suitable replacement for \mres{}, Nibble must
   be able to describe the same set of languages. Nibble achieves this by
   directly embedding \mres{}. Listing [[lst:nibble-embeds-mu]] shows how Nibble
   embeds the \mre{} from figure [[fig:mre-sheep]].

   #+label: lst:nibble-embeds-mu
   #+name: lst:nibble-embeds-mu
   #+caption: Nibble expressions can embed \mres{}.
   #+begin_src nibble
   match "b".!(/x/ (_|"a" . x));
   #+end_src

   # *TODO: Is this the best place for this paragraph? Could move to section
   # [[*\mres{}]]*
   
   The first difference between Nibble and \mres{} is that Nibble does not embed
   \(\bot\). By itself, \(\bot\) has no practical use -- there is no need to
   parse the empty language. When combined with other combinators, \(\bot\) is
   either an annihilator or the identity, demonstrated in figure [[fig:bot-elim]].
   This means any \mre{} containing \(\bot\) is either semantically equivalent
   to \(\bot\), or semantically equivalent to an \mre without \(\bot\).

   #+label: fig:bot-elim
   #+name: fig:bot-elim
   #+caption: Equalities to eliminate \(\bot\) from \mres{}.
   #+begin_figure
   \begin{align*}
     \bot \cdot e &= \bot \\
     e \cdot \bot &= \bot \\
     \bot \vert e &= e \\
     e \vert \bot &= e \\
     \mu x. \bot &= \bot
   \end{align*}
   #+end_figure

   In the current form, Nibble doesn't solve any of the problems with \mres{}.
   The first step to solving the repetition issue is to introduce let
   expressions. Listing [[lst:nibble-let-expression]] demonstrates how Nibble can
   eliminate the simple repetition from [[fig:mre-hex-colour]].

   #+label: lst:nibble-let-expression
   #+name: lst:nibble-let-expression
   #+caption: Nibble expressions can introduce variables with let statements.
   #+begin_src nibble
     let hex = "0"|"1"|"2"|"3"|"4"|"5"|"6"|"7"|"8"|"9"
             | "a"|"b"|"c"|"d"|"e"|"f"
             | "A"|"B"|"C"|"D"|"E"|"F";
     match "#" . hex . hex . hex . hex . hex . hex . (_ | hex . hex);
   #+end_src

   A let statement introduces a new variable name, the binding variable, and
   assigns it a Nibble expression, the bound expression. In this case, the
   variable ~hex~ is assigned to a hexadecimal character. The variable can be
   used repeatedly in following statements.

   # *TODO: Is this too tangential for this section? Maybe the evaluation is a
   # better fit...*

   Unlike BNF which, as we discussed in section [[*BNF]], has a mutually-recursive
   global namespace, Nibble's let statements use non-recursive lexical scoping.
   This means that a Nibble expression can only refer to variables in previous,
   complete let statements. Listing [[lst:nibble-lexical-scope]] shows two invalid
   variable uses.

   #+label: lst:nibble-lexical-scope
   #+name: lst:nibble-lexical-scope
   #+caption: An example violation of the Nibble language variable scoping
   #+caption: rules.
   #+begin_src nibble
     // Forbidden because `second` is defined after `first`.
     let first = "a" . second;
     let second = "b";

     // Forbidden because `e` cannot refer to itself.
     let e = _ | "a" . e;
   #+end_src

   Whilst let statements can eliminate verbatim repetition, they do not help
   with repetitive patterns, where there are only minor differences between
   different instances of a pattern. Nibble handles this with lambda
   abstractions, which are demonstrated in [[lst:nibble-lambda]].

   #+label: lst:nibble-lambda
   #+name: lst:nibble-lambda
   #+caption: Lambda and application expressions add functions to the Nibble
   #+caption: language.
   #+begin_src nibble
     let opt = /x/ _ | x;
     let plus x = !(/plus/ x . opt plus);
     match plus "a" . plus "b" . plus "c";
   #+end_src
   
   There are two ways to introduce a lambda abstraction: either through a lambda
   expression ~/x/ e~, or through a let-lambda statement ~let x(y) = e~.

   # *TODO: move sugar to normalisation in section [[*The Front-End: Parsing and
   # Normalisation]]?*

   The let-lambda statements are /syntactic sugar/ for a let-statement binding a
   lambda-expression. They are indistinguishable in their semantics and how they
   are type checked.

   Notice how in the second line of the figure, the fixed point operator ~!~
   takes the lambda expression as an argument. In general, the fixed point
   operator can accept any expression than evaluates to a first-order function.
   Whilst not demonstrated here, functions can take more than one argument.
   # *TODO: Maybe demonstrate instead.*

** Type Systems for Nibble
   I have designed two type systems for Nibble: \ky{}-\lambda and LM. The
   \ky{}-\lambda type system is a minimal departure from the \ky{} type system
   for \mres{}, which was presented in section [[*The \ky{} Type System]], treating
   lambda abstractions as parametric macros. This is what is implemented in
   Chomp. The LM type system is a theoretical system for Nibble, incorporating
   ideas from the \hm{} type system, introduced in section [[*The
   \hm{} Type System]]. This adds polymorphic function types on top of
   the \ky{} type system.
   
*** The \ky{}-\lambda Type System
    The \ky{}-\lambda type system is a type system for Nibble using the \ky{}
    type system, presented in section [[*The \ky{} Type System]], as the core. By
    treating lambda abstractions as parametric macros, Nibble expressions are
    reduced into embedded \mres{} using substitution. This \mre{} is type
    checked using the \ky{} type system.

    For a Nibble expression ~e~, we denote the reduction of ~e~ as \( \llbracket
    \mathtt{e} \rrbracket \).
    # *TODO: Full definition is where?*
    To summarise, reduction performs call-by-name evaluation of Nibble
    expressions. The only exception is the fixed-point operator, ~!e~. This
    first reduces the argument ~e~. If ~e~ reduces to ~/x/ f~, then ~f~ is
    reduced, keeping ~x~ free. Otherwise, reduction fails.

    # *TODO: include examples*

    There are some problems with this approach. Firstly, call-by-name evaluation
    of untyped terms is non-terminating. Consider listing [[lst:omega]]. The
    expression ~omega omega~ reduces to ~omega omega~. Users might be confused
    by the parser generator hanging instead of producing an error.
   
    #+label: lst:omega
    #+name: lst:omega
    #+caption: An example Nibble expression that does not reduce. Reducing
    #+caption: ~omega omega~ produces ~omega omega~.
    #+begin_src nibble
      let omega x = x x;
      match omega omega;
    #+end_src

    Another issue is that unused expressions are completely ignored. Whilst this
    has some computational benefits, it could cause confusion for users. An
    example is in listing [[lst:kyl-ignores]]. Due to the evaluation strategy, even
    though ~"a" | "a"~ fails to type check, because it is bound to ~ill-typed~,
    which is never used, type checking succeeds. If someone referenced
    ~ill-typed~, type checking would unexpectedly fail.

    #+label: lst:kyl-ignores
    #+name: lst:kyl-ignores
    #+caption: The \ky{}-\lambda type system ignores expressions bound by unused
    #+caption: variables.
    #+begin_src nibble
      let ill-typed = "a" | "a";
      match "baa";
    #+end_src

    Both the non-termination problem and, to some extent, ignoring ill-typed
    unused terms can be solved with a full syntactic type system.

*** The LM Type System
    # *TODO: reread carefully and simplify. This whole section assumes prior
    # knowledge readers won't have.*
    
    The LM type system is a purely-syntactic type system for Nibble. It combines
    features of the \hm{} type system discussed in section [[*The
    \hm{} Type System]] with the core of the \ky{} type system, presented
    in section [[*The \ky{} Type System]]. This allows for expressions bound by let
    statements to have polymorphic types, and removes the need to reduce
    expressions before type checking.

    Unlike all previous type systems, the variable contexts store both types and
    constraints. Constraints are relations between types in the \ky{} type
    system. They need to be stored in the variable context because these
    relations are not always decidable for type variables. For example, whether
    \(\alpha \# \tau_c\) holds depends on what \alpha is.
    
    Another change in the LM typing system is the form of the judgement. We
    extend the \ky{} typing judgement by adding a set of constraints \(C\) to
    the conclusion. The judgement has the form \( \Gamma; \Delta \vdash
    \mathtt{e} : \tau ; C \), which can be read: under unguarded variable
    context \Gamma and guarded context \Delta, the Nibble expression ~e~ has the
    type \tau given the constraints in \(C\) hold. Thus, type checking an
    expression becomes a two-step process: infer a type; then check the
    constraints hold.

    Figure [[fig:lm-type-rules]] shows the typing rules for the LM type system. We
    first talk through the various rules, then show some example inferences.
    Looking at those examples may help with understanding these rules.

    #+label: fig:lm-type-rules
    #+name: fig:lm-type-rules
    #+caption: The LM type rules
    #+begin_figure
    \centering
    % *TODO: define bar and substitutions.*
    
    \begin{math}
    \begin{array}{ccc}
      \begin{prooftree}
        \infer0{\Gamma; \Delta \vdash \mathtt{\_} : K(\tau_\epsilon); \emptyset}
      \end{prooftree}
      &&
      \begin{prooftree}
        \infer0{\Gamma; \Delta \vdash \mathtt{``cw"} : K(\tau_c); \emptyset}
      \end{prooftree} \\
      && \\
      \begin{prooftree}
        \hypo{\sigma = S\rho}
        \hypo{C' = S C}
        \infer2{\Gamma, x : (\rho, C); \Delta \vdash \mathtt{x} : \sigma; C'}
      \end{prooftree}
      && \\
      && \\
      \begin{prooftree}
        \hypo{\Gamma; \Delta \vdash \mathtt{e} : K(\tau); C}
        \hypo{\Gamma, \Delta; \cdot \vdash \mathtt{e'} : K(\tau'); C'}
        \infer2{\Gamma; \Delta \vdash \mathtt{e . e'} : K(\tau \cdot \tau'); C \cup C' \cup \{ \tau \circledast \tau' \}}
      \end{prooftree}
      &&
      \begin{prooftree}
        \hypo{\Gamma; \Delta \vdash \mathtt{e} : K(\tau); C}
        \hypo{\Gamma; \Delta \vdash \mathtt{e'} : K(\tau'); C'}
        \infer2{\Gamma; \Delta \vdash \mathtt{e | e'} : K(\tau \vee \tau'); C \cup C' \cup \{ \tau \# \tau' \}}
      \end{prooftree} \\
      && \\
      \begin{prooftree}
        \hypo{\Gamma, x : (\sigma, \emptyset); \Delta \vdash \mathtt{e} : \sigma'; C}
        \infer1{\Gamma; \Delta \vdash \mathtt{/x/ e} : \sigma \to \sigma'; C}
      \end{prooftree}
      &&
      \begin{prooftree}
        \hypo{\Gamma; \Delta, x : (\sigma, \emptyset) \vdash \mathtt{e} : \sigma'; C}
        \infer1{\Gamma; \Delta \vdash \mathtt{/x/ e} : \sigma \leadsto \sigma'; C}
      \end{prooftree} \\
      && \\
      \begin{prooftree}
        \hypo{\Gamma; \Delta \vdash \mathtt{e} : \sigma \to \sigma'; C}
        \hypo{\Gamma; \Delta \vdash \mathtt{e'} : \sigma; C'}
        \infer2{\Gamma; \Delta \vdash \mathtt{e e'} : \sigma'; C \cup C'}
      \end{prooftree}
      &&
      \begin{prooftree}
        \hypo{\Gamma; \Delta \vdash \mathtt{e} : \sigma \leadsto \sigma'; C}
        \hypo{\Gamma, \Delta; \cdot \vdash \mathtt{e'} : \sigma; C'}
        \infer2{\Gamma; \Delta \vdash \mathtt{e e'} : \sigma'; C \cup C'}
      \end{prooftree} \\
      && \\
      \begin{prooftree}
        \hypo{\Gamma; \Delta \vdash \mathtt{e} : K(\tau) \leadsto K(\tau); C}
        \infer1{\Gamma; \Delta \vdash \mathtt{!e} : K(\tau); C}
      \end{prooftree}
      &&
      \begin{prooftree}
        \hypo{\Gamma; \Delta \vdash \mathtt{e} : \sigma; C}
        \hypo{\Gamma; \Delta, x: (\bar{\sigma}, \bar{C}) \vdash \mathtt{e'} : \sigma'; C'}
        \infer2{\Gamma; \Delta \vdash \mathtt{let x = e; e'} : \sigma'; C'}
      \end{prooftree} \\
      && \\
    \end{array}
    \end{math}
    #+end_figure

    The epsilon and literal rules are the simplest LM typing rules, being
    largely unchanged from the \ky{} type system. The only differences are that
    the type is wrapped in a \(K\) constructor, to distinguish \ky{} types from
    function types, and they both return an empty set of constraints.

    The variable rule is a combination of the rules from the \hm{} and
    \ky{} type systems. First, the variable must be unguarded. This is to
    prevent infinite recursion, like in the \ky{} type system. Second, the
    output type and constraints are a specialisation of the type and constraints
    from the context. This is like the \hm{} variable typing rule. We
    specialise the type constraints because they can also include type
    variables.

    Concatenation and alternation remain similar to the \ky{} type system. Like
    the epsilon and literal rules, all the types are wrapped in a \(K\)
    constructor. Instead of the constraints appearing in the premise, as they do
    in the \ky{} type system, they are moved to the conclusion. This is so they
    can be checked later when all type variables are instantiated.

    Notice how there are two different typing rules for lambda abstraction. This
    is due to the two variable contexts from the \ky{} type system. One function
    type is for unguarded functions, where the formal parameter can be used in
    an unguarded context, and the other function type is for guarded functions,
    where the formal parameter can only be used in guarded contexts. Because
    lambda expressions are monomorphic, type constraints pass straight through.

    Again due to the presence of two function types, there are two typing rules
    for application. If the called function is an unguarded function type, then
    the argument is evaluated in the same context. If the function is a guarded
    function, then the argument is evaluated in an unguarded context -- the
    function body ensures the parameter only appears on the right side of a
    concatenation, so all variables are accessible.

    # *TODO: add guard < unguard to the specialisation rules instead?*
    
    The subsumption rule allows guarded functions to be used when an unguarded
    function was expected. This is safe due to an extension of the transfer
    property of the \ky{} type system.
    # *(TODO: link to transfer)*. 

    The fixed-point typing rule only accepts first-order guarded functions as
    the argument. Whilst fixed-points could accept higher-order functions, doing
    so would allow non-terminating reductions in Nibble. To prevent unguarded
    recursion, which we saw in section [[*The \ky{} Type System]] lead to
    non-determinism, the formal parameter must be guarded.

    Let expressions are taken from the \hm{} type system almost directly. Note
    that bound variables are always unguarded. 
    # *TODO: Why?* 
    This is also the only typing rule that adds constraints to variables in the
    context. A consequence of this typing rule is that constraints \(C\) on the
    bound expression are only checked if ~x~ is used in the body.

**** Examples
     
     # *TODO: example for embedded \mre{}*
    
     # *TODO: review these examples.*
    
     We will now justify the need for two different function contexts. Consider
     the expression ~/x/ _ | x~, corresponding to an optional ~x~. This
     expression is an essential combinator for real-world languages. ~x~ appears
     unguarded in the expression ~_ | x~, so this lambda expression can have the
     type \(\forall \alpha. \tau_\epsilon \vee \alpha\).
    
     Now consider the expression ~!(/x/ "a" | "(".x.")")~. As a \mre{}, this
     would be represented as \( \mu x. a \vee (x) \). Recalling the rules from
     the \ky{} typing judgement, \(x\) would be introduced to the guarded
     context. Hence, ~x~ should be introduced to the guarded context too.
     Therefore, this expression has the type \( \mu \alpha. \tau_a \vee \tau_{(}
     \cdot \alpha \cdot \tau_{)} \).
    
     # *TODO: include examples, especially for function types*
     
     # *TODO: ~let bad x = "a" | "a"; match "b";~*
     
     # *TODO: subsumption rule*
       
#     Finally we consider type inference. Types in the \ky{} type system form an
#     algebra. For example, \( \alpha \cdot \tau_\epsilon = \alpha \) for all
#     types \( \alpha \). Because of this algebraic nature, it is difficult to
#     determine whether two types are equal. For instance, do we have \( ((\alpha
#     \vee \beta) \cdot \gamma) \cdot \delta = (\alpha \cdot (\gamma \cdot
#     \delta)) \vee ((\beta \cdot \tau_\epsilon) \cdot (\gamma \cdot \delta))
#     \)[fn:: Yes]? Introducing fixed points only makes determining type equality
#     more difficult.

#     Recall that unification takes two types and instantiates type variables
#     until they are equal. Given that equality is so complex, how can we unify
#     two variables efficiently? The solution is to only use structural equality
#     for unification. Whilst it will reject some otherwise well-typed Nibble
#     expressions, using structural equality should have a huge performance
#     benefit. In any case, for this small set of rejected expressions, there will
#     be a Nibble expression with an equivalent language.
** Chomp Repository Overview
   Chomp is a parser generator from Nibble to Rust. Chomp is implemented in
   Rust, so it can utilise the procedural macro system, described in section
   [[*Procedural Macros]]. Table [[tbl:overview]] gives a brief description of the
   repository structure for Chomp.

   #+label: tbl:overview
   #+name: tbl:overview
   #+caption: Brief outline of the code repository structure. All code is 
   #+caption: written in Rust.
   #+attr_latex: :float t
   | Directory              | Description                                  | Lines of Code |
   |------------------------+----------------------------------------------+---------------|
   | ~src/nibble~           | Nibble parser and normalisation              |           738 |
   | ~src/chomp~            | Chomp type inference algorithm               |          1786 |
   | ~src/lower~            | Chewed parser code generation                |           403 |
   | ~tests~                | Minimal end-to-end tests of Chomp            |            57 |
   | ~chewed~               | Shared library for all chewed parsers        |           270 |
   | ~chomp-macro~          | Procedural macro interface                   |            42 |
   | ~autonibble/src~       | AutoNibble implementation                    |           489 |
   | ~autonibble/tests~     | Tests for correctness of AutoNibble          |            35 |
   | ~autonibble/benches~   | Benchmarks of AutoNibble                     |            57 |
   | ~chomp-bench/**/json~  | Benchmarks of various parsers for JSON       |           489 |
   | ~chomp-bench/**/arith~ | Benchmarks of various parsers for arithmetic |           274 |

   The ~src~ directory contains the main Chomp library. It is split into three
   parts. The front end is in the ~src/nibble~ directory. This parses an input
   stream of Nibble expressions and produces Chomp's IR, an abstract syntax
   tree. The middle end is in the ~src/chomp~ directory. This performs type
   inference using the \ky{}-\lambda type system. It outputs a typed \mre{}. The
   back end is in the ~src/lower~ directory. Its responsible for converting the
   typed \mre{} into Rust source code for a parser.
   
   The ~tests~ directory contains the code to run a number of end-to-end tests
   for Chomp, making sure certain examples type check correctly. Correctness of
   the produced parsers is left to benchmarking and other test code.
   
   The output of Chomp is called a /chewed parser/. All these parsers share some
   common code, such as the trait definition (introduced in section [[*Traits]]) for
   a parser. This common code is kept in the ~chewed~ library. The reason it
   forms a separate library is so that consumers of Chomp only need to include
   the small ~chewed~ library with their final binary, instead of the relatively
   large Chomp library.
   
   To help make Chomp easier for developers to include in their projects, a
   procedural macro interface was created. Due to current limitations in Rust,
   this interface has its own, small library in the ~chomp-macro~ directory.
   
   The success criterion for this project required bootstrapping the Nibble
   language -- using a Chomp-generated parser to parse the Nibble langauge. This
   parser, dubbed /AutoNibble/, is in the ~autonibble~ directory. This directory
   includes some simple tests of the correctness of AutoNibble, as well as some
   benchmarks to compare its performance against the handwritten parser.
   
   Finally, there is the ~chomp-bench~ directory. This is a small library for
   comparing the performance of chewed parsers against a handwritten and a
   different generated parser. Ideally, this library would be part of
   ~chomp-macro~. However, limitations in the build system for the other
   generated parser makes this impossible.
** The Front-End: Parsing and Normalisation
   The front-end of Chomp is responsible for converting the input stream of
   characters representing a Nibble expression into Chomp's IR. This occurs in
   three stages. The first is lexing, where characters in the input stream are
   split into different tokens. The second is parsing, where this token stream
   is transformed into a concrete syntax tree. Finally, normalisation converts
   this concrete syntax tree into an abstract syntax tree.
   
   In Chomp, the lexing is performed by the ~syn~ library. This can convert
   streams of characters into tokens from the Rust language. Nibble uses a
   subset of the tokens found in Rust, so lexing into Rust tokens makes the
   parser simpler. It also makes integration with procedural macros
   significantly easier, because procedural macros receive a stream of Rust
   tokens as input. 
   # *TODO: link back to procedural macros?*
   
   The parser in Chomp also uses the ~syn~ library. It provides lightweight
   interface to parse some often-used data structures. For example, it provides
   the ~Punctuated<T, P>~ type, which represents a list of values of type ~T~,
   separated by values of type ~P~. 
   # *TODO: discuss more interesting parsing.*
   
   Listing [[lst:parse-term]] shows how Chomp parses a Nibble term. This function
   makes use of Rust's type inference and trait systems to call one of four
   different functions, all written as ~input.parse()~. By checking what the
   next input token is, it is possible to determine exactly what type of term
   can appear.
   
   #+label: lst:parse-term
   #+name: lst:parse-term
   #+caption: Rust code snippet that parses a Nibble term.
   #+begin_src rust
     pub enum Term {
         Epsilon(Epsilon),
         Ident(Ident),
         Literal(Literal),
         Fix(Fix),
         Parens(ParenExpression),
     }

     impl Parse for Term {
         fn parse(input: ParseStream<'_>) -> syn::Result<Self> {
             let lookahead = input.lookahead1();

             if lookahead.peek(Token![_]) {
                 input.parse().map(Self::Epsilon)
             } else if lookahead.peek(LitStr) {
                 input.parse().map(Self::Literal)
             } else if lookahead.peek(Token![!]) {
                 input.parse().map(Self::Fix)
             } else if lookahead.peek(Paren) {
                 input.parse().map(Self::Parens)
             } else if lookahead.peek(Ident::peek_any) {
                 input.call(Ident::parse_any).map(Self::Ident)
             } else {
                 Err(lookahead.error())
             }
         }
     }
   #+end_src
   
   Finally there is normalisation. This converts the concrete syntax tree into
   an abstract syntax tree. This occurs in two stages that occur simultaneously.
   First, syntactic sugar is expanded. Second, variable names are converted to
   De Bruijn indices (introduced in section [[*De Bruijn Indices]]).
   
   In section [[*The Nibble Language]], we introduced let-lambda statements as
   syntactic sugar for a let statement binding a lambda expression. During
   normalisation, we convert let-lambda statements into this expanded form,
   instead of keeping them around as another node type in the abstract syntax
   tree. This reduces complexity in later stages of Chomp.
   
   The other part of normalisation is conversion to De Bruijn indices. Most of
   this work is achieved by the ~Context~ struct, shown in listing
   [[lst:parse-ctx]].
   
   #+label: lst:parse-ctx
   #+name: lst:parse-ctx
   #+caption: Rust code snippet that provides conversion to De Bruijn indices.
   #+begin_src rust
     pub struct Context {
         bindings: Vec<Name>,
     }

     impl Context {
         /// Get the De Bruijn index of `name`, if it is defined.
         pub fn lookup(&self, name: &Name) -> Option<usize> {
             self.bindings
                 .iter()
                 .rev()
                 .enumerate()
                 .find(|(_, n)| *n == name)
                 .map(|(idx, _)| idx)
         }

         /// Permanently add the variable `name` to the top of the stack.
         pub fn push_variable(&mut self, name: Name) {
             self.bindings.push(name);
         }

         /// Call `f` with the variable `name` pushed to the top of the stack.
         pub fn with_variable<F: FnOnce(&mut Self) -> R, R>(
             &mut self, 
             name: Name, 
             f: F,
         ) -> R {
             self.bindings.push(name);
             let res = f(self);
             self.bindings.pop();
             res
         }
     }
   #+end_src
   
   As discussed in section [[*De Bruijn Indices]], when we introduced De Bruijn
   indices, variables in lexically-scoped languages form a stack. The
   most-recently declared variable is at the top of the stack. New variables are
   pushed on top of the stack, and popped off when they leave scope.
   
   There are two ways to introduce variables in Nibble. The binding variables
   from let statements are in scope for the rest of the Nibble expression. The
   formal parameters of lambda expressions are in scope only for the body of the
   lambda expression. This is reflected by the two different ways to push
   variables to the stack.
   
   ~push_variable~ adds a variable onto the stack of ~bindings~ in a ~Context~
   permanently. The API provides no way to remove variables. This is called
   by let statements, after converting the bound expression but before
   converting their body. The ~with_variable~ method pushes a variable onto the
   stack only for the duration of a call to the function ~f~. This is used by
   lambda expressions, where ~f~ will convert the lambda body.
   
   The ~lookup~ method does all of the heavy-lifting. It numbers each member of
   the stack starting from the top. Then still from the top, it returns the
   index of the first item with a matching name. If no such item exists, then
   an error is returned.

** The Middle-End: Type Inference
   The middle end of Chomp performs type inference using the \ky{}-\lambda type
   system, to produce a typed \mre{}. First, the abstract syntax tree computed
   by the front-end is reduced. Next, its type is inferred using the \ky{} type
   system, and the output expression is built. First, we discuss the visitor
   design pattern used to implement both parts.
   
*** The Visitor Design Pattern
    Both reduction and the type inference algorithm use the visitor design
    pattern. This design pattern separates algorithms from the data structures
    they operate on. Like Rust's traits, discussed in section [[*Traits]], the
    visitor design pattern follows the open/closed principle -- the data
    structure is closed for modification, but the design pattern makes it open
    for new algorithms.
    
    Figure [[fig:visitor-uml]] shows a UML diagram for the visitor design pattern.
    The ~Visitor~ interface requires implementors to handle each type of object
    in the ~Visitable~ data type. The ~Visitable~ data type then only needs one
    generic dispatch method to implement all the algorithms that use this
    pattern. 
    
    #+name: src:visitor-uml
    #+begin_src dot :file ./images/uml.png :cache yes
      digraph {
        nodesep = 1.5;

        node [
          shape = record;
          width = 2;
        ];
        edge [
          dir = back;
        ];

        expression [ 
          label = "{\<\<enumeration\>\>\nVisitable|Variant1\lVariant2\lVariant3\l...}";
        ];
        visitor [
          label = "{\<\<interface\>\>\nVisitor|+ visit_one(Variant1)\l+ visit_two(Variant2)\l+ visit_three(Variant3)\l...}";
        ];
        reduction    [ label = "{VisitorA}" ];
        substitution [ label = "{VisitorB}" ];

        expression -> visitor   [ style = dashed; constraint = false ];
        visitor -> reduction    [ arrowtail = onormal ];
        visitor -> substitution [ arrowtail = onormal ];
      }
    #+end_src
    
    #+label: fig:visitor-uml
    #+name: fig:visitor-uml
    #+caption: A UML diagram depicting the visitor design pattern.
    #+results: src:visitor-uml
    [[./images/uml.png]]
    
    Listing [[lst:visitor-defs]] show how this pattern is implemented in Chomp. The
    ~Visitor~ trait includes a function signature for each type of
    abstract-syntax-tree node. ~NamedExpression~ then uses pattern matching to
    dispatch the appropriate method.
    
    #+label: lst:visitor-defs
    #+name: lst:visitor-defs
    #+caption: Rust code snippet showing the implementation of the visitor 
    #+caption: design pattern.
    #+begin_src rust
      pub type NameSpan<'a> = (Option<&'a Name>, Option<Span>);
      pub trait Visitor {
          type Out;
          fn visit_epsilon(&mut self, namespan: NameSpan, eps: &Epsilon) -> Self::Out;
          fn visit_literal(&mut self, namespan: NameSpan, lit: &Literal) -> Self::Out;
          fn visit_cat(&mut self, namespan: NameSpan, cat: &Cat) -> Self::Out;
          fn visit_alt(&mut self, namespan: NameSpan, alt: &Alt) -> Self::Out;
          fn visit_fix(&mut self, namespan: NameSpan, fix: &Fix) -> Self::Out;
          fn visit_var(&mut self, namespan: NameSpan, var: &Variable) -> Self::Out;
          fn visit_call(&mut self, namespan: NameSpan, call: &Call) -> Self::Out;
          fn visit_lambda(&mut self, namespan: NameSpan, lambda: &Lambda) -> Self::Out;
          fn visit_let(&mut self, namespan: NameSpan, stmt: &Let) -> Self::Out;
      }

      impl NamedExpression {
          pub fn visit<V : Visitor>(&self, visitor: &mut V) -> <V as Visitor>::Out {
              let namespan = (self.name.as_ref(), self.span);
              match &self.expr {
                  Self::Epsilon(e) => visitor.visit_epsilon(namespan, e),
                  Self::Literal(l) => visitor.visit_literal(namespan, l),
                  Self::Cat(c) => visitor.visit_cat(namespan, c),
                  Self::Alt(a) => visitor.visit_alt(namespan, a),
                  Self::Fix(f) => visitor.visit_fix(namespan, f),
                  Self::Variable(v) => visitor.visit_variable(namespan, v),
                  Self::Call(c) => visitor.visit_call(namespan, c),
                  Self::Lambda(l) => visitor.visit_lambda(namespan, l),
                  Self::Let(l) => visitor.visit_let(namespan, l),
              }
          }
      }
    #+end_src
    
    # *TODO: check this makes sense*
    
    An alternative to the visitor pattern in Rust is to define algorithms as
    traits directly on ~NamedExpression~. However, this would require making
    ~NamedExpression~ more open, so that it can be fully unwrapped. This would
    make changing the representation of ~NamedExpression~ more difficult in
    future.
*** Reduction
    The first step in using the \ky{}-\lambda type system is reduction of the
    expression. Recall from section [[*The \ky{}-\lambda Type System]] how reduction
    of Nibble is essentially call-by-name evaluation. This is implemented using
    a number of visitors.
    
    The outer-most visitor is the ~Reduce~ visitor. This is what drives the
    reduction. It searches for let expressions and application expressions to
    reduce, without stepping into lambda expressions. For the let and
    application expressions, it performs the appropriate substitution and then
    reduces the result.
    
    # *TODO: interesting snippet*
    
    The substitution is performed by another visitor, ~Substitute~. Given an
    expression and a De Bruijn index, it searches for uses of that index and
    replaces them with the expression. In section [[*De Bruijn Indices]], we
    noted that indices of free variables change inside of lambda bodies. The
    target index is shifted inside of lambda bodies, and free variables in the
    substitutand 
# (*TODO: not a word*) 
    need to be renamed.
    
    # *TODO: interesting snippet*
    
    The final visitor for reduction changes the De Bruijn indices of free
    variables in an expression.
    
    # *TODO: interesting snippet*
   
*** Inference
    Type inference is the second part of the \ky{}-\lambda type system. This
    uses the typing rules of the \ky{} type system to infer the type of
    expressions. It returns a \mre{} annotated with types.

    Recall how Nibble has some constructs that are not in \mres{}, namely let
    statements, application expressions and lambda expressions. The reduction in
    the previous section eliminates all let statements and application
    expressions. However, it does not eliminate lone lambda expressions.
    Therefore, if the type inference algorithm reaches a lambda expression, it
    fails.
    
    We now look at type inference for the other Nibble expressions, referring
    back to the \ky{} typing rules from figure [[fig:ky-rules]]. Inference for
    epsilon expressions is trivial -- always the type \tau_\epsilon. By
    combining the concatenation rule with the character rule, we find that a
    literal expression ~"cw"~ always has type \tau_c.
    
    By implementing types using ~BTreeSet~ from the Rust standard library to
    store the first and flast sets, alternation becomes simple. First, we infer
    the type of each sub-expression recursively. Next, to check that the \(\#\)
    constraint holds, we use the ~intersection~ function from the standard
    library. Finally, we use the ~append~ function to compute the new type.
    
    # *TODO: maybe snippet*
    
    Concatenation is nearly identical to alternation. There are two differences.
    First, the constraint is slightly different, although the checks are almost
    identical. The bigger difference is that the suffix of a concatenation needs
    to use an unguarded context for type inference. Listing [[lst:type-context]]
    shows the type context used to make this possible.
    
    #+label: lst:type-context
    #+name: lst:type-context
    #+caption: Rust code snippet showing the type context used by type 
    #+caption: inference.
    #+begin_src rust
      pub struct Context {
          vars: Vec<Type>,
          unguard_points: Vec<usize>,
      }

      impl Context {
          pub fn with_unguard<F: FnOnce(&mut Self) -> R, R>(&mut self, f: F) -> R {
              self.unguard_points.push(self.vars.len());
              let res = f(self);
              self.unguard_points.pop();
              res
          }

          pub fn get_variable_type(
              &self, 
              var: Variable,
          ) -> Result<&Type, GetVariableError> {
              self.vars
                  .iter()
                  .nth_back(var.index)
                  .ok_or(GetVariableError::FreeVariable)
                  .and_then(|ty| {
                      if self.unguard_points.last().unwrap_or(&0) + var.index
                          >= self.vars.len()
                      {
                          Ok(ty)
                      } else {
                          Err(GetVariableError::GuardedVariable)
                      }
                  })
          }

          pub fn with_variable_type<F: FnOnce(&mut Self) -> R, R>(
              &mut self, 
              ty: Type, 
              f: F,
          ) -> R {
              self.vars.push(ty);
              let res = f(self);
              self.vars.pop();
              res
          }
      }
    #+end_src
    
    The design of this type context is similar to the naming context used in
    the front end, shown in listing [[lst:parse-ctx]]. There are two ~with~ methods:
    ~with_variable_type~ for introducing new variables; and ~with_unguard~ for
    moving the guarded context into the unguarded one. Like the previous
    example, most of the work is performed by the ~get_variable_type~ method. 
    
    As discussed in section [[*De Bruijn Indices]] when we introduced De Bruijn
    indices, variables form a stack, ~vars~. The De Bruijn index is the position
    of the variable from the top of the stack. For most type systems, this
    completes the lookup function. However, the \ky type system has both a
    guarded and unguarded context. Which variables are unguarded is kept track
    of by ~unguard_points~, which stores the total number of variables that are
    unguarded. A simple arithmetic check then determines whether a variable is
    unguarded.
        
    Finally, we have fixed-point expressions and variables. Variables are a
    simple lookup in the type context. We find the type for fixed-point
    expressions using iteration. Initially, we assume the type is \(\tau_\bot\).
    Then we add this guess to the variable context and infer the type of the
    fixed-point body to find another approximation. We repeat this until the
    initial guess and next inferred types are equal, or there is a type error.
    This is shown listing [[lst:infer-fix]], which shows the type inference process.
    
    #+label: lst:infer-fix
    #+name: lst:infer-fix
    #+caption: Rust code snippet providing type inference for fixed-point 
    #+caption: expressions.
    #+begin_src rust
      fn fold_fix(&mut self, fix: Fix) -> Result<_, _> {
          let mut ty = Type::default();
          let inner = fix.inner.try_into_lambda()?.get_body();

          loop {
              // ? at end exits function if there is a type error.
              let res = self.context.with_variable_type(ty.clone(), |context| {
                  inner.clone().fold(&mut TypeInfer { context })
              })?;

              let next = res.get_type();

              if next == ty {
                  return Ok(/* ... */);
              }

              ty = next.clone();
          }
      }
    #+end_src

** The Back-End: Code Generation
   Code generation is the final step in Chomp. The typed \mre{} computed by the
   middle-end is converted into Rust code for both data types and parser
   implementations. A separate library called ~chewed~ contains the definitions
   of some data types and the parsing trait used by all chewed parsers.
   
   The substitutions performed while reducing the original Nibble expression
   results in a large amount of duplication of sub-expressions. In an attempt to
   reduce the amount of generated code, and to make integrating a chewed parser
   into a project easier, the back-end attempts to eliminate work generating
   code for duplicate expressions using interning.
   
   # *TODO: check wording here.*

   This interning occurs in three phases. First, each \mre{} is mapped to a
   handle. This is done structurally, such that two identical \mres{} receive
   the same handle. Next, the set of all handles that can be reached from the
   top-level \mre{} handle are computed. Finally, code is generated only for
   the \mres{} that were reached. In practice, the first and last phases are
   computed at once, and the second step performs a collation of results. 
   # *TODO: Why?*
   
   Mapping expressions to handles uses many caches, one for each type of
   expression. First, the handles of all sub-expressions are found. Next, the
   cache corresponding to the expression type is checked, using the
   sub-expression handles as keys. If the cache does not contain a handle for a
   key, a new one is generated.
   
   Finding the set of reachable handles uses well-known graph algorithms.
   
   The generated code takes the form of a stream of Rust source tokens. Creating
   such streams by hand would be tedious and error prone, given the complexity
   of parts of the generated code. Instead, we use a procedural macro from the
   ~quote~ library to let us write the output token stream as regular Rust code.
   The procedural macro, which are described more generally in section
   [[*Procedural Macros]], converts the Rust code into a token stream describing it.

   The form of generated code depends on the \mre{}. \epsilon is translated to
   the ~Epsilon~ type in the ~chewed~ library, on the assumption that almost all
   Nibble expressions will include an epsilon expression.
   
   # *TODO: clunky as heck.*
   
   Literals are each translated to a unique unit struct. By using unique types,
   no data needs to be stored with the type to determine what literal it is.
   This means that the Rust compiler can eliminate these literal types, keeping
   only the side effects of their computation.
   
   # *TODO: bold claim in first line.*
   
   Concatenations are quite simple in terms of code generation. Each
   concatenation is translated into a struct, where each sub-expression is a
   different field. To parse the concatenation, each field is parsed in turn.
   
   The most challenging expression form to translate into Rust code is
   alternation. Alternations are represented by enumerations, where each
   constructor corresponds to a different alternative. When parsing an
   alternation, which alternative to try and parse depends on the next character
   of input and the first sets of each alternative. Example output code is shown
   in listing [[lst:gen-alt]].
   
   #+label: lst:gen-alt
   #+name: lst:gen-alt
   #+caption: Chomp-generated Rust code for parsing an alternation.
   #+begin_src rust
     // Parser for `match (_|"a")|("b"|"B")|"c";`
     enum Ast {
         Branch1(/*...*/); //   _|"a"
         Branch2(/*...*/); // "b"|"B"
         Branch3(/*...*/); // "c"
     }
     impl Parse for Ast {
         fn take<P: Parser + ?Sized>(input: &mut P) -> Result<Self, TakeError> {
             match input.peek() {
                 Some('a')   => Ok(Self::Branch1(input.take()?)),
                 Some('b')
                 | Some('B') => Ok(Self::Branch2(input.take()?)),
                 Some('c')   => Ok(Self::Branch3(input.take()?)),
                 // Because `_|"a"` contains the empty string,
                 // we can always take a member of `_|"a"`.
                 _           => Ok(Self::Branch1(input.take()?))
             }
         }
     }
   #+end_src
   
   Finally there are fixed-point expressions and variables. Fixed points are
   implemented as type aliases. Variables then refer to the type of the
   declaring fixed point.
   
#     Ideally, the translation step would be unnecessary. Instead, Rust's generics
#     would allow parametric data types and parser implementations. There are two
#     practical problems with this. First, generic type arguments cannot be given
#     their own type arguments. Consider the function ~/f/ f "a"~. Ideally, this
#     would have the datatype declaration ~type Foo<F> = F<A>;~. Unfortunately,
#     the Rust compiler rejects the type argument ~<A>~ on ~F~. Secondly, generic
#     type arguments cannot provide compile-time constants to the generic type.
#     Consider the alternation ~"a"|"b"~. To get the full performance benefit of
#     type-checked parser combinators, the branching conditions, i.e. the first
#     sets, for each alternative need to be known at compile time. For a generic
#     implementation ~Alt<A, B>~, there is no way to get this information as a
#     compile-time constant.
   
# ** Chomp as a Procedural Macro
   # *TODO: fill in this section _or_ delete it.*
   
** Summary
   In section [[*The Nibble Language]] we introduced Nibble. Nibble is a DSL
   designed to fix the problems of \mres{}. We then introduced two type systems
   for Nibble -- the \ky{}-\lambda type system and the LM type system. 

   The \ky{}-\lambda type system, introduced in section [[*The \ky{}-\lambda Type
   System]] treats Nibble constructs as parametric macros for \mres{}, before
   deferring to the \ky{} type system. This causes type checking to be
   non-terminating, and well-typed Nibble to contain some ill-typed fragments.

   In section [[*The LM Type System]], we introduced the LM type system to solve
   these issues. By combining the \hm{} type system with the \ky{} type
   system, we gave lambda expressions in Nibble polymorphic types. To account
   for type variables, the constraints from the \ky{} type system were moved
   from the premise to the conclusion of the typing rules, to be solved
   separately. 
   # *TODO: Where do I discuss why it's not used in Chomp?*
   
   We then started discussing Chomp, a parser generator using the \ky{}-\lambda
   type system, in section [[*Chomp Repository Overview]]. Chomp is both implemented
   in and targets Rust to exploit traits and procedural macros, both introduced
   in section [[*Rust]]. Like most translators, Chomp is split into three "ends".
   
   Section [[*The Front-End: Parsing and Normalisation]] details the front-end of
   Chomp. This takes a concrete stream of characters and produces an abstract
   syntax tree of Nibble. An external lexer converts the character stream to a
   sequence of Rust tokens. A parser then converts the Rust tokens into a
   concrete syntax tree. The normalisation step then expands syntactic sugar and
   introduces De Bruijn indices, discussed in section [[*De Bruijn Indices]].
   
   The middle-end is described in section [[*The Middle-End: Type Inference]]. This
   is responsible for type inference using the \ky{}-\lambda type system. First,
   the Nibble expression represented by the abstract syntax tree is reduced to
   an embedded \mre{}. Next, the \ky{} type system is used to infer the type of
   this \mre{}. The output is an \mre{} where each sub-expression is annotated
   with a type.
   
   Finally, the back-end, outlined in section [[*The Back-End: Code Generation]],
   performs code generation. The type-annotated \mre{} is expanded into a
   sequence of Rust tokens describing a parser for its langauge. This process
   utilises interning to eliminate the replication of expressions introduced by
   the earlier reduction.

* Evaluation
  We now evaluate whether the implementation has fulfilled the goals of the
  project. In section [[*Meeting the Success Criterion]] we demonstrate the success
  criterion was satisfied. Following this, we compare the benefits of using
  Nibble to BNF in section [[*Comparing Nibble and BNF]]. Section [[*Integrating Chomp]]
  contrasts integrating Chomp into a project with integrating ~lalrpop~, a
  parser generator for Rust using BNF-inspired syntax. We pay particular
  attention to the impact of the \ky{} type system on Nibble. Finally, in
  section [[*Performance of Chewed Parsers]] we analyse the results of benchmarks to
  determine the run-time cost of using chewed parsers.
  
** Meeting the Success Criterion
   The success criterion required that the Chomp parser generator "can generate
   a parser implementation for [the Nibble language] that produces identical
   output to the hand-written Nibble parser". We start by considering whether I
   have produced the requisite deliverables, then move on to checking I have
   fulfilled the success criterion.
   
   We described the Nibble language in section [[*The Nibble Language]], which is a
   new DSL for describing formal languages. By embedding \mres{} in Nibble,
   Nibble provides a new way of describing \mres{}. The introduction of let
   statements and lambda expressions shifts Nibble away from its mathematical
   roots in \mres{} towards a more programmatic style.
   
   Whilst not explicitly stated in the proposal, it was required to design an
   appropriate type system for Nibble that was compatible with the \ky{} type
   system. I have achieved this in two different ways. The \ky{}-\lambda type
   system described in section [[*The \ky{}-\lambda Type System]] describes the
   minimal changes to the \ky{} type system necessary for use with Nibble.
   Section [[*The LM Type System]] describes the LM type system, which is a fresh
   type system directly on Nibble compatible with the \ky{} type system.
   
   Another core requirement was the creation of Chomp -- a parser generator
   taking Nibble as input. Chapter [[*Implementation]] discusses the implementation
   of Chomp at great length, including the three parts described in the original
   implementation.
   
   # *TODO: rest of section may be too detailed?*
   
   The rest of this section is concerned with the bootstrapping of Chomp
   required to satisfy the success criterion. All of this code is in the
   ~autonibble~ directory of the repository. 
   
   First, Nibble is sufficiently expressive to be able to describe itself. This
   is demonstrated in the ~autonibble/src/lib.rs~ file. This is a significant
   result -- Nibble syntax is similar to that of many programming languages used
   in industry. 
   # *TODO: Why do I care?*
   
   Secondly, Chomp can process this self-description and produce a chewed
   parser. This is achieved by the ~nibble!~ procedural macro, which is a small
   wrapper around the Chomp parser generator. The resulting Chomp-generated
   parser is called AutoNibble.
   # This is achieved by using the ~nibble!~ procedural macro, detailed in
   # section [[*Chomp as a Procedural Macro]]. 
   
   The rest of the code in ~autonibble/src/lib.rs~ is there to make AutoNibble a
   complete front-end for Chomp. It performs the same normalisation process on
   AutoNibble's concrete syntax tree as Chomp does on its own concrete syntax
   tree.
   
   Finally, we directly compare the outputs of the full AutoNibble front-end and
   Chomp's front-end. This is performed on the test cases present in the
   ~autonibble/tests/compare/~ directory. 
   # When writing the proposal, this did not
   # appear to be possible, hence why it compares Chomp against AutoChomp.
   # However, the highly modular architecture used in Chomp makes such comparisons
   # simple.
   
** Comparing Nibble and BNF
   Untyped Nibble and BNF, introduced in section [[*BNF]], are both DSLs for
   describing formal languages. By comparing the features and ergonomics of
   Nibble and BNF, we can conclude whether the Nibble language has the potential
   to join BNF as an effective way to describe formal languages.
   
   First, we explore the classes of formal languages that BNF and Nibble can
   describe. As stated in section [[*BNF]], BNF can describe all context-free
   languages. Nibble likely also only accepts context-free languages, despite
   reduction being able to perform arbitrary computation. 
   # *TODO: prove.*
   
   A fundamental principle of programming is DRY -- don't repeat yourself.
   Duplicating code makes maintenance more difficult. A conscious effort has to
   be made to keep all the duplicates in sync, and any bug found in one
   duplicate will be present in all others. Nibble avoids replication using let
   statements for verbatim copies, and lambda expressions for parametric copies.
   
   In contrast, whilst a new top-level form in BNF can eliminate verbatim and
   some cases of parametric repetition, other forms of parametric replication
   cannot be removed. Figure [[fig:dry-example]] shows how Nibble can remove
   replication of list definitions that cannot be removed from BNF. Whilst the
   BNF forms ~<xs>~ and ~<ys>~ have the same shape, there is no way to abstract
   that out. On the other hand, the Nibble variable ~list~ provides a parametric
   list definition. 
   
   #+label: fig:dry-example
   #+name: fig:dry-example
   #+caption: Comparison of BNF and the Nibble language, demonstrating that 
   #+caption: Nibble can abstract away patterns such as lists.
   #+begin_figure
     # *TODO: side-by-side*
     \begin{minted}{bnf}
       <xs>    ::= "x" | "x" <xs>
       <ys>    ::= "y" | "y" <ys>
       <start> ::= <xs> <ys>
     \end{minted}
   
     \begin{minted}{nibble.py -x}
       let list inner = !(/list/ inner . (_ | list));
       match list "x" . list "y";
     \end{minted}
   #+end_figure
   
   Another difference between BNF and Nibble is the variable scope rules. The
   mutually-recursive global scope of BNF allows other forms to be referenced
   without qualification from anywhere. Nibble uses much more restrictive
   non-recursive lexical scoping rules -- only variables defined previously can
   be used. This is shown in figure [[fig:scope-example]]. In BNF, the ~<term>~ form
   can directly refer to the ~<expr>~ form, even though it is defined later.
   Conversely, Nibble requires ~term~ to be a function with ~expr~ as a
   parameter. Also note that Nibble makes recursion explicit using the fixed
   point, compared to BNF's implicit recursion.
   
   #+label: fig:scope-example
   #+name: fig:scope-example
   #+caption: Comparison of BNF and the Nibble language demonstrating the 
   #+caption: difference in variable scope rules.
   #+begin_figure
     # *TODO: side-by-side*
    \begin{minted}{bnf}
       <term> ::= <number> | "(" <expr> ")"
       <expr> ::= <term>   | <term> + <expr>
     \end{minted}
     
     \begin{minted}{nibble.py -x}
       let term expr = number | "(" . expr . ")";
       let expr      = !(/expr/ term expr . (_ | ("+" | "-") . expr));
     \end{minted}
   #+end_figure
   
   # *TODO: is this good or bad?*
   
#     Nibble is as expressive as standard BNF. As discussed earlier, surjective
#     translation means that Nibble expressions describe the same set of languages
#     as \mres{}. As stated in section [[*Parser Combinators]], Leiß cite:null found
#     that \mres{} describe all context-free languages. BNF also describes all
#     context-free languages, so Nibble is at least as expressive as BNF.

#     Untyped Nibble is a more practical description of languages than BNF. Nibble
#     is less repetitive, more descriptive and has a lower cognitive load. BNF can
#     only have alternatives at the top level. This means that the BNF declaration
#     for the Nibble expression ~match x.("b"|"c").x~ would have to double the
#     number of occurrences of ~x~. BNF also has no equivalent for the lambda
#     expressions found in Nibble. Going back to (*TODO: figure*), whilst Nibble
#     can invoke the ~list~ expression twice, BNF has to fully expand it twice.

#     *TODO: descriptive.*

#     BNF uses a single mutually-recursive namespace. This is demonstrated in
#     listing [[lst:bnf-mut-rec]]. *TODO: Example.* This means that when a programmer finds
#     a BNF non-terminal, its declaration could be anywhere in the file. By
#     contrast, Nibble uses multiple nested lexical scopes. All variables are
#     declared earlier in the expression, either from a let expression or lambda
#     expression. *TODO: How does this help?*
    
#     #+label: lst:bnf-mut-rec
#     #+name: lst:bnf-mut-rec
#     #+caption: Demonstration of BNF's single mutually-recursive namespace
#     #+begin_src text
#       TODO: finish source block
#     #+end_src
   
** Integrating Chomp
   Whilst Nibble is interesting in its own right, most of its use in practice
   depends on Chomp. Here, we compare Chomp and \ky{}-\lambda-typed Nibble
   against ~lalrpop~, a parser generator for Rust using BNF-inspired syntax. We
   first discuss the language classes accepted by both \ky{}-typed Nibble and
   ~lalrpop~, and the consequences on the form of descriptions. Then we look at
   how to integrate both Chomp and ~lalrpop~ into a project build system.
   
   # *TODO: ~lalrpop~ parses LR languages. \ky-typed Nibble parses LL languages.
   # How do I explain this without defining LL or LR?*
   
   One practical example of this differences in allowed language descriptions is
   shown in figure [[fig:compare-number]]. This shows how to parse an
   optionally-signed number in both ~lalrpop~ and \ky{}-\lambda-typed Nibble.
   ~lalrpop~ has no problems with having the optional sign as a prefix to the
   number. On the other hand, the \ky{}-\lambda typing rules use the
   \(\circledast\) constraint to prevent the prefix of a concatenation from
   being \(\textsc{Null}\). This means we have to use ~number~ twice -- once for
   unsigned numbers, and another for signed numbers.
   
   #+label: fig:compare-number
   #+name: fig:compare-number
   #+caption: Comparison of ~lalrpop~ and the Chomp parser generator
   #+caption: demonstrating  that ~lalrpop~ can accept optional prefixes before 
   #+caption: a string.
   #+begin_figure
   # *TODO: side-by-side*
   
   \begin{minted}{rust}
     Sign : bool = {
         "+" => true,
         "-" => false,
     }

     SignedNumber = <s : Sign?> <n : Number>;
   \end{minted}
   
   \begin{minted}{nibble.py -x}
     let signed_number = number | ("+"|"-") . number;
   \end{minted}
   #+end_figure
   
   Another major difference between ~lalrpop~ and \ky{}-\lambda-typed Nibble is
   that the \ky{}-\lambda type system only accepts left-factored expressions.
   Figure [[fig:left-factor]] shows how ~lalrpop~ and \ky{}-\lambda-typed Nibble
   would describe the small language consisting of the three strings "bat",
   "band" and "brook". In ~lalrpop~, all three strings appear as literals.
   However, \ky-\lambda-typed Nibble factors out the common prefixes.
  
   #+label: fig:left-factor
   #+name: fig:left-factor
   #+caption: Comparison of ~lalrpop~ and the Chomp parser generator that 
   #+caption: demonstrates that ~lalrpop~ does not need left-factoring.
   #+begin_figure
   # *TODO: side-by-side*
   
   \begin{minted}{rust}
     Language = {
         "bat",
         "band",
         "brook",
     }
   \end{minted}
   
   \begin{minted}{nibble.py -x}
     match "b" . ("a" . ("t" | "nd") | "rook");
   \end{minted}
   #+end_figure
   
   The Nibble has to be left-factored to satisfy the \(\#\) constraint -- each
   alternative must have a disjoint \(\textsc{First}\) set. Consider listing
   [[lst:unfactored]], which shows the output parser implementation for the
   unfactored expression. Each alternative has the same condition of
   ~Some('b')~. As there is no backtracking, parsing this unfactored expression
   would not perform as expected.
   
   #+label: lst:unfactored
   #+name: lst:unfactored
   #+caption: Chomp-generated Rust code assuming left-factoring was unnecessary. 
   #+caption: Notice how the conditions for three of the branches are all 
   #+caption: ~Some('b')~.
   #+begin_src rust
     impl Parse for Ast {
         fn take<P: Parser + ?Sized>(input: &mut P) -> Result<Self, TakeError> {
             match input.peek() {
                 // "bat"
                 Some('b') => Ok(Self::Bat(input.take()?),
                 // "band"
                 Some('b') => Ok(Self::Band(input.take()?),
                 // "brook"
                 Some('b') => Ok(Self::Brook(input.take()?),
                 // Errors
                 None      => Err(TakeError::EndOfInput(/*...*/)
                 Some(c)   => Err(TakeError::BadBranch(/*...*/)),
             }
         }
     }
   #+end_src
   
   Left factoring causes many problems. Firstly, the resulting expression is
   more difficult to read. This hinders developers trying to understand what
   language the expression is describe. Secondly, searching for a literal can
   now fail. When searching for a word, users start at the left and work towards
   the right. However, left-factoring separates the left-most characters from
   the rest of a literal. More generally, left factoring obfuscates the
   semantics of an expression. 
   
   Additionally, left factoring introduces replication. Consider a programming
   language like Java. Two common statements are variable assignments and if
   statements. Variables can be any sequence of lowercase characters that is not
   a keyword. Figure [[fig:java-factor]] shows fragments of ~lalrpop~ and
   \ky{}-\lambda-typed Nibble that match these two types of statements. The
   ~lalrpop~ fragment is as expected -- ~"if"~ does one thing and variables do
   another. The Nibble fragment introduces a lot of repetition to avoid
   left-factoring. 
   
   #+label: fig:java-factor
   #+name: fig:java-factor
   #+caption: Comparison of ~lalrpop~ and the Chomp parser generator showing 
   #+caption: Chomp's left-factoring for a Java-like language.
   #+begin_figure
   \begin{minted}{rust}
   Statement = {
       "if" "(" <e: Expression> ")" "{" <then: Statements> "}",
       <var: r"[a-z]+"> "=" <e : Expression> ";" => {
           if !var.is_keyword() { 
               // all good
           } else {
               // error!
           }
   }
   \end{minted}
   
   # *TODO: maybe split into two figures?*
   
   \begin{minted}{nibble.py -x}
   let if_body = "(" . expr . ")" . opt ws . "{" . stmts . "}";
   let assign_body = opt ws . "=" . expr . ";";
   let stmt = 
     "i" . ( "f" . ( if_body 
                   | plus a_to_z . assign_body)
           | assign_body 
           | ("a"|/* ... */|"e"|"g"|/* ... */|"z") . star a_to_z . assign_body
           )
   | ("a"|/* ... */|"h"|"j"|/* ... */|"z") . star a_to_z . assign_body
   ;
   \end{minted}
   #+end_figure
   
   ~lalrpop~ avoids the left factoring problem via two mechanisms. The first is
   that it accepts a larger range of language descriptions, as described above.
   Secondly, it has a lexing stage. Section [[*Translators]] talks about lexers in
   some depth. To summarise, the input character stream is split into different
   tokens, which is given to the parser. The lexer decides whether a particular
   sequence of characters is a keyword or just a regular name.
   
   # *TODO: explains why Chomp doesn't have a lexer, but otherwise feels out of
   # place. Maybe cut.*
   
   Chomp does not use a lexer. Whilst lexers are useful to avoid left factoring,
   they are not necessary. Throughout the project, a more powerful type system
   was judged to be a more useful addition to Chomp. Given a sufficiently smart
   type system, the left factoring problem can be eliminated entirely.
   
   Another feature of ~lalrpop~ not present in Nibble is the use of semantic
   actions. These are small computations performed by the parser during parsing,
   often used to build the abstract syntax tree. 
   # *TODO: example*. 

   Chomp can bypass semantic actions using Rust's trait system. All data types
   used by chewed parsers are made publicly accessible. Developers using Chomp
   can define trait implementations for these data types, giving them arbitrary
   properties. This is how AutoNibble converts its concrete syntax tree into an
   abstract syntax tree for the Nibble language -- it uses the same ~Convert~
   trait used in the front-end of Chomp.
   
   # *TODO: And now, this.* 
   Chomp is easier to integrate into projects than ~lalrpop~. Chomp uses Rust's
   procedural macro system to transform Nibble code embedded in a Rust file into
   a chewed parser. ~lalrpop~ instead requires an external build script to first
   convert ~lalrpop~ descriptions into Rust source code, which has to be
   explicitly included in the project.
   
   # *TODO: rewrite next paragraph. It might not even be true.*
   
   Finally, Chomp computes explicit types for Nibble expressions. This has
   benefits in terms of error reporting over ~lalrpop~. Under the \ky{} type
   system, an \mre{} is only rejected if one of the \(\circledast\) or \(\#\)
   constraints does not hold. The types of the relevant sub expressions then
   trace the origin of the problem. Using ~lalrpop~, parser generation only
   fails if there is a problem generating the output parser.
   
# *** Comparison with BNF
#     Typed Nibble is much less practical than BNF used by most other parser
#     generators. Typed Nibble reintroduces some repetition. Listing
#     [[lst:nibble-left-factor]] shows two Nibble expressions. Their languages are
#     equivalent, but the first fails to type check whilst the second succeeds. 

#     #+label: lst:nibble-left-factor
#     #+name: lst:nibble-left-factor
#     #+caption: Left factoring of a Nibble expression
#     #+begin_src nibble
#       // Unfactored
#       let x = ...;
#       let y = ...;
#       let alpha = "a" | "b" | "c" | ... | "z";
#       let ident = [rec](alpha.(_|rec));
#       match "if".x | "iter".y | ident;

#       // Left factored
#       let alpha = "a" | "b" | "c" | ... | "z";
#       let ident = [rec](alpha.(_|rec));
#       let ident_cont = _|ident;
#       match
#         "i".(_|
#              "f".(x|ident_cont)|
#              "t".(_|
#                   "e".(_|
#                        "r".(y|ident_cont)|
#                        ("a"|"b"|...).ident_cont)|
#                   ("a"|"b"|...).ident_cont)
#              ("a"|"b"|...).ident_cont)|
#         ("a"|"b"|...).ident_cont;
#     #+end_src

#     The first expression is not well-typed because the first sets are not
#     disjoint. ~"if"~, ~"iter"~ and ~ident~ can all start with ~i~. By
#     left-factoring, we remove an ~"i"~ from each expression and then try and
#     parse the rest. This has to be recursively repeated until every first set is
#     disjoint.

#     A left-factored expression has a lot of repetition. Originally, ~ident~ was
#     used once, and the long concatenation of characters appeared only once.
#     After left-factoring, ~ident_cont~ appears five times, and there are four
#     additional long alternations of characters. There is also a huge increase in
#     cognitive load. Before, it was easy to tell that something special could
#     happen after parsing an ~if~ or an ~iter~. This is obfuscated in the
#     left-factored version.

#     Parser generators using BNF typically do not experience this problem. The
#     bottom-up nature of the parsers generated from BNF allow for BNF
#     declarations that are not left factored. They also typically include a
#     lexer, which would split ~"if"~, ~"iter~ and ~ident~ into distinct tokens.
    
# *** Comparison with Other Rust Libraries
#     Finally, we compare how much effort goes is required to integrate Nibble and
#     Chomp into a project compared to integrating ~lalrpop~, a popular
#     traditional parser generator for Rust. The first major difference is that
#     ~lalrpop~ requires using a build script. *TODO: why is this bad?*

#     ~lalrpop~ features semantic actions that are not present in Nibble. Semantic
#     actions provide a way for a parser to execute arbitrary code during parsing.
#     This can eliminate the need for parse tree data structures, and let the
#     parser build the desired datatype directly. 

#     As stated in section [[*Requirements Analysis]], semantic actions were a stretch
#     goal for Nibble. Upon further research, there were some conflicting
#     requirements that would complicate their implementation. Notably, Nibble
#     expressions should be independent of the target programming language of
#     Chomp. Typically, parser generators pass semantic actions straight through
#     to the target programming language. Such a technique would break the
#     requirement for Nibble. Hence, another programming language would have to be
#     defined, which could be translated into the target programming language.
#     This appeared to be significantly more effort than other stretch
#     requirements, so semantic actions have not been added to Chomp.

#     Chomp produces Rust source code. Rust has a strong separation between where
#     data types are declared and where methods are defined on them. Although
#     Chomp declares the parse tree data types and some of their methods, these
#     features of Rust allow the user to define their own methods. Using the
#     procedural macro system, semantic actions can be defined near the Nibble
#     expression, even though they cannot be embedded within it. One problem with
#     this approach is that it requires the user to reference data types generated
#     by Chomp. Most of these names are automatically generated, and there is no
#     easy way to discover those names.

#     The final significant usability difference between Nibble and ~lalrpop~ is
#     that  ~lalrpop~ uses a lexer. Chomp does not use a lexer for two reasons.
#     First, it would require Nibble to include a datatype definition for the
#     tokens. To do this in a way that is independent from Chomp's target language
#     would need a translator and language syntax. Secondly, many practical
#     languages change the lexer behaviour depending on the parsing state. For
#     example, characters in a JSON string are treated differently from characters
#     elsewhere. If the lexer requires the parser state, and the parser can
#     perform the functions of the lexer, the complexity of adding a stateful
#     lexer does not seem justified to me.
   
** Performance of Chewed Parsers
   # *TODO: trend lines in graphs are misleadingly linear. Explain or remove them.*
   
   One claim made by Krishnaswami and Yallop cite:10.1145/3314221.3314625 was
   that the parsing algorithm they presented produced linear parsers. After a
   discussion on the benchmark methodology, we test whether this claim extends
   to Chomp and chewed parsers and compare the performance of chewed parsers to
   ~lalrpop~ and handwritten parsers.

*** Methodology
    Benchmarks were performed using the ~criterion~ library for Rust. By
    analysing the timing data produced by ~criterion~, it is possible to estimate
    how long it takes a function to run and the variance of that measurement. To
    estimate the asymptotic behaviour of a parser on the size of the input,
    benchmarks are performed for many different input lengths.

    First, we describe how ~criterion~ measures performance. There are two
    stages it takes to benchmark every function: warm up and measurement. The
    warm up stage prepares the system for executing the desired function. The
    function is executed once, then twice, then four times and so on, until the
    total warm up time exceeds three seconds.

    Measurements are split into samples. Each sample consists of a number of
    iterations, or executions, of the function. ~criterion~ measures the
    duration of each of one hundred samples. For the \(n\)th sample, the number
    of iterations \( I_n = n d \), where \(d\) is a scaling factor chosen so
    that all one hundred samples are collected in approximately thirty seconds.
    \(d\) is calculated by the number of iterations and duration of the warm up
    period.

    We assume that a sample measurement \( T_n = M + n t + \epsilon \), where
    \(M\) is a constant measurement error, \(t\) is the time of one function
    iteration, and \epsilon is a normally distributed error. By performing
    linear regression on the sample measurements with respect to the sample
    size, you can compute \(\bar{t}\) and \(\bar{\sigma^2}\), an estimate for
    \(t\) and the variance of the estimate.

    Only one benchmark was performed at a time. Every benchmark used a single
    thread of execution. The measurements were collected on a desktop computer
    with the following specifications:
    # *TODO: specs.*

*** Performance of AutoNibble
    AutoNibble is the most complex example of Nibble that exists, and hence
    produces the most complex chewed parser. Roughly eighty lines of Nibble are
    translated into over 4800 lines of Rust source code.
    # *TODO: footnote on how collected?*
    We compare the performance of AutoNibble to the Nibble parser in
    Chomp's front end.

    Figure [[fig:autonibble-vs-chomp]] shows the benchmark results for AutoNibble
    and the handwritten Nibble parser in Chomp. The benchmarks were computed on
    Nibble expressions of various lengths, ranging from a few bytes to 3200
    bytes. Because AutoNibble and Chomp's parser produce different concrete
    syntax trees, the benchmarks include normalisation to the Nibble abstract
    syntax tree, as detailed in section [[*The Front-End: Parsing and
    Normalisation]].
    
    #+label: fig:autonibble-vs-chomp
    #+name: fig:autonibble-vs-chomp
    #+caption: Graph of wall-time to parse a Nibble expression against the
    #+caption: length of the Nibble expression in bytes. This compares the
    #+caption: handwritten Nibble parser used by the Chomp parser generator
    #+caption: against AutoNibble.
    [[./images/autonibble.png]]
  
    The trend in the data for both AutoNibble and Chomp's Nibble parser suggests
    that both parsers operate in linear time. Note that even the largest input
    is quite small, so behaviour might change for bigger inputs. 

    This graph also suggests that AutoNibble outperforms Chomp's parser.
    # *TODO: Why?*
   
#     Firstly, AutoNibble can only parse the ASCII subset of Nibble. The Nibble
#     language is defined on Unicode characters. AutoNibble was restricted to the
#     ASCII subset because it is only a technical demonstration of Nibble and
#     Chomp.

#     Secondly, AutoNibble discards information about the origin of tokens. To aid
#     users in writing well-typed Nibble, Chomp preserves the source code location
#     of tokens. When Chomp finds a type error, it can report the exact location
#     to users. AutoChomp will not be used in practice, so it does not construct
#     or preserve this information.

#     Finally, *TODO: third thing?*

#     * Chewed parsers have linear time complexity
#       * Use an F test - linear is no worse than other model
#         * linear vs exponential: \(y = a + bx + e ^{cx} + \epsilon\)
#         * linear vs polynomial: \(y = a + b x^c + \epsilon\)
#     * Chewed parser performance
#       * Use a Chow test - combined is no worse than separate
*** Performance against ~lalrpop~
    Whilst Autonibble is the most complex example of Nibble, using Nibble as the
    benchmark language has some issues. There are very few examples of Nibble,
    and the complexity of writing Nibble descriptions for other parser
    generators seems daunting, especially given how the language was
    continuously evolving. Instead, chewed, handwritten and ~lalrpop~ parsers
    were created for two widely-used languages: JSON and arithmetic.

    Figures [[fig:bench-json]] and [[fig:bench-arith]] show the benchmark results for
    JSON and arithmetic respectively. For JSON, each parser produced a Rust
    datatype corresponding to the JSON value. The input data ranges from roughly
    one hundred bytes in length to 28000 bytes, and was taken from a weather
    API. For arithmetic, the result of evaluating the expression was computed.
    The length of data goes from 100 bytes up to 32000 and was randomly
    generated in advance of writing the benchmark.

    #+label: fig:bench-json
    #+name: fig:bench-json
    #+caption: Graph of wall-time to parse a JSON value string against the
    #+caption: length of the string in bytes. This compares a Chomp-generated
    #+caption: parser, a ~lalrpop~-generated parser and a handwritten parser.
    [[./images/json.png]]

    #+label: fig:bench-arith
    #+name: fig:bench-arith
    #+caption: Graph of wall-time to parse and evaluate an arithmetic expression
    #+caption: against the length of the arithmetic expression in bytes. This 
    #+caption: compares a Chomp-generated parser, a ~lalrpop~-generated parser 
    #+caption: and a handwritten parser.
    [[./images/arith.png]]

    In both cases, the chewed parsers appear to run in linear time. This gives
    further evidence that the parsers truly do have linear performance.

    In these two tests, the handwritten parser outperforms both generated
    parsers. The handwritten parser can immediately produce values of the
    appropriate type. On the other hand, the generated parsers first produce a
    concrete syntax tree before converting it to the correct type. Chewed
    parsers create the full tree and are then converted. ~lalrpop~ parsers
    create fragments of concrete syntax trees before performing conversion.

    # *TODO: compare ~lalrpop~ and chewed parsers.*
    
    For arithmetic, the chewed parser appears to outperform the ~lalrpop~
    parser.
    # *TODO: Why?*

    Conversely, ~lalrpop~ appears to slightly outperform the chewed parser for
    JSON.
    # *TODO: Why?*
   
#     In both cases, the handwritten parser performs better than the chewed parser
#     and the other generated parser. There are some potential reasons for this.
#     Firstly, the handwritten parser produces values of the target datatype
#     directly. Because the developer is in control of all the code, there do not
#     need to be any intermediate conversions. In contrast, chewed parsers have to
#     produce a full parse tree before conversion. *What about ~lalrpop~?*

#     Secondly, *another reason.*

#     Chewed parsers have comparable performance to ~lalrpop~ parsers. *Why?*
#     *Can verify this claim with a Chow test.*

* Conclusion
  # My project was a success. I achieved the success criterion of implementing
  # and testing AutoNibble. AutoNibble even managed to outperform the handwritten
  # parser for the Nibble language, suggesting the Chomp parser generator is
  # useful in real-world applications.
  
  This project set out to develop a parser generator based on \mres{}. By
  creating the Nibble language, I designed an ergonomic way to describe \mres{}
  (section [[*The Nibble Language]]). I then designed two type systems for the
  Nibble language, which extend the \ky{} type system to the syntax of Nibble
  (section [[*Type Systems for Nibble]]). One of these type systems was implemented
  in the Chomp parser generator (section [[*Chomp Repository Overview]]), to produce
  parsers implemented in Rust.

  We proved that the Nibble language is capable of being able to describe
  itself. Further, using the Chomp parser generator to create the AutoNibble
  parser, we found that AutoNibble outperforms a hand-written parser. We also
  found that chewed parsers have comparable performance to
  traditionally-generated parsers.
  
  # I also completed one extension requirement and made significant progress
  # towards implementing another. Chomp is integrated with Rust's procedural macro
  # system, which makes integrating Chomp much easier than integrating some other
  # parser generators. I also designed the LM type system, which can assign
  # polymorphic function types to Nibble expressions.
  
  # Chewed parsers have performance comparable to other generated parsers (section
  # [[*Quantitative]]) and, unlike ordinary parser combinators, come with a
  # compile-time guarantee of linear performance (section [[*AutoNibble]]).
  
** Lessons Learned
   The time table proposed in my original project proposal was quite optimistic.
   I underestimated the effect that external time pressures -- such as
   supervisions during Lent term -- would have on the time I could spend on this
   project. Fortunately I had allocated plenty of slack time in the proposed
   schedule. However, a more relaxed schedule would have been the better option.

   The code structure used by the Chomp parser generator has three separate
   modules for the front-, middle-, and back-ends. As described in section
   [[*Translators]], this is the architecture typically used by translators in
   industry. When the interface to each end is stable, there is little coupling
   between the three ends. However, because the Nibble language and the Chomp
   parser generator are both brand-new, the interface for each end was
   constantly evolving. Every new feature changed one of these critical
   interfaces, so almost every file had to be modified. More research should
   have been spent trying to find an architecture that could withstand the fast
   development cycle used by the project.

   The end-to-end tests for this project were written relatively early on in the
   evolution of the Nibble language and the Chomp parser generator. Several
   times, these tests and benchmarks had to be almost completely rewritten to
   account for changes in the syntax of Nibble or the structure of
   Chomp-generated parsers. Writing more unit tests could have reduced the
   number of necessary end-to-end tests, and, since unit tests are faster to
   change than end-to-end tests, more time could have been spent adding new
   features instead of maintaining tests.
   
** Future Work
   The LM Type system generates the potential for lots of future work. First,
   there is no proof that it is a correct type system. Whilst the modifications
   to the \hm{} and \ky{} type systems are small, these changes could require
   imaginative solutions to prove soundness and completeness properties. Formal
   proofs of these properties are necessary to make sure the LM type system
   achieves what it sets out to complete.
    
   Secondly, the Chomp parser generator could be modified to use the LM type
   system. Even without proof, a practical implementation can provide evidence
   for the claims made by the LM type system. It can also help to justify some
   design decisions of the LM type system, such as the choice to use structural
   unification for types. Work on this implementation can proceed alongside a
   proof for the type system. Using a dependant-type system such as Agda could
   allow for the proof and implementation to be tightly coupled, in the sense
   that changes to one necessitates changes to the other.
    
   I conjectured that the LM type system does not accept expressions for a
   larger class of languages than the \ky{} type system does. Unfortunately both
   type systems reject some simple expressions that can be parsed in linear
   time, for example the \mre{} \((\epsilon \vert a) \cdot b \), representing an
   optional prefix symbol \(a\) before the symbol \(b\). Searching for a type
   system for \mres{} that permits accepts a wider range of expressions would
   make writing well-typed Nibble easier.
    
   In section [[*The \ky{}-\lambda Type System]], we discussed how reduction of
   Nibble expressions can perform arbitrary computations, using Church encoding.
   In contrast, the LM type system performs no such computation, because of the
   syntactic nature of the type system. The Nibble language and the LM type
   system could be extended to support additional data types, such as natural
   numbers. This would make it possible to express a richer set of parser
   combinators in Nibble.
   
#+latex: %TC:ignore
* References
  \printbibliography[heading=none]{}

\appendix

* Project Proposal
  \input{proposal.tex}
  
# * Pink Book
# ** Introduction [0/2]
#    * [ ] Clear motivation
#    * [ ] Justifies potential benefits of success
# ** Preparation [0/4]
#    * [ ] Good or excellent requirements analysis
#    * [ ] Justified and documented selection of suitable tools
#    * [ ] Good engineering approach
#    * [ ] Clear presentation of challenging background beyond Part IB
# ** Implementation [0/7]
#    * [ ] Contribution to the field
#    * [ ] Application of extra-curricular reading
#    * [ ] Original interpretation of previous work
#    * [ ] Challenging goals and substantial deliverables
#    * [ ] Selection and application of appropriate mathematical and engineering techniques
#    * [ ] Clear and justified repository overview
#    * [ ] At most minor faults in execution or understanding
# ** Evaluation [0/3]
#    * [ ] Clearly presented argument demonstrating success criteria met
#    * [ ] Excellent evidence of critical thought and interpretation of results
#    * [ ] Substantiate any claims of success, improvements or novelty
# ** Conclusion [0/3]
#    * [ ] Provide an effective summary of work completed
#    * [ ] Good future work
#    * [ ] Personal reflection on the lessons learned
# * Niggles
#   * [ ] Front-end or front end

#+latex: %TC:endignore
